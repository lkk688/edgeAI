
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../01a_nvidia_jetson/">
      
      
        <link rel="next" href="../08_prompt_engineering_langchain_jetson/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>NLP + Inference Optimization - Jetson Cyber & AI Summer Camp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#nlp-applications-llm-optimization-on-jetson" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-header__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Jetson Cyber & AI Summer Camp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              NLP + Inference Optimization
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-nav__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Jetson Cyber & AI Summer Camp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00_sjsujetsontool_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sjsujetsontool Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01a_nvidia_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to NVIDIA Jetson
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01b_linux_networking_tools.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Linux and Networking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Core Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_programming_env_python_cpp_cuda.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Programming Environment (Python/C++/CUDA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_accelerated_computing_python_cuda.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerated Python + CUDA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04a_numpy_pytorch_intro.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Numpy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04b_pytorch_intro.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cyber Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Cyber Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01b_linux_networking_tools.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux Networking Tools
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01c_packet_sniffing_monitoring.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Packet Sniffing & Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01d_linux_cyber_defense_basics.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux Cyber Defense Tools
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01e_linux_cyber_attack_simulation.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simulated Attacks & Detection
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    AI & LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            AI & LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_cnn_image_processing_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CNNs + Image Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_transformers_llms_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformers & LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    NLP + Inference Optimization
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    NLP + Inference Optimization
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-optimize-llms-on-jetson" class="md-nav__link">
    <span class="md-ellipsis">
      🤖 Why Optimize LLMs on Jetson?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 1. Model Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✅ 1. Model Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-q4_k_m" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 What is Q4_K_M?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-use-smaller-or-distilled-models" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 2. Use Smaller or Distilled Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-use-tensorrt-or-onnx-for-inference" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 3. Use TensorRT or ONNX for Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-offload-selected-layers" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 4. Offload Selected Layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nlp-application-evaluation-labs" class="md-nav__link">
    <span class="md-ellipsis">
      📊 NLP Application Evaluation Labs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📊 NLP Application Evaluation Labs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#all-in-one-nlp-toolkit" class="md-nav__link">
    <span class="md-ellipsis">
      🛠️ All-in-One NLP Toolkit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🛠️ All-in-One NLP Toolkit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-1-multi-application-nlp-benchmark-suite" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 1: Multi-Application NLP Benchmark Suite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧪 Lab 1: Multi-Application NLP Benchmark Suite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-evaluation-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Setup Evaluation Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-comprehensive-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Run Comprehensive Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-container" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Container:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-llm-inference-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 Run LLM Inference Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-2-advanced-optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 2: Advanced Optimization Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧪 Lab 2: Advanced Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#run-optimization-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      Run Optimization Benchmarks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#record-results" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Record Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-deliverables" class="md-nav__link">
    <span class="md-ellipsis">
      📋 Lab Deliverables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📋 Lab Deliverables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-lab-1-multi-application-benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 1 (Multi-Application Benchmark):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-2-optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 2 (Optimization Techniques):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-3-llm-container-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 3 (LLM Container Comparison):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-nlp-optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Advanced NLP Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🎯 Advanced NLP Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-pruning-for-jetson" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🔧 Model Pruning for Jetson
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🚀 Knowledge Distillation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-dynamic-batching-for-real-time-inference" class="md-nav__link">
    <span class="md-ellipsis">
      3. 🔄 Dynamic Batching for Real-time Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-real-time-performance-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      4. 📊 Real-time Performance Monitoring
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-3-compare-llm-inference-in-containers" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 3: Compare LLM Inference in Containers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#objective" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Objective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-container-for-each-method" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 Setup Container for Each Method
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔧 Setup Container for Each Method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#huggingface-container" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace Container:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llamacpp-container" class="md-nav__link">
    <span class="md-ellipsis">
      llama.cpp Container:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-container_1" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Container:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-llm-inference-comparison_1" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 Run LLM Inference Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt_1" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies_1" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#record-results_1" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Record Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-deliverables_1" class="md-nav__link">
    <span class="md-ellipsis">
      📋 Lab Deliverables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📋 Lab Deliverables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-lab-1-multi-application-benchmark_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 1 (Multi-Application Benchmark):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-2-optimization-techniques_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 2 (Optimization Techniques):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-3-llm-container-comparison_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 3 (LLM Container Comparison):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-nlp-optimization-strategies_1" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Advanced NLP Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🎯 Advanced NLP Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-pruning-for-jetson_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🔧 Model Pruning for Jetson
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-knowledge-distillation_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🚀 Knowledge Distillation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-dynamic-batching-for-real-time-inference_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 🔄 Dynamic Batching for Real-time Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-real-time-performance-monitoring_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 📊 Real-time Performance Monitoring
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt_2" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies_2" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 🌐 FastAPI Production Server">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#api-endpoints-available" class="md-nav__link">
    <span class="md-ellipsis">
      API Endpoints Available:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-the-server" class="md-nav__link">
    <span class="md-ellipsis">
      Testing the Server:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-criteria" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Evaluation Criteria
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bonus-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Bonus Challenges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_prompt_engineering_langchain_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_rag_app_langchain_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG Apps
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_local_ai_agents_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Local AI Agents
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Final Project
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Final Project
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_final_challenges_hackathon.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hackathon & Challenges
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-optimize-llms-on-jetson" class="md-nav__link">
    <span class="md-ellipsis">
      🤖 Why Optimize LLMs on Jetson?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 1. Model Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✅ 1. Model Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-q4_k_m" class="md-nav__link">
    <span class="md-ellipsis">
      🔍 What is Q4_K_M?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-use-smaller-or-distilled-models" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 2. Use Smaller or Distilled Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-use-tensorrt-or-onnx-for-inference" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 3. Use TensorRT or ONNX for Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-offload-selected-layers" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 4. Offload Selected Layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nlp-application-evaluation-labs" class="md-nav__link">
    <span class="md-ellipsis">
      📊 NLP Application Evaluation Labs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📊 NLP Application Evaluation Labs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#all-in-one-nlp-toolkit" class="md-nav__link">
    <span class="md-ellipsis">
      🛠️ All-in-One NLP Toolkit
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🛠️ All-in-One NLP Toolkit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-1-multi-application-nlp-benchmark-suite" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 1: Multi-Application NLP Benchmark Suite
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧪 Lab 1: Multi-Application NLP Benchmark Suite">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-evaluation-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Setup Evaluation Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-comprehensive-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Run Comprehensive Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-container" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Container:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-llm-inference-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 Run LLM Inference Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-2-advanced-optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 2: Advanced Optimization Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧪 Lab 2: Advanced Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#run-optimization-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      Run Optimization Benchmarks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#record-results" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Record Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-deliverables" class="md-nav__link">
    <span class="md-ellipsis">
      📋 Lab Deliverables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📋 Lab Deliverables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-lab-1-multi-application-benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 1 (Multi-Application Benchmark):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-2-optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 2 (Optimization Techniques):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-3-llm-container-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 3 (LLM Container Comparison):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-nlp-optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Advanced NLP Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🎯 Advanced NLP Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-pruning-for-jetson" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🔧 Model Pruning for Jetson
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🚀 Knowledge Distillation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-dynamic-batching-for-real-time-inference" class="md-nav__link">
    <span class="md-ellipsis">
      3. 🔄 Dynamic Batching for Real-time Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-real-time-performance-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      4. 📊 Real-time Performance Monitoring
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-3-compare-llm-inference-in-containers" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Lab 3: Compare LLM Inference in Containers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#objective" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Objective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-container-for-each-method" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 Setup Container for Each Method
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔧 Setup Container for Each Method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#huggingface-container" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace Container:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llamacpp-container" class="md-nav__link">
    <span class="md-ellipsis">
      llama.cpp Container:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-container_1" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Container:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#run-llm-inference-comparison_1" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 Run LLM Inference Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt_1" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies_1" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#record-results_1" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Record Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-deliverables_1" class="md-nav__link">
    <span class="md-ellipsis">
      📋 Lab Deliverables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📋 Lab Deliverables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#for-lab-1-multi-application-benchmark_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 1 (Multi-Application Benchmark):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-2-optimization-techniques_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 2 (Optimization Techniques):
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#for-lab-3-llm-container-comparison_1" class="md-nav__link">
    <span class="md-ellipsis">
      For Lab 3 (LLM Container Comparison):
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-nlp-optimization-strategies_1" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Advanced NLP Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🎯 Advanced NLP Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-model-pruning-for-jetson_1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🔧 Model Pruning for Jetson
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-knowledge-distillation_1" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🚀 Knowledge Distillation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-dynamic-batching-for-real-time-inference_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 🔄 Dynamic Batching for Real-time Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-real-time-performance-monitoring_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 📊 Real-time Performance Monitoring
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bonus-lab-export-huggingface-onnx-tensorrt_2" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#production-deployment-strategies_2" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Production Deployment Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Production Deployment Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-multi-stage-docker-optimization_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 🐳 Multi-Stage Docker Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fastapi-production-server_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 🌐 FastAPI Production Server
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 🌐 FastAPI Production Server">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#api-endpoints-available" class="md-nav__link">
    <span class="md-ellipsis">
      API Endpoints Available:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-the-server" class="md-nav__link">
    <span class="md-ellipsis">
      Testing the Server:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-criteria" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Evaluation Criteria
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bonus-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Bonus Challenges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="nlp-applications-llm-optimization-on-jetson">🧠 NLP Applications &amp; LLM Optimization on Jetson<a class="headerlink" href="#nlp-applications-llm-optimization-on-jetson" title="Permanent link">&para;</a></h1>
<p><strong>Author:</strong> Dr. Kaikai Liu, Ph.D.<br />
<strong>Position:</strong> Associate Professor, Computer Engineering<br />
<strong>Institution:</strong> San Jose State University<br />
<strong>Contact:</strong> <a href="mailto:kaikai.liu@sjsu.edu">kaikai.liu@sjsu.edu</a></p>
<p>This tutorial covers essential techniques for deploying and optimizing NLP applications on Jetson devices. You'll learn about various optimization strategies, benchmark different NLP tasks, and implement production-ready solutions. All examples and implementations are combined into a single, easy-to-use command-line tool (<code>jetson_nlp_toolkit.py</code>) that you can use to experiment with different approaches.</p>
<h2 id="why-optimize-llms-on-jetson">🤖 Why Optimize LLMs on Jetson?<a class="headerlink" href="#why-optimize-llms-on-jetson" title="Permanent link">&para;</a></h2>
<p>Jetson Orin Nano has limited power and memory (e.g., 8GB), so optimizing models for:</p>
<ul>
<li>💾 Lower memory usage</li>
<li>⚡ Faster inference latency</li>
<li>🔌 Better energy efficiency</li>
</ul>
<p>Enables real-time NLP applications at the edge.</p>
<hr />
<h2 id="optimization-strategies">🚀 Optimization Strategies<a class="headerlink" href="#optimization-strategies" title="Permanent link">&para;</a></h2>
<h3 id="1-model-quantization">✅ 1. Model Quantization<a class="headerlink" href="#1-model-quantization" title="Permanent link">&para;</a></h3>
<p>Quantization reduces the precision of model weights (e.g., FP32 → INT8 or Q4) to shrink size and improve inference speed.</p>
<h4 id="what-is-q4_k_m">🔍 What is Q4_K_M?<a class="headerlink" href="#what-is-q4_k_m" title="Permanent link">&para;</a></h4>
<ul>
<li>Q4 = 4-bit quantization (16x smaller than FP32)</li>
<li>K = Grouped quantization for accuracy preservation</li>
<li>M = Variant with optimized metadata handling</li>
</ul>
<p>Q4_K_M is commonly used in <code>llama.cpp</code> for <strong>best quality/speed tradeoff</strong> on Jetson.</p>
<h3 id="2-use-smaller-or-distilled-models">✅ 2. Use Smaller or Distilled Models<a class="headerlink" href="#2-use-smaller-or-distilled-models" title="Permanent link">&para;</a></h3>
<p>Distillation creates smaller models (e.g., DistilBERT) by mimicking larger models while reducing parameters.</p>
<ul>
<li>Faster and lighter than full LLMs</li>
</ul>
<h3 id="3-use-tensorrt-or-onnx-for-inference">✅ 3. Use TensorRT or ONNX for Inference<a class="headerlink" href="#3-use-tensorrt-or-onnx-for-inference" title="Permanent link">&para;</a></h3>
<p>Export HuggingFace or PyTorch models to ONNX and use:</p>
<ul>
<li><code>onnxruntime-gpu</code></li>
<li><code>TensorRT</code> engines (for low latency and reduced memory use)</li>
</ul>
<h3 id="4-offload-selected-layers">✅ 4. Offload Selected Layers<a class="headerlink" href="#4-offload-selected-layers" title="Permanent link">&para;</a></h3>
<p>For large models, tools like <code>llama-cpp-python</code> allow setting <code>n_gpu_layers</code> to control how many transformer layers use GPU vs CPU.</p>
<hr />
<h2 id="nlp-application-evaluation-labs">📊 NLP Application Evaluation Labs<a class="headerlink" href="#nlp-application-evaluation-labs" title="Permanent link">&para;</a></h2>
<h3 id="all-in-one-nlp-toolkit">🛠️ All-in-One NLP Toolkit<a class="headerlink" href="#all-in-one-nlp-toolkit" title="Permanent link">&para;</a></h3>
<p>We've created a comprehensive Python script that combines all the NLP applications, optimization techniques, and evaluation methods from this tutorial into a single command-line tool.</p>
<p>&lt;!-- #### Installation</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Clone the repository if you haven&#39;t already</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/yourusername/edgeAI.git
<span class="nb">cd</span><span class="w"> </span>edgeAI

<span class="c1"># Install dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>evaluate<span class="w"> </span>rouge-score<span class="w"> </span>fastapi<span class="w"> </span>uvicorn<span class="w"> </span>aiohttp<span class="w"> </span>psutil<span class="w"> </span>matplotlib<span class="w"> </span>numpy

<span class="c1"># Optional dependencies for specific features</span>
pip<span class="w"> </span>install<span class="w"> </span>redis<span class="w"> </span>llama-cpp-python<span class="w"> </span>requests
<span class="sb">```</span><span class="w"> </span>--&gt;

<span class="c1">#### Usage</span>

The<span class="w"> </span>toolkit<span class="w"> </span>provides<span class="w"> </span>several<span class="w"> </span>commands<span class="w"> </span><span class="k">for</span><span class="w"> </span>different<span class="w"> </span>NLP<span class="w"> </span>tasks<span class="w"> </span>and<span class="w"> </span>optimizations:

<span class="sb">```</span>bash
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span><span class="o">[</span>command<span class="o">]</span><span class="w"> </span><span class="o">[</span>options<span class="o">]</span>
</code></pre></div>
<p>Available commands:
- <code>evaluate</code>: Run NLP evaluation suite
- <code>optimize</code>: Run optimization benchmarks
- <code>llm</code>: Compare LLM inference methods
- <code>server</code>: Run NLP server
- <code>loadtest</code>: Run load tests against NLP server</p>
<h4 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h4>
<p><strong>1. Evaluate NLP Tasks:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Run full evaluation suite</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>all

<span class="c1"># Evaluate specific task (sentiment, qa, summarization, ner)</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>sentiment

<span class="c1"># Save results to custom file</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>all<span class="w"> </span>--output<span class="w"> </span>my_results.json
</code></pre></div>
<p><strong>2. Benchmark Optimization Techniques:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Compare quantization methods</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>quantization

<span class="c1"># Test model pruning</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>pruning<span class="w"> </span>--ratio<span class="w"> </span><span class="m">0</span>.3

<span class="c1"># Use custom model</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>all<span class="w"> </span>--model<span class="w"> </span>bert-base-uncased
</code></pre></div>
<p><strong>3. Compare LLM Inference Methods:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Test all inference methods</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>all

<span class="c1"># Test specific method with custom model</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>huggingface<span class="w"> </span>--model<span class="w"> </span>gpt2

<span class="c1"># Test llama.cpp with custom model path</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>llamacpp<span class="w"> </span>--model-path<span class="w"> </span>/path/to/model.gguf
</code></pre></div>
<p><strong>4. Run NLP Server:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Start server on default port (8000)</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>server

<span class="c1"># Specify host and port</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>server<span class="w"> </span>--host<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span>--port<span class="w"> </span><span class="m">5000</span>
</code></pre></div>
<p><strong>5. Run Load Tests:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Test server with default settings</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest

<span class="c1"># Custom test configuration</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000<span class="w"> </span>--concurrent<span class="w"> </span><span class="m">20</span><span class="w"> </span>--requests<span class="w"> </span><span class="m">500</span>
</code></pre></div>
<h3 id="lab-1-multi-application-nlp-benchmark-suite">🧪 Lab 1: Multi-Application NLP Benchmark Suite<a class="headerlink" href="#lab-1-multi-application-nlp-benchmark-suite" title="Permanent link">&para;</a></h3>
<p><strong>Objective</strong>: Evaluate and compare different NLP applications on Jetson using standardized datasets</p>
<h4 id="setup-evaluation-environment">Setup Evaluation Environment<a class="headerlink" href="#setup-evaluation-environment" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Create evaluation container</span>
docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/nlp_eval:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/datasets:/datasets<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>nvcr.io/nvidia/pytorch:24.04-py3<span class="w"> </span>/bin/bash

<span class="c1"># Install evaluation dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>datasets<span class="w"> </span>evaluate<span class="w"> </span>rouge-score<span class="w"> </span>sacrebleu<span class="w"> </span>spacy
python<span class="w"> </span>-m<span class="w"> </span>spacy<span class="w"> </span>download<span class="w"> </span>en_core_web_sm
</code></pre></div>
<h4 id="run-comprehensive-evaluation">Run Comprehensive Evaluation<a class="headerlink" href="#run-comprehensive-evaluation" title="Permanent link">&para;</a></h4>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py"> compiled)</p>
<h4 id="ollama-container">Ollama Container:<a class="headerlink" href="#ollama-container" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>ollama:/root/.ollama<span class="w"> </span>ollama/ollama
</code></pre></div>
<hr />
<h3 id="run-llm-inference-comparison">🔁 Run LLM Inference Comparison<a class="headerlink" href="#run-llm-inference-comparison" title="Permanent link">&para;</a></h3>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py"> to run NLP evaluations:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Run all NLP evaluations</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>all

<span class="c1"># Run specific task evaluations</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>sentiment
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>qa
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>summarization
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--task<span class="w"> </span>ner

<span class="c1"># Save results to custom file</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>evaluate<span class="w"> </span>--output<span class="w"> </span>my_results.json
</code></pre></div>
<p>The evaluation suite includes:
- <strong>Sentiment Analysis</strong>: IMDB dataset with DistilBERT
- <strong>Question Answering</strong>: SQuAD dataset with DistilBERT
- <strong>Text Summarization</strong>: CNN/DailyMail with T5-small
- <strong>Named Entity Recognition</strong>: CoNLL-2003 with BERT-large</p>
<p>Each evaluation measures:
- Accuracy/F1 scores
- Latency and throughput
- GPU memory usage
- CPU utilization</p>
<hr />
<h3 id="lab-2-advanced-optimization-techniques">🧪 Lab 2: Advanced Optimization Techniques<a class="headerlink" href="#lab-2-advanced-optimization-techniques" title="Permanent link">&para;</a></h3>
<p><strong>Objective</strong>: Implement and compare advanced optimization strategies for NLP models on Jetson</p>
<h4 id="run-optimization-benchmarks">Run Optimization Benchmarks<a class="headerlink" href="#run-optimization-benchmarks" title="Permanent link">&para;</a></h4>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py">mory usage
- Model loading time
- Response quality</p>
<hr />
<h2 id="record-results">📊 Record Results<a class="headerlink" href="#record-results" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Latency (s)</th>
<th>Tokens/sec</th>
<th>GPU Mem (MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>HuggingFace PyTorch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>llama-cpp-python</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Ollama REST API</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Use <code>tegrastats</code> or <code>jtop</code> to observe GPU memory and CPU usage during inference.</p>
<hr />
<h2 id="lab-deliverables">📋 Lab Deliverables<a class="headerlink" href="#lab-deliverables" title="Permanent link">&para;</a></h2>
<h3 id="for-lab-1-multi-application-benchmark">For Lab 1 (Multi-Application Benchmark):<a class="headerlink" href="#for-lab-1-multi-application-benchmark" title="Permanent link">&para;</a></h3>
<ul>
<li>Completed evaluation results JSON file</li>
<li>Performance comparison charts for all NLP tasks</li>
<li>Analysis report identifying best models for each task on Jetson</li>
<li>Resource utilization graphs (<code>tegrastats</code> screenshots)</li>
</ul>
<h3 id="for-lab-2-optimization-techniques">For Lab 2 (Optimization Techniques):<a class="headerlink" href="#for-lab-2-optimization-techniques" title="Permanent link">&para;</a></h3>
<ul>
<li>Quantization comparison table</li>
<li>Memory usage analysis</li>
<li>Speedup and compression ratio calculations</li>
<li>Recommendations for production deployment</li>
</ul>
<h3 id="for-lab-3-llm-container-comparison">For Lab 3 (LLM Container Comparison):<a class="headerlink" href="#for-lab-3-llm-container-comparison" title="Permanent link">&para;</a></h3>
<ul>
<li>Completed benchmark table</li>
<li>Screenshots of <code>tegrastats</code> during inference</li>
<li>Analysis: Which approach is fastest, lightest, and most accurate for Jetson?</li>
</ul>
<hr />
<h2 id="advanced-nlp-optimization-strategies">🎯 Advanced NLP Optimization Strategies<a class="headerlink" href="#advanced-nlp-optimization-strategies" title="Permanent link">&para;</a></h2>
<h3 id="1-model-pruning-for-jetson">1. 🔧 Model Pruning for Jetson<a class="headerlink" href="#1-model-pruning-for-jetson" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># model_pruning.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.prune</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">prune</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="k">def</span><span class="w"> </span><span class="nf">prune_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply structured pruning to transformer model&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">prune</span><span class="o">.</span><span class="n">l1_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
            <span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Example usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">prune_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div>
<h3 id="2-knowledge-distillation">2. 🚀 Knowledge Distillation<a class="headerlink" href="#2-knowledge-distillation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># knowledge_distillation.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DistillationTrainer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">teacher_model</span><span class="p">,</span> <span class="n">student_model</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">teacher_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">student_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">distillation_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate distillation loss&quot;&quot;&quot;</span>
        <span class="c1"># Soft targets from teacher</span>
        <span class="n">teacher_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">teacher_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">student_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">student_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Distillation loss</span>
        <span class="n">distill_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span><span class="p">(</span><span class="n">student_log_probs</span><span class="p">,</span> <span class="n">teacher_probs</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Hard target loss</span>
        <span class="n">hard_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce_loss</span><span class="p">(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Combined loss</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">distill_loss</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">hard_loss</span>
        <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div>
<h3 id="3-dynamic-batching-for-real-time-inference">3. 🔄 Dynamic Batching for Real-time Inference<a class="headerlink" href="#3-dynamic-batching-for-real-time-inference" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># dynamic_batching.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DynamicBatcher</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_wait_time</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">=</span> <span class="n">max_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_wait_time</span> <span class="o">=</span> <span class="n">max_wait_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">add_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add inference request to queue&quot;&quot;&quot;</span>
        <span class="n">future</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Future</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">text</span><span class="p">,</span> <span class="n">future</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">processing</span><span class="p">:</span>
            <span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_batch</span><span class="p">())</span>

        <span class="k">return</span> <span class="k">await</span> <span class="n">future</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">process_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process requests in batches&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># Collect batch</span>
            <span class="k">while</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="ow">and</span> 
                   <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span> <span class="ow">and</span> 
                   <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_wait_time</span><span class="p">):</span>

                <span class="n">text</span><span class="p">,</span> <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="p">:</span>
                    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Small wait for more requests</span>

            <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
                <span class="c1"># Process batch</span>
                <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="c1"># Return results</span>
                <span class="k">for</span> <span class="n">future</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">futures</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
                    <span class="n">future</span><span class="o">.</span><span class="n">set_result</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">inference_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run inference on batch&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">texts</span><span class="p">,</span> 
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> 
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Prediction: </span><span class="si">{</span><span class="n">pred</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>
</code></pre></div>
<h3 id="4-real-time-performance-monitoring">4. 📊 Real-time Performance Monitoring<a class="headerlink" href="#4-real-time-performance-monitoring" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># performance_monitor.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">threading</span><span class="w"> </span><span class="kn">import</span> <span class="n">Thread</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonNLPMonitor</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;latency&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;throughput&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;gpu_memory&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;cpu_usage&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;timestamps&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">start_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Start background monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_monitor_loop</span><span class="p">)</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stop_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stop monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_monitor_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Background monitoring loop&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span><span class="p">:</span>
            <span class="n">timestamp</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># GPU memory</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>  <span class="c1"># MB</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># CPU usage</span>
            <span class="n">cpu_usage</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">cpu_percent</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gpu_memory</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cpu_usage</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;timestamps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timestamp</span><span class="p">)</span>

            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Monitor every 100ms</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latency</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log inference metrics&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Convert to ms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">/</span> <span class="n">latency</span><span class="p">)</span>  <span class="c1"># samples/sec</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current statistics&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;avg_latency_ms&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_throughput&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_gpu_memory_mb&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_cpu_usage&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;total_inferences&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&quot;nlp_performance.png&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot performance metrics&quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

        <span class="c1"># Latency</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Inference Latency (ms)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Latency (ms)&#39;</span><span class="p">)</span>

        <span class="c1"># Throughput</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Throughput (samples/sec)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Samples/sec&#39;</span><span class="p">)</span>

        <span class="c1"># GPU Memory</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPU Memory Usage (MB)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Memory (MB)&#39;</span><span class="p">)</span>

        <span class="c1"># CPU Usage</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CPU Usage (%)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;CPU %&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;📊 Performance plots saved to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="bonus-lab-export-huggingface-onnx-tensorrt">🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT<a class="headerlink" href="#bonus-lab-export-huggingface-onnx-tensorrt" title="Permanent link">&para;</a></h2>
<ol>
<li>Export:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">dummy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">dummy</span><span class="p">,),</span> <span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</code></pre></div>
<ol>
<li>Convert:</li>
</ol>
<div class="highlight"><pre><span></span><code>trtexec<span class="w"> </span>--onnx<span class="o">=</span>model.onnx<span class="w"> </span>--saveEngine<span class="o">=</span>model.trt
</code></pre></div>
<ol>
<li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li>
</ol>
<hr />
<h2 id="production-deployment-strategies">🚀 Production Deployment Strategies<a class="headerlink" href="#production-deployment-strategies" title="Permanent link">&para;</a></h2>
<h3 id="1-multi-stage-docker-optimization">1. 🐳 Multi-Stage Docker Optimization<a class="headerlink" href="#1-multi-stage-docker-optimization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c"># Dockerfile.nlp-production</span>
<span class="c"># Multi-stage build for optimized NLP deployment</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s">builder</span>

<span class="c"># Install build dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>torch-audio<span class="w"> </span>torchaudio<span class="w"> </span>torchvision
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>onnx<span class="w"> </span>onnxruntime-gpu<span class="w"> </span>tensorrt

<span class="c"># Copy and optimize models</span>
<span class="k">COPY</span><span class="w"> </span>models/<span class="w"> </span>/tmp/models/
<span class="k">COPY</span><span class="w"> </span>scripts/optimize_models.py<span class="w"> </span>/tmp/
<span class="k">RUN</span><span class="w"> </span>python<span class="w"> </span>/tmp/optimize_models.py

<span class="c"># Production stage</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span>

<span class="c"># Install only runtime dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.36.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.1.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>onnxruntime-gpu<span class="o">==</span><span class="m">1</span>.16.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">fastapi</span><span class="o">==</span><span class="m">0</span>.104.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">uvicorn</span><span class="o">==</span><span class="m">0</span>.24.0

<span class="c"># Copy optimized models</span>
<span class="k">COPY</span><span class="w"> </span>--from<span class="o">=</span>builder<span class="w"> </span>/tmp/optimized_models/<span class="w"> </span>/app/models/
<span class="k">COPY</span><span class="w"> </span>src/<span class="w"> </span>/app/src/

<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8000</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;uvicorn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;src.main:app&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--host&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--port&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;8000&quot;</span><span class="p">]</span>
</code></pre></div>
<h3 id="2-fastapi-production-server">2. 🌐 FastAPI Production Server<a class="headerlink" href="#2-fastapi-production-server" title="Permanent link">&para;</a></h3>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py"> to compare different optimization methods:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Run all optimization benchmarks</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>all

<span class="c1"># Run specific optimization methods</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>quantization
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>pruning<span class="w"> </span>--ratio<span class="w"> </span><span class="m">0</span>.3
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--method<span class="w"> </span>distillation

<span class="c1"># Benchmark with custom model</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--model<span class="w"> </span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="w"> </span>--samples<span class="w"> </span><span class="m">100</span>

<span class="c1"># Save optimization results</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>optimize<span class="w"> </span>--output<span class="w"> </span>optimization_results.json
</code></pre></div>
<p>The optimization benchmark compares:
- <strong>FP32</strong>: Full precision baseline
- <strong>FP16</strong>: Half precision for GPU acceleration
- <strong>INT8 Dynamic</strong>: Dynamic quantization for CPU
- <strong>TorchScript</strong>: Graph optimization
- <strong>Model Pruning</strong>: Structured weight pruning
- <strong>Knowledge Distillation</strong>: Teacher-student training</p>
<p>Metrics measured:
- Average latency per sample
- Throughput (samples/second)
- Memory usage
- Model size compression
- Speedup vs baseline</p>
<hr />
<h3 id="lab-3-compare-llm-inference-in-containers">🧪 Lab 3: Compare LLM Inference in Containers<a class="headerlink" href="#lab-3-compare-llm-inference-in-containers" title="Permanent link">&para;</a></h3>
<h3 id="objective">🎯 Objective<a class="headerlink" href="#objective" title="Permanent link">&para;</a></h3>
<p>Evaluate inference speed and memory usage for different LLM deployment methods on Jetson inside Docker containers.</p>
<h3 id="setup-container-for-each-method">🔧 Setup Container for Each Method<a class="headerlink" href="#setup-container-for-each-method" title="Permanent link">&para;</a></h3>
<h4 id="huggingface-container">HuggingFace Container:<a class="headerlink" href="#huggingface-container" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>nvcr.io/nvidia/pytorch:24.04-py3<span class="w"> </span>/bin/bash
</code></pre></div>
<p>Inside container:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>accelerate<span class="w"> </span>torch
</code></pre></div>
<h4 id="llamacpp-container">llama.cpp Container:<a class="headerlink" href="#llamacpp-container" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/models:/models<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>jetson-llama-cpp<span class="w"> </span>/bin/bash
</code></pre></div>
<p>(Assumes container has CUDA + llama.cpp compiled)</p>
<h4 id="ollama-container_1">Ollama Container:<a class="headerlink" href="#ollama-container_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>ollama:/root/.ollama<span class="w"> </span>ollama/ollama
</code></pre></div>
<hr />
<h3 id="run-llm-inference-comparison_1">🔁 Run LLM Inference Comparison<a class="headerlink" href="#run-llm-inference-comparison_1" title="Permanent link">&para;</a></h3>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py">ime Performance Monitoring</p>
<div class="highlight"><pre><span></span><code><span class="c1"># performance_monitor.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">threading</span><span class="w"> </span><span class="kn">import</span> <span class="n">Thread</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonNLPMonitor</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;latency&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;throughput&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;gpu_memory&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;cpu_usage&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;timestamps&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">start_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Start background monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_monitor_loop</span><span class="p">)</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stop_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stop monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_monitor_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Background monitoring loop&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span><span class="p">:</span>
            <span class="n">timestamp</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># GPU memory</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>  <span class="c1"># MB</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># CPU usage</span>
            <span class="n">cpu_usage</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">cpu_percent</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gpu_memory</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cpu_usage</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;timestamps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timestamp</span><span class="p">)</span>

            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Monitor every 100ms</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latency</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log inference metrics&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Convert to ms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">/</span> <span class="n">latency</span><span class="p">)</span>  <span class="c1"># samples/sec</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current statistics&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;avg_latency_ms&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_throughput&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_gpu_memory_mb&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_cpu_usage&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;total_inferences&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&quot;nlp_performance.png&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot performance metrics&quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

        <span class="c1"># Latency</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Inference Latency (ms)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Latency (ms)&#39;</span><span class="p">)</span>

        <span class="c1"># Throughput</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Throughput (samples/sec)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Samples/sec&#39;</span><span class="p">)</span>

        <span class="c1"># GPU Memory</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPU Memory Usage (MB)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Memory (MB)&#39;</span><span class="p">)</span>

        <span class="c1"># CPU Usage</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CPU Usage (%)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;CPU %&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;📊 Performance plots saved to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="bonus-lab-export-huggingface-onnx-tensorrt_1">🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT<a class="headerlink" href="#bonus-lab-export-huggingface-onnx-tensorrt_1" title="Permanent link">&para;</a></h2>
<ol>
<li>Export:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">dummy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">dummy</span><span class="p">,),</span> <span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</code></pre></div>
<ol>
<li>Convert:</li>
</ol>
<div class="highlight"><pre><span></span><code>trtexec<span class="w"> </span>--onnx<span class="o">=</span>model.onnx<span class="w"> </span>--saveEngine<span class="o">=</span>model.trt
</code></pre></div>
<ol>
<li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li>
</ol>
<hr />
<h2 id="production-deployment-strategies_1">🚀 Production Deployment Strategies<a class="headerlink" href="#production-deployment-strategies_1" title="Permanent link">&para;</a></h2>
<h3 id="1-multi-stage-docker-optimization_1">1. 🐳 Multi-Stage Docker Optimization<a class="headerlink" href="#1-multi-stage-docker-optimization_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c"># Dockerfile.nlp-production</span>
<span class="c"># Multi-stage build for optimized NLP deployment</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s">builder</span>

<span class="c"># Install build dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>torch-audio<span class="w"> </span>torchaudio<span class="w"> </span>torchvision
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>onnx<span class="w"> </span>onnxruntime-gpu<span class="w"> </span>tensorrt

<span class="c"># Copy and optimize models</span>
<span class="k">COPY</span><span class="w"> </span>models/<span class="w"> </span>/tmp/models/
<span class="k">COPY</span><span class="w"> </span>scripts/optimize_models.py<span class="w"> </span>/tmp/
<span class="k">RUN</span><span class="w"> </span>python<span class="w"> </span>/tmp/optimize_models.py

<span class="c"># Production stage</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span>

<span class="c"># Install only runtime dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.36.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.1.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>onnxruntime-gpu<span class="o">==</span><span class="m">1</span>.16.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">fastapi</span><span class="o">==</span><span class="m">0</span>.104.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">uvicorn</span><span class="o">==</span><span class="m">0</span>.24.0

<span class="c"># Copy optimized models</span>
<span class="k">COPY</span><span class="w"> </span>--from<span class="o">=</span>builder<span class="w"> </span>/tmp/optimized_models/<span class="w"> </span>/app/models/
<span class="k">COPY</span><span class="w"> </span>src/<span class="w"> </span>/app/src/

<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8000</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;uvicorn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;src.main:app&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--host&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--port&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;8000&quot;</span><span class="p">]</span>
</code></pre></div>
<h3 id="2-fastapi-production-server_1">2. 🌐 FastAPI Production Server<a class="headerlink" href="#2-fastapi-production-server_1" title="Permanent link">&para;</a></h3>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py"> to compare different LLM inference methods:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Compare all available LLM inference methods</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>all

<span class="c1"># Test specific inference methods</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>huggingface<span class="w"> </span>--model<span class="w"> </span><span class="s2">&quot;microsoft/DialoGPT-small&quot;</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>llamacpp<span class="w"> </span>--model-path<span class="w"> </span><span class="s2">&quot;/models/mistral.gguf&quot;</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--method<span class="w"> </span>ollama<span class="w"> </span>--model<span class="w"> </span><span class="s2">&quot;mistral&quot;</span>

<span class="c1"># Custom prompts and settings</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--prompts<span class="w"> </span><span class="s2">&quot;Explain the future of AI in education.&quot;</span><span class="w"> </span>--max-tokens<span class="w"> </span><span class="m">100</span>

<span class="c1"># Save comparison results</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>llm<span class="w"> </span>--output<span class="w"> </span>llm_comparison_results.json
</code></pre></div>
<p>The LLM comparison evaluates:
- <strong>HuggingFace Transformers</strong>: GPU-optimized inference
- <strong>llama.cpp</strong>: CPU-optimized quantized models<br />
- <strong>Ollama API</strong>: Containerized LLM serving</p>
<p>Metrics measured:
- Average latency per prompt
- Tokens generated per second
- Memory usage
- Model loading time
- Response quality</p>
<hr />
<h2 id="record-results_1">📊 Record Results<a class="headerlink" href="#record-results_1" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Latency (s)</th>
<th>Tokens/sec</th>
<th>GPU Mem (MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>HuggingFace PyTorch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>llama-cpp-python</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Ollama REST API</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Use <code>tegrastats</code> or <code>jtop</code> to observe GPU memory and CPU usage during inference.</p>
<hr />
<h2 id="lab-deliverables_1">📋 Lab Deliverables<a class="headerlink" href="#lab-deliverables_1" title="Permanent link">&para;</a></h2>
<h3 id="for-lab-1-multi-application-benchmark_1">For Lab 1 (Multi-Application Benchmark):<a class="headerlink" href="#for-lab-1-multi-application-benchmark_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Completed evaluation results JSON file</li>
<li>Performance comparison charts for all NLP tasks</li>
<li>Analysis report identifying best models for each task on Jetson</li>
<li>Resource utilization graphs (<code>tegrastats</code> screenshots)</li>
</ul>
<h3 id="for-lab-2-optimization-techniques_1">For Lab 2 (Optimization Techniques):<a class="headerlink" href="#for-lab-2-optimization-techniques_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Quantization comparison table</li>
<li>Memory usage analysis</li>
<li>Speedup and compression ratio calculations</li>
<li>Recommendations for production deployment</li>
</ul>
<h3 id="for-lab-3-llm-container-comparison_1">For Lab 3 (LLM Container Comparison):<a class="headerlink" href="#for-lab-3-llm-container-comparison_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Completed benchmark table</li>
<li>Screenshots of <code>tegrastats</code> during inference</li>
<li>Analysis: Which approach is fastest, lightest, and most accurate for Jetson?</li>
</ul>
<hr />
<h2 id="advanced-nlp-optimization-strategies_1">🎯 Advanced NLP Optimization Strategies<a class="headerlink" href="#advanced-nlp-optimization-strategies_1" title="Permanent link">&para;</a></h2>
<h3 id="1-model-pruning-for-jetson_1">1. 🔧 Model Pruning for Jetson<a class="headerlink" href="#1-model-pruning-for-jetson_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># model_pruning.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.prune</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">prune</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="k">def</span><span class="w"> </span><span class="nf">prune_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply structured pruning to transformer model&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">prune</span><span class="o">.</span><span class="n">l1_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
            <span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Example usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">prune_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div>
<h3 id="2-knowledge-distillation_1">2. 🚀 Knowledge Distillation<a class="headerlink" href="#2-knowledge-distillation_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># knowledge_distillation.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DistillationTrainer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">teacher_model</span><span class="p">,</span> <span class="n">student_model</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">teacher_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">student_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">distillation_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate distillation loss&quot;&quot;&quot;</span>
        <span class="c1"># Soft targets from teacher</span>
        <span class="n">teacher_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">teacher_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">student_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">student_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Distillation loss</span>
        <span class="n">distill_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_loss</span><span class="p">(</span><span class="n">student_log_probs</span><span class="p">,</span> <span class="n">teacher_probs</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Hard target loss</span>
        <span class="n">hard_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce_loss</span><span class="p">(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Combined loss</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">distill_loss</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">hard_loss</span>
        <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div>
<h3 id="3-dynamic-batching-for-real-time-inference_1">3. 🔄 Dynamic Batching for Real-time Inference<a class="headerlink" href="#3-dynamic-batching-for-real-time-inference_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># dynamic_batching.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DynamicBatcher</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_wait_time</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">=</span> <span class="n">max_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_wait_time</span> <span class="o">=</span> <span class="n">max_wait_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">add_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add inference request to queue&quot;&quot;&quot;</span>
        <span class="n">future</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Future</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">text</span><span class="p">,</span> <span class="n">future</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">processing</span><span class="p">:</span>
            <span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_batch</span><span class="p">())</span>

        <span class="k">return</span> <span class="k">await</span> <span class="n">future</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">process_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process requests in batches&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># Collect batch</span>
            <span class="k">while</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="ow">and</span> 
                   <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span> <span class="ow">and</span> 
                   <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_wait_time</span><span class="p">):</span>

                <span class="n">text</span><span class="p">,</span> <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">request_queue</span><span class="p">:</span>
                    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Small wait for more requests</span>

            <span class="k">if</span> <span class="n">batch</span><span class="p">:</span>
                <span class="c1"># Process batch</span>
                <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">inference_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="c1"># Return results</span>
                <span class="k">for</span> <span class="n">future</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">futures</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
                    <span class="n">future</span><span class="o">.</span><span class="n">set_result</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">processing</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">inference_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run inference on batch&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">texts</span><span class="p">,</span> 
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> 
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Prediction: </span><span class="si">{</span><span class="n">pred</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>
</code></pre></div>
<h3 id="4-real-time-performance-monitoring_1">4. 📊 Real-time Performance Monitoring<a class="headerlink" href="#4-real-time-performance-monitoring_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># performance_monitor.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">threading</span><span class="w"> </span><span class="kn">import</span> <span class="n">Thread</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonNLPMonitor</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;latency&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;throughput&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;gpu_memory&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;cpu_usage&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">),</span>
            <span class="s1">&#39;timestamps&#39;</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">start_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Start background monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_monitor_loop</span><span class="p">)</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">monitor_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stop_monitoring</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stop monitoring&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_monitor_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Background monitoring loop&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">monitoring</span><span class="p">:</span>
            <span class="n">timestamp</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># GPU memory</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>  <span class="c1"># MB</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gpu_memory</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># CPU usage</span>
            <span class="n">cpu_usage</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">cpu_percent</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gpu_memory</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cpu_usage</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;timestamps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timestamp</span><span class="p">)</span>

            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Monitor every 100ms</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latency</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log inference metrics&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latency</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Convert to ms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">/</span> <span class="n">latency</span><span class="p">)</span>  <span class="c1"># samples/sec</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current statistics&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;avg_latency_ms&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_throughput&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_gpu_memory_mb&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;avg_cpu_usage&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]),</span>
            <span class="s1">&#39;total_inferences&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">])</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&quot;nlp_performance.png&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot performance metrics&quot;&quot;&quot;</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

        <span class="c1"># Latency</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;latency&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Inference Latency (ms)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Latency (ms)&#39;</span><span class="p">)</span>

        <span class="c1"># Throughput</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;throughput&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Throughput (samples/sec)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Samples/sec&#39;</span><span class="p">)</span>

        <span class="c1"># GPU Memory</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gpu_memory&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;GPU Memory Usage (MB)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Memory (MB)&#39;</span><span class="p">)</span>

        <span class="c1"># CPU Usage</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;cpu_usage&#39;</span><span class="p">]))</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CPU Usage (%)&#39;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;CPU %&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;📊 Performance plots saved to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="bonus-lab-export-huggingface-onnx-tensorrt_2">🧪 Bonus Lab: Export HuggingFace → ONNX → TensorRT<a class="headerlink" href="#bonus-lab-export-huggingface-onnx-tensorrt_2" title="Permanent link">&para;</a></h2>
<ol>
<li>Export:</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">dummy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">dummy</span><span class="p">,),</span> <span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</code></pre></div>
<ol>
<li>Convert:</li>
</ol>
<div class="highlight"><pre><span></span><code>trtexec<span class="w"> </span>--onnx<span class="o">=</span>model.onnx<span class="w"> </span>--saveEngine<span class="o">=</span>model.trt
</code></pre></div>
<ol>
<li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li>
</ol>
<hr />
<h2 id="production-deployment-strategies_2">🚀 Production Deployment Strategies<a class="headerlink" href="#production-deployment-strategies_2" title="Permanent link">&para;</a></h2>
<h3 id="1-multi-stage-docker-optimization_2">1. 🐳 Multi-Stage Docker Optimization<a class="headerlink" href="#1-multi-stage-docker-optimization_2" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c"># Dockerfile.nlp-production</span>
<span class="c"># Multi-stage build for optimized NLP deployment</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s">builder</span>

<span class="c"># Install build dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>torch-audio<span class="w"> </span>torchaudio<span class="w"> </span>torchvision
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>onnx<span class="w"> </span>onnxruntime-gpu<span class="w"> </span>tensorrt

<span class="c"># Copy and optimize models</span>
<span class="k">COPY</span><span class="w"> </span>models/<span class="w"> </span>/tmp/models/
<span class="k">COPY</span><span class="w"> </span>scripts/optimize_models.py<span class="w"> </span>/tmp/
<span class="k">RUN</span><span class="w"> </span>python<span class="w"> </span>/tmp/optimize_models.py

<span class="c"># Production stage</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">nvcr.io/nvidia/pytorch:24.04-py3</span>

<span class="c"># Install only runtime dependencies</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.36.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.1.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>onnxruntime-gpu<span class="o">==</span><span class="m">1</span>.16.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">fastapi</span><span class="o">==</span><span class="m">0</span>.104.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">uvicorn</span><span class="o">==</span><span class="m">0</span>.24.0

<span class="c"># Copy optimized models</span>
<span class="k">COPY</span><span class="w"> </span>--from<span class="o">=</span>builder<span class="w"> </span>/tmp/optimized_models/<span class="w"> </span>/app/models/
<span class="k">COPY</span><span class="w"> </span>src/<span class="w"> </span>/app/src/

<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8000</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;uvicorn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;src.main:app&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--host&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--port&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;8000&quot;</span><span class="p">]</span>
</code></pre></div>
<h3 id="2-fastapi-production-server_2">2. 🌐 FastAPI Production Server<a class="headerlink" href="#2-fastapi-production-server_2" title="Permanent link">&para;</a></h3>
<p>Use the <mcfile name="jetson_nlp_toolkit.py" path="jetson/jetson_nlp_toolkit.py"></mcfile> to deploy a production-ready NLP server:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Start production NLP server</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>server<span class="w"> </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span>--port<span class="w"> </span><span class="m">8000</span>

<span class="c1"># Start with custom models</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>server<span class="w"> </span>--sentiment-model<span class="w"> </span><span class="s2">&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;</span>

<span class="c1"># Start with monitoring enabled</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>server<span class="w"> </span>--enable-monitoring
</code></pre></div>
<p>The production server includes:
- <strong>FastAPI Framework</strong>: High-performance async API
- <strong>Model Optimization</strong>: FP16 precision and GPU acceleration
- <strong>Batch Processing</strong>: Efficient handling of multiple requests
- <strong>Health Monitoring</strong>: System status and performance metrics
- <strong>Error Handling</strong>: Robust error management and logging
- <strong>WebSocket Support</strong>: Real-time chat interface</p>
<h4 id="api-endpoints-available">API Endpoints Available:<a class="headerlink" href="#api-endpoints-available" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>GET /health</strong>: System health and model status</li>
<li><strong>POST /sentiment</strong>: Batch sentiment analysis</li>
<li><strong>POST /qa</strong>: Question answering</li>
<li><strong>POST /summarize</strong>: Text summarization</li>
<li><strong>WebSocket /chat</strong>: Real-time chat interface</li>
</ul>
<h4 id="testing-the-server">Testing the Server:<a class="headerlink" href="#testing-the-server" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Test health endpoint</span>
curl<span class="w"> </span>http://localhost:8000/health

<span class="c1"># Test sentiment analysis</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="s2">&quot;http://localhost:8000/sentiment&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;texts&quot;: [&quot;I love this product!&quot;, &quot;This is terrible&quot;], &quot;batch_size&quot;: 2}&#39;</span>

<span class="c1"># Test question answering</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="s2">&quot;http://localhost:8000/qa&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;questions&quot;: [&quot;What is AI?&quot;], &quot;contexts&quot;: [&quot;Artificial Intelligence is the simulation of human intelligence.&quot;]}&#39;</span>

<span class="c1">### 3. 📊 Load Testing &amp; Performance Validation</span>

Use<span class="w"> </span>the<span class="w"> </span>&lt;mcfile<span class="w"> </span><span class="nv">name</span><span class="o">=</span><span class="s2">&quot;jetson_nlp_toolkit.py&quot;</span><span class="w"> </span><span class="nv">path</span><span class="o">=</span><span class="s2">&quot;jetson/jetson_nlp_toolkit.py&quot;</span>&gt;&lt;/mcfile&gt;<span class="w"> </span><span class="k">for</span><span class="w"> </span>comprehensive<span class="w"> </span>load<span class="w"> </span>testing:

<span class="sb">```</span>bash
<span class="c1"># Run load test on running server</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000

<span class="c1"># Custom load test parameters</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000<span class="w"> </span>--concurrent<span class="w"> </span><span class="m">20</span><span class="w"> </span>--requests<span class="w"> </span><span class="m">500</span>

<span class="c1"># Test specific endpoints</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000<span class="w"> </span>--endpoint<span class="w"> </span>sentiment
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000<span class="w"> </span>--endpoint<span class="w"> </span>qa

<span class="c1"># Save load test results</span>
python<span class="w"> </span>jetson_nlp_toolkit.py<span class="w"> </span>loadtest<span class="w"> </span>--url<span class="w"> </span>http://localhost:8000<span class="w"> </span>--output<span class="w"> </span>load_test_results.json
</code></pre></div>
<p>The load tester provides:
- <strong>Concurrent Testing</strong>: Simulates multiple users
- <strong>Endpoint Coverage</strong>: Tests all API endpoints
- <strong>Performance Metrics</strong>: Latency, throughput, success rate
- <strong>Error Analysis</strong>: Detailed failure reporting
- <strong>Results Export</strong>: JSON format for further analysis
<div class="highlight"><pre><span></span><code>---

## 🎯 Final Challenge: Complete NLP Pipeline on Jetson

### 🏆 Challenge Objective

Build a complete, production-ready NLP pipeline that processes real-world data and demonstrates all optimization techniques learned in this tutorial.

### 📋 Challenge Requirements

#### 1. **Multi-Modal NLP System**
Implement a system that handles:
- **Text Classification** (sentiment analysis on product reviews)
- **Information Extraction** (NER on news articles)
- **Question Answering** (FAQ system for customer support)
- **Text Summarization** (news article summarization)
- **Real-time Chat** (customer service chatbot)

#### 2. **Optimization Implementation**
- Apply **quantization** (FP16 minimum, INT8 preferred)
- Implement **dynamic batching** for throughput optimization
- Use **model pruning** to reduce memory footprint
- Deploy with **TensorRT** optimization where possible
- Implement **caching** for frequently requested content

#### 3. **Production Deployment**
- **Containerized deployment** with multi-stage Docker builds
- **REST API** with proper error handling and logging
- **Load balancing** for high availability
- **Monitoring and metrics** collection
- **Auto-scaling** based on resource utilization

#### 4. **Performance Benchmarking**
- **Latency analysis** (P50, P95, P99 percentiles)
- **Throughput measurement** (requests per second)
- **Resource utilization** (GPU/CPU/memory usage)
- **Accuracy validation** on standard datasets
- **Cost analysis** (inference cost per request)

### 🛠️ Implementation Guide

Use the comprehensive &lt;mcfile name=&quot;jetson_nlp_toolkit.py&quot; path=&quot;jetson/jetson_nlp_toolkit.py&quot;&gt;&lt;/mcfile&gt; to implement the complete challenge:

```bash
# Run comprehensive evaluation across all NLP tasks
python jetson_nlp_toolkit.py evaluate --all-tasks --save-results challenge_evaluation.json

# Apply all optimization techniques
python jetson_nlp_toolkit.py optimize --all-methods --model distilbert-base-uncased-finetuned-sst-2-english

# Deploy production server with monitoring
python jetson_nlp_toolkit.py server --enable-monitoring --host 0.0.0.0 --port 8000

# Run comprehensive load testing
python jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --concurrent 50 --requests 1000

# Compare LLM inference methods
python jetson_nlp_toolkit.py llm --all-methods --model microsoft/DialoGPT-small
</code></pre></div></p>
<p>The toolkit provides all necessary components:
- <strong>Multi-task NLP Pipeline</strong>: Sentiment, NER, QA, Summarization, Chat
- <strong>Optimization Techniques</strong>: Quantization, Pruning, Distillation, TensorRT
- <strong>Production Deployment</strong>: FastAPI server with WebSocket support
- <strong>Performance Monitoring</strong>: Real-time metrics and analysis
- <strong>Load Testing</strong>: Comprehensive performance validation
- <strong>Caching &amp; Batching</strong>: Redis integration and dynamic batching</p>
<h3 id="evaluation-criteria">📊 Evaluation Criteria<a class="headerlink" href="#evaluation-criteria" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Criterion</th>
<th>Weight</th>
<th>Excellent (90-100%)</th>
<th>Good (70-89%)</th>
<th>Satisfactory (50-69%)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Functionality</strong></td>
<td>25%</td>
<td>All 5 NLP tasks working perfectly</td>
<td>4/5 tasks working</td>
<td>3/5 tasks working</td>
</tr>
<tr>
<td><strong>Optimization</strong></td>
<td>25%</td>
<td>All optimization techniques applied</td>
<td>Most optimizations applied</td>
<td>Basic optimizations</td>
</tr>
<tr>
<td><strong>Performance</strong></td>
<td>20%</td>
<td>&lt;50ms P95 latency, &gt;100 req/s</td>
<td>&lt;100ms P95, &gt;50 req/s</td>
<td>&lt;200ms P95, &gt;20 req/s</td>
</tr>
<tr>
<td><strong>Production Ready</strong></td>
<td>15%</td>
<td>Full deployment with monitoring</td>
<td>Basic deployment</td>
<td>Local deployment only</td>
</tr>
<tr>
<td><strong>Code Quality</strong></td>
<td>10%</td>
<td>Clean, documented, tested</td>
<td>Well-structured</td>
<td>Basic implementation</td>
</tr>
<tr>
<td><strong>Innovation</strong></td>
<td>5%</td>
<td>Novel optimizations/features</td>
<td>Creative solutions</td>
<td>Standard implementation</td>
</tr>
</tbody>
</table>
<h3 id="bonus-challenges">🎯 Bonus Challenges<a class="headerlink" href="#bonus-challenges" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Multi-Language Support</strong>: Extend the pipeline to handle multiple languages</li>
<li><strong>Edge Deployment</strong>: Deploy on actual Jetson hardware with resource constraints</li>
<li><strong>Federated Learning</strong>: Implement model updates without centralized data</li>
<li><strong>Real-time Streaming</strong>: Process continuous data streams with low latency</li>
<li><strong>Custom Models</strong>: Train and deploy domain-specific models</li>
</ol>
<hr />
<h2 id="summary">📌 Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Comprehensive NLP Applications</strong>: Covered 6 major NLP tasks with Jetson-specific optimizations</li>
<li><strong>Advanced Optimization Techniques</strong>: Quantization, pruning, distillation, and dynamic batching</li>
<li><strong>Production Deployment</strong>: Multi-stage Docker builds, FastAPI servers, and load testing</li>
<li><strong>Performance Monitoring</strong>: Real-time metrics collection and analysis</li>
<li><strong>Practical Evaluation</strong>: Standardized benchmarking on popular datasets</li>
<li><strong>Complete Pipeline</strong>: End-to-end solution from development to production</li>
<li><strong>All-in-One Toolkit</strong>: Unified command-line tool (<code>jetson_nlp_toolkit.py</code>) combining all examples and implementations</li>
</ul>
<p>This tutorial provides a comprehensive foundation for deploying production-ready NLP applications on Jetson devices, balancing performance, accuracy, and resource efficiency. The included all-in-one toolkit makes it easy to experiment with different NLP tasks, optimization techniques, and deployment strategies using a simple command-line interface.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>