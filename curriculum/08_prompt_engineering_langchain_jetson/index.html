
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../07_nlp_applications_llm_optimization/">
      
      
        <link rel="next" href="../09_rag_app_langchain_jetson/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>‚úçÔ∏è Prompt Engineering & LangChain - Jetson Cyber & AI Summer Camp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#advanced-prompt-engineering-with-langchain-on-jetson" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-header__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Jetson Cyber & AI Summer Camp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ‚úçÔ∏è Prompt Engineering & LangChain
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-nav__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Jetson Cyber & AI Summer Camp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üî∞ Getting Started with Jetson
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            üî∞ Getting Started with Jetson
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00_sjsujetsontool_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ‚úÖ sjsujetsontool Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00b_sjsujetsontool_cheatsheet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üìã sjsujetsontool Cheatsheet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01a_nvidia_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üîß Introduction to NVIDIA Jetson
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01b_jetson_cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üöÄ CUDA Programming on Jetson
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üêß Linux Fundamentals
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            üêß Linux Fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02a_linux_basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üí° Linux OS Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03a_linux_networking_tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üåê Linux Networking Tools
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    ü§ñ AI & LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            ü§ñ AI & LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_deeplearning_cnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üß† Deep Learning & CNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_transformers_nlp_applications/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üß† Transformers & NLP Applications
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_llms_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üöÄ Large Language Models on Jetson
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_nlp_applications_llm_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üìö NLP Applications & LLM Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    ‚úçÔ∏è Prompt Engineering & LangChain
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    ‚úçÔ∏è Prompt Engineering & LangChain
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ What is Prompt Engineering?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-engineering-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      üèóÔ∏è Prompt Engineering Fundamentals
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üèóÔ∏è Prompt Engineering Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-principles" class="md-nav__link">
    <span class="md-ellipsis">
      üìã Core Principles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-anatomy" class="md-nav__link">
    <span class="md-ellipsis">
      üé® Prompt Anatomy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#platform-setup-three-inference-pathways" class="md-nav__link">
    <span class="md-ellipsis">
      üí° Platform Setup: Three Inference Pathways
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üí° Platform Setup: Three Inference Pathways">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-openai-api-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 1. OpenAI API Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-local-ollama-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 2. Local Ollama Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-local-llama-cpp-python-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 3. Local llama-cpp-python Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-prompt-engineering-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      üé® Core Prompt Engineering Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üé® Core Prompt Engineering Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-chain-of-thought-cot-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      üß† 1. Chain-of-Thought (CoT) Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-few-shot-learning" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ 2. Few-Shot Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-think-step-by-step-vs-think-hard" class="md-nav__link">
    <span class="md-ellipsis">
      üîÑ 3. Think Step by Step vs Think Hard
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-role-based-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      üé≠ 4. Role-Based Prompting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-in-context-learning" class="md-nav__link">
    <span class="md-ellipsis">
      üîÑ 5. In-Context Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_rag_app_langchain_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üîé RAG Applications with LangChain
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ What is Prompt Engineering?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prompt-engineering-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      üèóÔ∏è Prompt Engineering Fundamentals
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üèóÔ∏è Prompt Engineering Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-principles" class="md-nav__link">
    <span class="md-ellipsis">
      üìã Core Principles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-anatomy" class="md-nav__link">
    <span class="md-ellipsis">
      üé® Prompt Anatomy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#platform-setup-three-inference-pathways" class="md-nav__link">
    <span class="md-ellipsis">
      üí° Platform Setup: Three Inference Pathways
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üí° Platform Setup: Three Inference Pathways">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-openai-api-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 1. OpenAI API Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-local-ollama-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 2. Local Ollama Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-local-llama-cpp-python-setup" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ 3. Local llama-cpp-python Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-prompt-engineering-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      üé® Core Prompt Engineering Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üé® Core Prompt Engineering Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-chain-of-thought-cot-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      üß† 1. Chain-of-Thought (CoT) Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-few-shot-learning" class="md-nav__link">
    <span class="md-ellipsis">
      üéØ 2. Few-Shot Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-think-step-by-step-vs-think-hard" class="md-nav__link">
    <span class="md-ellipsis">
      üîÑ 3. Think Step by Step vs Think Hard
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-role-based-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      üé≠ 4. Role-Based Prompting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-in-context-learning" class="md-nav__link">
    <span class="md-ellipsis">
      üîÑ 5. In-Context Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="advanced-prompt-engineering-with-langchain-on-jetson">üß† Advanced Prompt Engineering with LangChain on Jetson<a class="headerlink" href="#advanced-prompt-engineering-with-langchain-on-jetson" title="Permanent link">&para;</a></h1>
<p><strong>Author:</strong> Dr. Kaikai Liu, Ph.D.<br />
<strong>Position:</strong> Associate Professor, Computer Engineering<br />
<strong>Institution:</strong> San Jose State University<br />
<strong>Contact:</strong> <a href="mailto:kaikai.liu@sjsu.edu">kaikai.liu@sjsu.edu</a></p>
<blockquote>
<p><strong>Note:</strong> All code examples in this tutorial have been consolidated into a unified Python script called <code>jetson_prompt_toolkit.py</code>. See the <a href="#-unified-python-script">Unified Python Script</a> section at the end of this document for installation and usage instructions.</p>
</blockquote>
<h2 id="what-is-prompt-engineering">üéØ What is Prompt Engineering?<a class="headerlink" href="#what-is-prompt-engineering" title="Permanent link">&para;</a></h2>
<p>Prompt engineering is the art and science of crafting effective inputs to guide large language models (LLMs) toward desired outputs. It's the bridge between human intent and AI understanding.</p>
<p><strong>Why it matters on Jetson:</strong>
- üöÄ <strong>Efficiency</strong>: Better prompts = fewer tokens = faster inference on edge devices
- üéØ <strong>Accuracy</strong>: Precise instructions lead to more reliable outputs
- üí∞ <strong>Cost-effective</strong>: Reduces API calls and computational overhead
- üîí <strong>Safety</strong>: Proper prompting prevents harmful or biased responses</p>
<hr />
<h2 id="prompt-engineering-fundamentals">üèóÔ∏è Prompt Engineering Fundamentals<a class="headerlink" href="#prompt-engineering-fundamentals" title="Permanent link">&para;</a></h2>
<h3 id="core-principles">üìã Core Principles<a class="headerlink" href="#core-principles" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Clarity</strong>: Be specific and unambiguous</li>
<li><strong>Context</strong>: Provide relevant background information</li>
<li><strong>Structure</strong>: Use consistent formatting and organization</li>
<li><strong>Examples</strong>: Show the model what you want (few-shot learning)</li>
<li><strong>Constraints</strong>: Set clear boundaries and expectations</li>
</ol>
<h3 id="prompt-anatomy">üé® Prompt Anatomy<a class="headerlink" href="#prompt-anatomy" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>[SYSTEM MESSAGE] - Sets the AI&#39;s role and behavior
[CONTEXT] - Background information
[INSTRUCTION] - What you want the AI to do
[FORMAT] - How you want the output structured
[EXAMPLES] - Sample inputs and outputs
[CONSTRAINTS] - Limitations and requirements
</code></pre></div>
<hr />
<h2 id="platform-setup-three-inference-pathways">üí° Platform Setup: Three Inference Pathways<a class="headerlink" href="#platform-setup-three-inference-pathways" title="Permanent link">&para;</a></h2>
<h3 id="1-openai-api-setup">üîπ 1. OpenAI API Setup<a class="headerlink" href="#1-openai-api-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">class</span><span class="w"> </span><span class="nc">OpenAIPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>  <span class="c1"># Cost-effective for learning</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Send prompt to OpenAI API&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_tokens&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">simple_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">system_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple prompt interface&quot;&quot;&quot;</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">system_msg</span><span class="p">:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_msg</span><span class="p">})</span>
        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">openai_prompter</span> <span class="o">=</span> <span class="n">OpenAIPrompter</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span>
    <span class="s2">&quot;Explain edge computing in simple terms&quot;</span><span class="p">,</span>
    <span class="s2">&quot;You are a helpful AI assistant specializing in technology.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--backends<span class="w"> </span>openai
</code></pre></div></p>
<p>The LangChain integration is also available:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>compare<span class="w"> </span>--backends<span class="w"> </span>openai
</code></pre></div></p>
</blockquote>
<h3 id="2-local-ollama-setup">üîπ 2. Local Ollama Setup<a class="headerlink" href="#2-local-ollama-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>

<span class="k">class</span><span class="w"> </span><span class="nc">OllamaPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:11434&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;llama3.2:3b&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_url</span> <span class="o">=</span> <span class="n">base_url</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">is_available</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if Ollama is running&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/api/tags&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">system_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Send prompt to Ollama&quot;&quot;&quot;</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
            <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span>
                <span class="s2">&quot;num_predict&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_tokens&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">system_msg</span><span class="p">:</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">&quot;system&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">system_msg</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/api/generate&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="n">payload</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;response&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ollama error: </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">chat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Chat interface for conversation&quot;&quot;&quot;</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
            <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;options&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/api/chat&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="n">payload</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ollama error: </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Setup and usage</span>
<span class="n">ollama_prompter</span> <span class="o">=</span> <span class="n">OllamaPrompter</span><span class="p">()</span>

<span class="k">if</span> <span class="n">ollama_prompter</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama_prompter</span><span class="o">.</span><span class="n">prompt</span><span class="p">(</span>
        <span class="s2">&quot;Explain the benefits of running AI models locally on Jetson devices&quot;</span><span class="p">,</span>
        <span class="s2">&quot;You are an expert in edge AI and NVIDIA Jetson platforms.&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ollama not available. Start with: ollama serve&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--backends<span class="w"> </span>ollama
</code></pre></div></p>
</blockquote>
<h3 id="3-local-llama-cpp-python-setup">üîπ 3. Local llama-cpp-python Setup<a class="headerlink" href="#3-local-llama-cpp-python-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llama_cpp</span><span class="w"> </span><span class="kn">import</span> <span class="n">Llama</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LlamaCppPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span> <span class="o">=</span> <span class="n">model_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
            <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
            <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_gpu_layers&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Use all GPU layers</span>
            <span class="n">n_ctx</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_ctx&#39;</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>  <span class="c1"># Context window</span>
            <span class="n">n_batch</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;n_batch&#39;</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>  <span class="c1"># Batch size</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Loaded model: </span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate response from prompt&quot;&quot;&quot;</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_tokens&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">),</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;top_p&#39;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
            <span class="n">stop</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">]),</span>
            <span class="n">echo</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">inference_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚è±Ô∏è Inference time: </span><span class="si">{</span><span class="n">inference_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">chat_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">system_msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">user_msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Format as chat conversation&quot;&quot;&quot;</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;&lt;|im_start|&gt;system</span>
<span class="si">{</span><span class="n">system_msg</span><span class="si">}</span><span class="s2">&lt;|im_end|&gt;</span>
<span class="s2">&lt;|im_start|&gt;user</span>
<span class="si">{</span><span class="n">user_msg</span><span class="si">}</span><span class="s2">&lt;|im_end|&gt;</span>
<span class="s2">&lt;|im_start|&gt;assistant</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;|im_end|&gt;&quot;</span><span class="p">])</span>

<span class="c1"># Usage (requires downloaded model)</span>
<span class="c1"># llama_prompter = LlamaCppPrompter(&quot;/models/qwen2.5-3b-instruct-q4_k_m.gguf&quot;)</span>
<span class="c1"># response = llama_prompter.chat_prompt(</span>
<span class="c1">#     &quot;You are an AI assistant specialized in edge computing.&quot;,</span>
<span class="c1">#     &quot;What are the advantages of running LLMs on Jetson devices?&quot;</span>
<span class="c1"># )</span>
<span class="c1"># print(response)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--backends<span class="w"> </span>llamacpp<span class="w"> </span>--model_path<span class="w"> </span>/path/to/your/model.gguf
</code></pre></div></p>
</blockquote>
<hr />
<h2 id="core-prompt-engineering-techniques">üé® Core Prompt Engineering Techniques<a class="headerlink" href="#core-prompt-engineering-techniques" title="Permanent link">&para;</a></h2>
<h3 id="1-chain-of-thought-cot-reasoning">üß† 1. Chain-of-Thought (CoT) Reasoning<a class="headerlink" href="#1-chain-of-thought-cot-reasoning" title="Permanent link">&para;</a></h3>
<p>Guide the model to think step-by-step for complex problems.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_chain_of_thought</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare basic vs chain-of-thought prompting&quot;&quot;&quot;</span>

    <span class="c1"># Basic prompt</span>
    <span class="n">basic_prompt</span> <span class="o">=</span> <span class="s2">&quot;What is 15</span><span class="si">% o</span><span class="s2">f 240?&quot;</span>

    <span class="c1"># Chain-of-thought prompt</span>
    <span class="n">cot_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Solve this step by step:</span>
<span class="s2">What is 15</span><span class="si">% o</span><span class="s2">f 240?</span>

<span class="s2">Think through this carefully:</span>
<span class="s2">1. First, convert the percentage to a decimal</span>
<span class="s2">2. Then multiply by the number</span>
<span class="s2">3. Show your work</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="c1"># Advanced CoT with reasoning</span>
    <span class="n">advanced_cot</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a math tutor. Solve this problem step-by-step, explaining your reasoning:</span>

<span class="s2">Problem: What is 15</span><span class="si">% o</span><span class="s2">f 240?</span>

<span class="s2">Please:</span>
<span class="s2">1. Explain what the problem is asking</span>
<span class="s2">2. Show the mathematical steps</span>
<span class="s2">3. Verify your answer makes sense</span>
<span class="s2">4. Provide the final answer</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">basic_prompt</span><span class="p">,</span> <span class="n">cot_prompt</span><span class="p">,</span> <span class="n">advanced_cot</span>

<span class="c1"># Test with different models</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_cot_across_models</span><span class="p">():</span>
    <span class="n">basic</span><span class="p">,</span> <span class="n">cot</span><span class="p">,</span> <span class="n">advanced</span> <span class="o">=</span> <span class="n">demonstrate_chain_of_thought</span><span class="p">()</span>

    <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;OpenAI&quot;</span><span class="p">:</span> <span class="n">openai_prompter</span><span class="p">,</span>
        <span class="s2">&quot;Ollama&quot;</span><span class="p">:</span> <span class="n">ollama_prompter</span> <span class="k">if</span> <span class="n">ollama_prompter</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># &quot;LlamaCpp&quot;: llama_prompter  # Uncomment if available</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">prompter</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">prompter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> - Chain of Thought Comparison&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

        <span class="c1"># Basic prompt</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prompter</span><span class="p">,</span> <span class="s1">&#39;simple_prompt&#39;</span><span class="p">):</span>
            <span class="n">basic_response</span> <span class="o">=</span> <span class="n">prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">basic</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">basic_response</span> <span class="o">=</span> <span class="n">prompter</span><span class="o">.</span><span class="n">prompt</span><span class="p">(</span><span class="n">basic</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Basic: </span><span class="si">{</span><span class="n">basic_response</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

        <span class="c1"># CoT prompt</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prompter</span><span class="p">,</span> <span class="s1">&#39;simple_prompt&#39;</span><span class="p">):</span>
            <span class="n">cot_response</span> <span class="o">=</span> <span class="n">prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">advanced</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cot_response</span> <span class="o">=</span> <span class="n">prompter</span><span class="o">.</span><span class="n">prompt</span><span class="p">(</span><span class="n">advanced</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CoT: </span><span class="si">{</span><span class="n">cot_response</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

<span class="c1"># Run the test</span>
<span class="c1"># test_cot_across_models()</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> These Chain-of-Thought examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>cot
</code></pre></div></p>
</blockquote>
<h3 id="2-few-shot-learning">üéØ 2. Few-Shot Learning<a class="headerlink" href="#2-few-shot-learning" title="Permanent link">&para;</a></h3>
<p>Provide examples to guide the model's behavior.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_few_shot_learning</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show the power of examples in prompting&quot;&quot;&quot;</span>

    <span class="c1"># Zero-shot prompt</span>
    <span class="n">zero_shot</span> <span class="o">=</span> <span class="s2">&quot;Classify the sentiment of this text: &#39;The Jetson Orin is amazing for AI development!&#39;&quot;</span>

    <span class="c1"># Few-shot prompt with examples</span>
    <span class="n">few_shot</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Classify the sentiment of the following texts as Positive, Negative, or Neutral:</span>

<span class="s2">Examples:</span>
<span class="s2">Text: &quot;I love using CUDA for parallel computing!&quot;</span>
<span class="s2">Sentiment: Positive</span>

<span class="s2">Text: &quot;The installation process was frustrating and took hours.&quot;</span>
<span class="s2">Sentiment: Negative</span>

<span class="s2">Text: &quot;The device has 8GB of RAM.&quot;</span>
<span class="s2">Sentiment: Neutral</span>

<span class="s2">Text: &quot;This AI model runs incredibly fast on the Jetson!&quot;</span>
<span class="s2">Sentiment: Positive</span>

<span class="s2">Now classify this:</span>
<span class="s2">Text: &quot;The Jetson Orin is amazing for AI development!&quot;</span>
<span class="s2">Sentiment:</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="c1"># Structured few-shot with format specification</span>
    <span class="n">structured_few_shot</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a sentiment analysis expert. Classify text sentiment and provide confidence.</span>

<span class="s2">Format your response as: Sentiment: [POSITIVE/NEGATIVE/NEUTRAL] (Confidence: X%)</span>

<span class="s2">Examples:</span>
<span class="s2">Text: &quot;CUDA programming is so powerful!&quot;</span>
<span class="s2">Response: Sentiment: POSITIVE (Confidence: 95%)</span>

<span class="s2">Text: &quot;The setup documentation is unclear.&quot;</span>
<span class="s2">Response: Sentiment: NEGATIVE (Confidence: 85%)</span>

<span class="s2">Text: &quot;The device weighs 500 grams.&quot;</span>
<span class="s2">Response: Sentiment: NEUTRAL (Confidence: 90%)</span>

<span class="s2">Now analyze:</span>
<span class="s2">Text: &quot;The Jetson Orin is amazing for AI development!&quot;</span>
<span class="s2">Response:</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">zero_shot</span><span class="p">,</span> <span class="n">few_shot</span><span class="p">,</span> <span class="n">structured_few_shot</span>

<span class="c1"># Test few-shot learning</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_few_shot_learning</span><span class="p">():</span>
    <span class="n">zero</span><span class="p">,</span> <span class="n">few</span><span class="p">,</span> <span class="n">structured</span> <span class="o">=</span> <span class="n">demonstrate_few_shot_learning</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Few-Shot Learning Demonstration&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="c1"># Test with OpenAI</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìù Zero-shot:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">zero</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìö Few-shot:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">few</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üèóÔ∏è Structured few-shot:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">structured</span><span class="p">))</span>

<span class="c1"># test_few_shot_learning()</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> These Few-Shot Learning examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>few_shot
</code></pre></div></p>
</blockquote>
<h3 id="3-think-step-by-step-vs-think-hard">üîÑ 3. Think Step by Step vs Think Hard<a class="headerlink" href="#3-think-step-by-step-vs-think-hard" title="Permanent link">&para;</a></h3>
<p>Compare different reasoning triggers.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compare_reasoning_triggers</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare different ways to trigger reasoning&quot;&quot;&quot;</span>

    <span class="n">problem</span> <span class="o">=</span> <span class="s2">&quot;A Jetson Orin Nano has 8GB RAM. If an AI model uses 60</span><span class="si">% o</span><span class="s2">f available RAM, and the system reserves 1GB for OS, how much RAM is the model actually using?&quot;</span>

    <span class="n">prompts</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Direct&quot;</span><span class="p">:</span> <span class="n">problem</span><span class="p">,</span>

        <span class="s2">&quot;Think Step by Step&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">problem</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Think step by step.&quot;</span><span class="p">,</span>

        <span class="s2">&quot;Think Hard&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">problem</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Think hard about this problem.&quot;</span><span class="p">,</span>

        <span class="s2">&quot;Let&#39;s Work Through This&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">problem</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Let&#39;s work through this systematically:&quot;</span><span class="p">,</span>

        <span class="s2">&quot;Detailed Analysis&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Problem: </span><span class="si">{</span><span class="n">problem</span><span class="si">}</span>

<span class="s2">Please provide a detailed analysis:</span>
<span class="s2">1. Identify what information we have</span>
<span class="s2">2. Determine what we need to calculate</span>
<span class="s2">3. Show your mathematical work</span>
<span class="s2">4. Verify your answer makes sense</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>

        <span class="s2">&quot;Expert Mode&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a computer systems expert. Analyze this memory allocation problem:</span>

<span class="si">{</span><span class="n">problem</span><span class="si">}</span>

<span class="s2">Provide:</span>
<span class="s2">- Clear breakdown of available vs. used memory</span>
<span class="s2">- Step-by-step calculation</span>
<span class="s2">- Practical implications for AI development</span>
<span class="s2">&quot;&quot;&quot;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">prompts</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_reasoning_triggers</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test different reasoning approaches&quot;&quot;&quot;</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">compare_reasoning_triggers</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üß† Reasoning Trigger Comparison&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">trigger_name</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç </span><span class="si">{</span><span class="n">trigger_name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="c1"># Show first 200 characters</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="si">}{</span><span class="s1">&#39;...&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">200</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">()</span>  <span class="c1"># Add spacing</span>

<span class="c1"># test_reasoning_triggers()</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> These Reasoning Trigger examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>reasoning
</code></pre></div></p>
</blockquote>
<h3 id="4-role-based-prompting">üé≠ 4. Role-Based Prompting<a class="headerlink" href="#4-role-based-prompting" title="Permanent link">&para;</a></h3>
<p>Assign specific roles to get specialized responses.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_role_based_prompting</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show how different roles affect responses&quot;&quot;&quot;</span>

    <span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;How should I optimize my deep learning model for deployment on Jetson Orin?&quot;</span>

    <span class="n">roles</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;Generic AI&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;system&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful AI assistant.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">question</span>
        <span class="p">},</span>

        <span class="s2">&quot;ML Engineer&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;system&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a senior machine learning engineer with 10 years of experience in model optimization and edge deployment.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">question</span>
        <span class="p">},</span>

        <span class="s2">&quot;NVIDIA Expert&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;system&quot;</span><span class="p">:</span> <span class="s2">&quot;You are an NVIDIA developer advocate specializing in Jetson platforms, TensorRT optimization, and edge AI deployment.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">question</span>
        <span class="p">},</span>

        <span class="s2">&quot;Performance Specialist&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;system&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a performance optimization specialist focused on real-time AI inference on resource-constrained devices.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="si">{</span><span class="n">question</span><span class="si">}</span>

<span class="s2">Please provide:</span>
<span class="s2">1. Specific optimization techniques</span>
<span class="s2">2. Performance benchmarking approaches</span>
<span class="s2">3. Trade-offs between accuracy and speed</span>
<span class="s2">4. Practical implementation steps</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="p">},</span>

        <span class="s2">&quot;Beginner-Friendly Tutor&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;system&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a patient AI tutor who explains complex concepts in simple terms with practical examples.&quot;</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="si">{</span><span class="n">question</span><span class="si">}</span>

<span class="s2">Please explain this in beginner-friendly terms with:</span>
<span class="s2">- Simple explanations of technical concepts</span>
<span class="s2">- Step-by-step guidance</span>
<span class="s2">- Common pitfalls to avoid</span>
<span class="s2">- Practical examples</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">roles</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_role_based_prompting</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test different role assignments&quot;&quot;&quot;</span>
    <span class="n">roles</span> <span class="o">=</span> <span class="n">demonstrate_role_based_prompting</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üé≠ Role-Based Prompting Comparison&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">role_name</span><span class="p">,</span> <span class="n">role_config</span> <span class="ow">in</span> <span class="n">roles</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üë§ </span><span class="si">{</span><span class="n">role_name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">openai_prompter</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span>
                <span class="n">role_config</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">],</span>
                <span class="n">role_config</span><span class="p">[</span><span class="s2">&quot;system&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="c1"># Show first 300 characters</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span><span class="si">}{</span><span class="s1">&#39;...&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">300</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">()</span>  <span class="c1"># Add spacing</span>

<span class="c1"># test_role_based_prompting()</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> These Role-Based Prompting examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>roles
</code></pre></div></p>
</blockquote>
<h3 id="5-in-context-learning">üîÑ 5. In-Context Learning<a class="headerlink" href="#5-in-context-learning" title="Permanent link">&para;</a></h3>
<p>Teach the model new tasks within the conversation.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_in_context_learning</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show how to teach models new formats/tasks&quot;&quot;&quot;</span>

    <span class="c1"># Teaching a custom format for Jetson specs</span>
    <span class="n">in_context_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">I&#39;ll teach you a format for describing Jetson device specifications:</span>

<span class="s2">Format: [Device] | [GPU] | [CPU] | [RAM] | [Storage] | [Power] | [Use Case]</span>

<span class="s2">Examples:</span>
<span class="s2">Jetson Nano | 128-core Maxwell GPU | Quad-core ARM A57 | 4GB LPDDR4 | microSD | 5W | IoT/Education</span>
<span class="s2">Jetson Xavier NX | 384-core Volta GPU | 6-core Carmel ARM | 8GB LPDDR4x | microSD | 10W | Edge AI</span>
<span class="s2">Jetson AGX Orin | 2048-core Ampere GPU | 12-core Cortex-A78AE | 64GB LPDDR5 | NVMe SSD | 60W | Autonomous Vehicles</span>

<span class="s2">Now format this device:</span>
<span class="s2">Jetson Orin Nano: 1024-core Ampere GPU, 6-core Cortex-A78AE CPU, 8GB LPDDR5 RAM, microSD storage, 15W power consumption, suitable for robotics and edge AI applications.</span>

<span class="s2">Formatted specification:</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="c1"># Teaching code documentation style</span>
    <span class="n">code_doc_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">I&#39;ll teach you how to document Jetson AI code with our team&#39;s style:</span>

<span class="s2">Style: Brief description + Parameters + Example + Performance note</span>

<span class="s2">Example 1:</span>
<span class="s2">```python</span>
<span class="s2">def load_tensorrt_model(engine_path: str) -&gt; trt.ICudaEngine:</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="n">Load</span> <span class="n">TensorRT</span> <span class="n">engine</span> <span class="k">for</span> <span class="n">optimized</span> <span class="n">inference</span><span class="o">.</span>

    <span class="n">Args</span><span class="p">:</span>
        <span class="n">engine_path</span><span class="p">:</span> <span class="n">Path</span> <span class="n">to</span> <span class="o">.</span><span class="n">engine</span> <span class="n">file</span>

    <span class="n">Returns</span><span class="p">:</span>
        <span class="n">TensorRT</span> <span class="n">engine</span> <span class="n">ready</span> <span class="k">for</span> <span class="n">inference</span>

    <span class="n">Example</span><span class="p">:</span>
        <span class="n">engine</span> <span class="o">=</span> <span class="n">load_tensorrt_model</span><span class="p">(</span><span class="s2">&quot;yolo.engine&quot;</span><span class="p">)</span>

    <span class="n">Performance</span><span class="p">:</span> <span class="o">~</span><span class="mi">3</span><span class="n">x</span> <span class="n">faster</span> <span class="n">than</span> <span class="n">ONNX</span> <span class="n">on</span> <span class="n">Jetson</span> <span class="n">Orin</span>
    <span class="s2">&quot;&quot;&quot;</span>
</code></pre></div>
<p>Example 2:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess image for model inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        image: Input image as numpy array</span>
<span class="sd">        target_size: (height, width) for resizing</span>

<span class="sd">    Returns:</span>
<span class="sd">        Preprocessed tensor ready for model</span>

<span class="sd">    Example:</span>
<span class="sd">        tensor = preprocess_image(img, (640, 640))</span>

<span class="sd">    Performance: GPU preprocessing saves 15ms per frame</span>
<span class="sd">    &quot;&quot;&quot;</span>
</code></pre></div></p>
<p>Now document this function using our style:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">optimize_model_for_jetson</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">precision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fp16&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># This function converts PyTorch models to TensorRT for Jetson deployment</span>
    <span class="c1"># It takes a model path and precision setting</span>
    <span class="c1"># Returns path to optimized engine file</span>
    <span class="c1"># Typical speedup is 2-4x on Jetson devices</span>
</code></pre></div></p>
<p>Documented function:
"""</p>
<div class="highlight"><pre><span></span><code>return in_context_prompt, code_doc_prompt
</code></pre></div>
<p>def test_in_context_learning():
    """Test in-context learning capabilities"""
    spec_prompt, code_prompt = demonstrate_in_context_learning()</p>
<div class="highlight"><pre><span></span><code>print(&quot;üîÑ In-Context Learning Demonstration&quot;)
print(&quot;=&quot; * 50)

print(&quot;\nüìã Learning Custom Specification Format:&quot;)
print(&quot;-&quot; * 40)
response1 = openai_prompter.simple_prompt(spec_prompt)
print(response1)

print(&quot;\nüíª Learning Code Documentation Style:&quot;)
print(&quot;-&quot; * 40)
response2 = openai_prompter.simple_prompt(code_prompt)
print(response2)
</code></pre></div>
<h1 id="test_in_context_learning">test_in_context_learning()<a class="headerlink" href="#test_in_context_learning" title="Permanent link">&para;</a></h1>
<div class="highlight"><pre><span></span><code>&gt; **Note:** These In-Context Learning examples are included in the unified `jetson_prompt_toolkit.py` script. You can run them with:
&gt; ```bash
&gt; python jetson_prompt_toolkit.py --mode basic --technique in_context
&gt; ```

---

## üîó Introduction to LangChain

LangChain is a powerful framework that simplifies building applications with Large Language Models (LLMs). It provides abstractions for prompt management, model integration, and complex workflows.

### üåü Why LangChain for Prompt Engineering?

1. **Model Agnostic**: Switch between OpenAI, Ollama, and local models seamlessly
2. **Prompt Templates**: Reusable, parameterized prompts
3. **Chain Composition**: Combine multiple prompts and models
4. **Memory Management**: Maintain conversation context
5. **Output Parsing**: Structure model responses automatically

### üì¶ Installation for Jetson

```bash
# Core LangChain
pip install langchain langchain-community

# For OpenAI integration
pip install langchain-openai

# For local model support
pip install langchain-ollama

# For structured output
pip install pydantic
</code></pre></div>
<h3 id="langchain-with-openai">üîπ LangChain with OpenAI<a class="headerlink" href="#langchain-with-openai" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LangChainOpenAIPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
            <span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">simple_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple prompting with optional system message&quot;&quot;&quot;</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">system_message</span><span class="p">:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">))</span>
        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">prompt</span><span class="p">))</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">template_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Use prompt templates for reusable prompts&quot;&quot;&quot;</span>
        <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
        <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">formatted_prompt</span><span class="p">)])</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">chain_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Chain multiple prompts together&quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompts</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">full_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Previous context: </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">New task: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">full_prompt</span> <span class="o">=</span> <span class="n">prompt</span>

            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">full_prompt</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">response</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span>  <span class="c1"># Keep context manageable</span>

        <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Example usage</span>
<span class="n">langchain_openai</span> <span class="o">=</span> <span class="n">LangChainOpenAIPrompter</span><span class="p">(</span><span class="s2">&quot;your-api-key&quot;</span><span class="p">)</span>

<span class="c1"># Template example</span>
<span class="n">jetson_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a Jetson AI expert. Explain </span><span class="si">{concept}</span><span class="s2"> for </span><span class="si">{audience}</span><span class="s2"> level.</span>
<span class="s2">Focus on </span><span class="si">{platform}</span><span class="s2"> platform specifics.</span>
<span class="s2">Provide </span><span class="si">{num_examples}</span><span class="s2"> practical examples.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">langchain_openai</span><span class="o">.</span><span class="n">template_prompt</span><span class="p">(</span>
    <span class="n">jetson_template</span><span class="p">,</span>
    <span class="n">concept</span><span class="o">=</span><span class="s2">&quot;TensorRT optimization&quot;</span><span class="p">,</span>
    <span class="n">audience</span><span class="o">=</span><span class="s2">&quot;intermediate&quot;</span><span class="p">,</span>
    <span class="n">platform</span><span class="o">=</span><span class="s2">&quot;Jetson Orin Nano&quot;</span><span class="p">,</span>
    <span class="n">num_examples</span><span class="o">=</span><span class="s2">&quot;3&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<h3 id="langchain-with-local-ollama">üîπ LangChain with Local Ollama<a class="headerlink" href="#langchain-with-local-ollama" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LangChainOllamaPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;llama3.1:8b&quot;</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:11434&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOllama</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">is_available</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if Ollama is running&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/api/tags&quot;</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">simple_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple prompting with optional system message&quot;&quot;&quot;</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">system_message</span><span class="p">:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">))</span>
        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">prompt</span><span class="p">))</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">streaming_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stream response for long outputs&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">prompt</span><span class="p">)]):</span>
            <span class="k">yield</span> <span class="n">chunk</span><span class="o">.</span><span class="n">content</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">batch_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process multiple prompts efficiently&quot;&quot;&quot;</span>
        <span class="n">messages_list</span> <span class="o">=</span> <span class="p">[[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">prompt</span><span class="p">)]</span> <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">messages_list</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">content</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">]</span>

<span class="c1"># Example usage</span>
<span class="n">langchain_ollama</span> <span class="o">=</span> <span class="n">LangChainOllamaPrompter</span><span class="p">()</span>

<span class="k">if</span> <span class="n">langchain_ollama</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="c1"># Streaming example</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üîÑ Streaming response:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">langchain_ollama</span><span class="o">.</span><span class="n">streaming_prompt</span><span class="p">(</span>
        <span class="s2">&quot;Explain how to optimize YOLO models for Jetson Orin in detail&quot;</span>
    <span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Batch processing</span>
    <span class="n">jetson_questions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;What is the difference between Jetson Nano and Orin?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;How to install PyTorch on Jetson?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Best practices for TensorRT optimization?&quot;</span>
    <span class="p">]</span>

    <span class="n">batch_responses</span> <span class="o">=</span> <span class="n">langchain_ollama</span><span class="o">.</span><span class="n">batch_prompts</span><span class="p">(</span><span class="n">jetson_questions</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jetson_questions</span><span class="p">,</span> <span class="n">batch_responses</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Q: </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;A: </span><span class="si">{</span><span class="n">a</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s2">...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This LangChain Ollama integration is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>compare<span class="w"> </span>--backends<span class="w"> </span>ollama
</code></pre></div></p>
</blockquote>
<h3 id="langchain-with-llama-cpp-python">üîπ LangChain with llama-cpp-python<a class="headerlink" href="#langchain-with-llama-cpp-python" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.llms</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaCpp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.callbacks.manager</span><span class="w"> </span><span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.callbacks.streaming_stdout</span><span class="w"> </span><span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LangChainLlamaCppPrompter</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">n_gpu_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">80</span><span class="p">):</span>
        <span class="c1"># Callback for streaming</span>
        <span class="n">callback_manager</span> <span class="o">=</span> <span class="n">CallbackManager</span><span class="p">([</span><span class="n">StreamingStdOutCallbackHandler</span><span class="p">()])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">LlamaCpp</span><span class="p">(</span>
            <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
            <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">simple_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">system_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple prompting with optional system message&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">system_message</span><span class="p">:</span>
            <span class="n">full_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;System: </span><span class="si">{</span><span class="n">system_message</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Human: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Assistant:&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Human: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Assistant:&quot;</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">full_prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">template_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Use prompt templates&quot;&quot;&quot;</span>
        <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
        <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">conversation_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Handle conversation format&quot;&quot;&quot;</span>
        <span class="n">conversation</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
            <span class="n">role</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;role&quot;</span><span class="p">,</span> <span class="s2">&quot;user&quot;</span><span class="p">)</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;system&quot;</span><span class="p">:</span>
                <span class="n">conversation</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;System: </span><span class="si">{</span><span class="n">content</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;user&quot;</span><span class="p">:</span>
                <span class="n">conversation</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Human: </span><span class="si">{</span><span class="n">content</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;assistant&quot;</span><span class="p">:</span>
                <span class="n">conversation</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Assistant: </span><span class="si">{</span><span class="n">content</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>

        <span class="n">conversation</span> <span class="o">+=</span> <span class="s2">&quot;Assistant:&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">conversation</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

<span class="c1"># Example usage (uncomment when model is available)</span>
<span class="c1"># langchain_llamacpp = LangChainLlamaCppPrompter(&quot;/path/to/model.gguf&quot;)</span>

<span class="c1"># Conversation example</span>
<span class="c1"># conversation = [</span>
<span class="c1">#     {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a Jetson AI expert.&quot;},</span>
<span class="c1">#     {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How do I optimize models for Jetson?&quot;},</span>
<span class="c1">#     {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;There are several key approaches...&quot;},</span>
<span class="c1">#     {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me more about TensorRT specifically.&quot;}</span>
<span class="c1"># ]</span>
<span class="c1"># </span>
<span class="c1"># response = langchain_llamacpp.conversation_prompt(conversation)</span>
<span class="c1"># print(response)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This LangChain LlamaCpp integration is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>compare<span class="w"> </span>--backends<span class="w"> </span>llamacpp<span class="w"> </span>--model_path<span class="w"> </span>/path/to/your/model.gguf
</code></pre></div></p>
</blockquote>
<h3 id="comparing-langchain-approaches">üîÑ Comparing LangChain Approaches<a class="headerlink" href="#comparing-langchain-approaches" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compare_langchain_backends</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare the same prompt across different LangChain backends&quot;&quot;&quot;</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Explain the key differences between these Jetson optimization techniques:</span>
<span class="s2">1. TensorRT optimization</span>
<span class="s2">2. CUDA kernel optimization</span>
<span class="s2">3. Memory management optimization</span>

<span class="s2">Provide practical examples for each.</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="n">backends</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;OpenAI (GPT-3.5)&quot;</span><span class="p">:</span> <span class="n">langchain_openai</span><span class="p">,</span>
        <span class="s2">&quot;Ollama (Local)&quot;</span><span class="p">:</span> <span class="n">langchain_ollama</span> <span class="k">if</span> <span class="n">langchain_ollama</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># &quot;LlamaCpp (Local)&quot;: langchain_llamacpp  # Uncomment if available</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üîÑ LangChain Backend Comparison&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">backend_name</span><span class="p">,</span> <span class="n">backend</span> <span class="ow">in</span> <span class="n">backends</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">‚ùå </span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s2">: Not available&quot;</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîç </span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">simple_prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response time: </span><span class="si">{</span><span class="n">end_time</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">()</span>

<span class="c1"># compare_langchain_backends()</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This LangChain backend comparison is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>compare
</code></pre></div></p>
</blockquote>
<hr />
<h2 id="structured-output-with-langchain">üèóÔ∏è Structured Output with LangChain<a class="headerlink" href="#structured-output-with-langchain" title="Permanent link">&para;</a></h2>
<p>LangChain excels at converting unstructured LLM responses into structured data using Pydantic models.</p>
<h3 id="basic-structured-output">üìã Basic Structured Output<a class="headerlink" href="#basic-structured-output" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonOptimizationPlan</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Structured plan for Jetson model optimization&quot;&quot;&quot;</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Name of the AI model&quot;</span><span class="p">)</span>
    <span class="n">current_performance</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Current FPS and latency metrics&quot;</span><span class="p">)</span>
    <span class="n">optimization_steps</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;List of optimization techniques to apply&quot;</span><span class="p">)</span>
    <span class="n">expected_improvement</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Expected performance gains&quot;</span><span class="p">)</span>
    <span class="n">estimated_time</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Time required for optimization&quot;</span><span class="p">)</span>
    <span class="n">difficulty_level</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Beginner, Intermediate, or Advanced&quot;</span><span class="p">)</span>
    <span class="n">required_tools</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Tools and libraries needed&quot;</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonDeviceComparison</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Structured comparison of Jetson devices&quot;&quot;&quot;</span>
    <span class="n">device_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Jetson device name&quot;</span><span class="p">)</span>
    <span class="n">gpu_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of GPU cores&quot;</span><span class="p">)</span>
    <span class="n">cpu_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of CPU cores&quot;</span><span class="p">)</span>
    <span class="n">ram_gb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;RAM in GB&quot;</span><span class="p">)</span>
    <span class="n">power_consumption</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Power consumption&quot;</span><span class="p">)</span>
    <span class="n">best_use_cases</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Recommended use cases&quot;</span><span class="p">)</span>
    <span class="n">price_range</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Approximate price range&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_structured_prompter</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a prompter that returns structured data&quot;&quot;&quot;</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">StructuredPrompter</span><span class="p">:</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_optimization_plan</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_description</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">JetsonOptimizationPlan</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Get a structured optimization plan&quot;&quot;&quot;</span>
            <span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">JetsonOptimizationPlan</span><span class="p">)</span>

            <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
                <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a Jetson optimization expert. Create a detailed optimization plan for the following model:</span>

<span class="s2">Model Description: </span><span class="si">{model_description}</span>

<span class="s2">Provide a comprehensive optimization strategy.</span>

<span class="si">{format_instructions}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>
                <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;model_description&quot;</span><span class="p">],</span>
                <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
            <span class="p">)</span>

            <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_description</span><span class="o">=</span><span class="n">model_description</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parsing error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Raw response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">None</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">compare_jetson_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">JetsonDeviceComparison</span><span class="p">]:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Get structured comparison of Jetson devices&quot;&quot;&quot;</span>
            <span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">JetsonDeviceComparison</span><span class="p">)</span>

            <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
                    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Provide detailed specifications and information about the </span><span class="si">{device}</span><span class="s2">.</span>

<span class="s2">Include technical specs, performance characteristics, and recommended use cases.</span>

<span class="si">{format_instructions}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>
                    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">],</span>
                    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
                <span class="p">)</span>

                <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">)</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="n">parsed_result</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parsed_result</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parsing error for </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">continue</span>

            <span class="k">return</span> <span class="n">results</span>

    <span class="k">return</span> <span class="n">StructuredPrompter</span>

<span class="c1"># Example usage</span>
<span class="n">StructuredPrompter</span> <span class="o">=</span> <span class="n">create_structured_prompter</span><span class="p">()</span>
<span class="n">structured_prompter</span> <span class="o">=</span> <span class="n">StructuredPrompter</span><span class="p">(</span><span class="n">langchain_openai</span><span class="o">.</span><span class="n">llm</span><span class="p">)</span>

<span class="c1"># Get optimization plan</span>
<span class="n">model_desc</span> <span class="o">=</span> <span class="s2">&quot;YOLOv8 object detection model, currently running at 15 FPS on Jetson Orin Nano, need to achieve 30 FPS for real-time application&quot;</span>
<span class="n">optimization_plan</span> <span class="o">=</span> <span class="n">structured_prompter</span><span class="o">.</span><span class="n">get_optimization_plan</span><span class="p">(</span><span class="n">model_desc</span><span class="p">)</span>

<span class="k">if</span> <span class="n">optimization_plan</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üèóÔ∏è Structured Optimization Plan:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model: </span><span class="si">{</span><span class="n">optimization_plan</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current Performance: </span><span class="si">{</span><span class="n">optimization_plan</span><span class="o">.</span><span class="n">current_performance</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Steps: </span><span class="si">{</span><span class="n">optimization_plan</span><span class="o">.</span><span class="n">optimization_steps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected Improvement: </span><span class="si">{</span><span class="n">optimization_plan</span><span class="o">.</span><span class="n">expected_improvement</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difficulty: </span><span class="si">{</span><span class="n">optimization_plan</span><span class="o">.</span><span class="n">difficulty_level</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare devices</span>
<span class="n">jetson_devices</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Jetson Nano&quot;</span><span class="p">,</span> <span class="s2">&quot;Jetson Orin Nano&quot;</span><span class="p">,</span> <span class="s2">&quot;Jetson AGX Orin&quot;</span><span class="p">]</span>
<span class="n">device_comparisons</span> <span class="o">=</span> <span class="n">structured_prompter</span><span class="o">.</span><span class="n">compare_jetson_devices</span><span class="p">(</span><span class="n">jetson_devices</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìä Device Comparison:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">device_comparisons</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">device_name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  GPU Cores: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">gpu_cores</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  RAM: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">ram_gb</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Power: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">power_consumption</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Use Cases: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">best_use_cases</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This Structured Output example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>structured
</code></pre></div></p>
</blockquote>
<h3 id="tool-calling-with-structured-output">üõ†Ô∏è Tool Calling with Structured Output<a class="headerlink" href="#tool-calling-with-structured-output" title="Permanent link">&para;</a></h3>
<p>Enable LLMs to call external tools and functions through structured prompting.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Literal</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ToolCall</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a tool call request&quot;&quot;&quot;</span>
    <span class="n">tool_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Name of the tool to call&quot;</span><span class="p">)</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Parameters for the tool&quot;</span><span class="p">)</span>
    <span class="n">reasoning</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Why this tool is needed&quot;</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonSystemInfo</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;System information tool result&quot;&quot;&quot;</span>
    <span class="n">gpu_memory_used</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;GPU memory usage in MB&quot;</span><span class="p">)</span>
    <span class="n">cpu_usage</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;CPU usage percentage&quot;</span><span class="p">)</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;System temperature in Celsius&quot;</span><span class="p">)</span>
    <span class="n">available_models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Available AI models&quot;</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ModelBenchmark</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model benchmarking tool result&quot;&quot;&quot;</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Name of the benchmarked model&quot;</span><span class="p">)</span>
    <span class="n">fps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Frames per second&quot;</span><span class="p">)</span>
    <span class="n">latency_ms</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Latency in milliseconds&quot;</span><span class="p">)</span>
    <span class="n">memory_usage_mb</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Memory usage in MB&quot;</span><span class="p">)</span>
    <span class="n">accuracy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Model accuracy if available&quot;</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonToolkit</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collection of Jetson-specific tools&quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_system_info</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">JetsonSystemInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current Jetson system information&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Simulate getting GPU memory (would use actual NVIDIA tools)</span>
            <span class="n">gpu_memory</span> <span class="o">=</span> <span class="mf">2048.5</span>  <span class="c1"># MB</span>
            <span class="n">cpu_usage</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">cpu_percent</span><span class="p">()</span>
            <span class="n">temperature</span> <span class="o">=</span> <span class="mf">45.2</span>  <span class="c1"># Would read from thermal sensors</span>

            <span class="c1"># Simulate available models</span>
            <span class="n">available_models</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;yolov8n.engine&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50.onnx&quot;</span><span class="p">,</span> <span class="s2">&quot;mobilenet.trt&quot;</span><span class="p">]</span>

            <span class="k">return</span> <span class="n">JetsonSystemInfo</span><span class="p">(</span>
                <span class="n">gpu_memory_used</span><span class="o">=</span><span class="n">gpu_memory</span><span class="p">,</span>
                <span class="n">cpu_usage</span><span class="o">=</span><span class="n">cpu_usage</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">available_models</span><span class="o">=</span><span class="n">available_models</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error getting system info: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">benchmark_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">input_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">ModelBenchmark</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Benchmark a model on Jetson&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Simulate benchmarking (would run actual inference)</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># Simulate results based on model type</span>
            <span class="k">if</span> <span class="s2">&quot;yolo&quot;</span> <span class="ow">in</span> <span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                <span class="n">fps</span> <span class="o">=</span> <span class="mf">28.5</span>
                <span class="n">latency</span> <span class="o">=</span> <span class="mf">35.1</span>
                <span class="n">memory</span> <span class="o">=</span> <span class="mf">1024.0</span>
            <span class="k">elif</span> <span class="s2">&quot;resnet&quot;</span> <span class="ow">in</span> <span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                <span class="n">fps</span> <span class="o">=</span> <span class="mf">45.2</span>
                <span class="n">latency</span> <span class="o">=</span> <span class="mf">22.1</span>
                <span class="n">memory</span> <span class="o">=</span> <span class="mf">512.0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fps</span> <span class="o">=</span> <span class="mf">60.0</span>
                <span class="n">latency</span> <span class="o">=</span> <span class="mf">16.7</span>
                <span class="n">memory</span> <span class="o">=</span> <span class="mf">256.0</span>

            <span class="k">return</span> <span class="n">ModelBenchmark</span><span class="p">(</span>
                <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">fps</span><span class="o">=</span><span class="n">fps</span><span class="p">,</span>
                <span class="n">latency_ms</span><span class="o">=</span><span class="n">latency</span><span class="p">,</span>
                <span class="n">memory_usage_mb</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
                <span class="n">accuracy</span><span class="o">=</span><span class="mf">0.85</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error benchmarking model: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">optimize_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_precision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fp16&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Optimize a model for Jetson deployment&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Simulate optimization process</span>
            <span class="n">optimization_result</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;success&quot;</span><span class="p">,</span>
                <span class="s2">&quot;original_size_mb&quot;</span><span class="p">:</span> <span class="mf">245.6</span><span class="p">,</span>
                <span class="s2">&quot;optimized_size_mb&quot;</span><span class="p">:</span> <span class="mf">123.2</span><span class="p">,</span>
                <span class="s2">&quot;speedup_factor&quot;</span><span class="p">:</span> <span class="mf">2.3</span><span class="p">,</span>
                <span class="s2">&quot;output_path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;/opt/models/optimized_</span><span class="si">{</span><span class="n">model_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="s2">&quot;optimization_time_seconds&quot;</span><span class="p">:</span> <span class="mf">45.2</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">optimization_result</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;error&quot;</span><span class="p">,</span> <span class="s2">&quot;message&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ToolCallingPrompter</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM prompter with tool calling capabilities&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toolkit</span> <span class="o">=</span> <span class="n">JetsonToolkit</span><span class="p">()</span>

        <span class="c1"># Define available tools</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">available_tools</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;get_system_info&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Get current Jetson system information including GPU memory, CPU usage, and temperature&quot;</span><span class="p">,</span>
                <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{},</span>
                <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="s2">&quot;JetsonSystemInfo object&quot;</span>
            <span class="p">},</span>
            <span class="s2">&quot;benchmark_model&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Benchmark an AI model on Jetson to measure performance&quot;</span><span class="p">,</span>
                <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;model_path&quot;</span><span class="p">:</span> <span class="s2">&quot;Path to the model file&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="s2">&quot;Input size as (height, width) tuple, default (640, 640)&quot;</span>
                <span class="p">},</span>
                <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="s2">&quot;ModelBenchmark object with FPS, latency, and memory usage&quot;</span>
            <span class="p">},</span>
            <span class="s2">&quot;optimize_model&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Optimize a model for Jetson deployment using TensorRT&quot;</span><span class="p">,</span>
                <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;model_path&quot;</span><span class="p">:</span> <span class="s2">&quot;Path to the model file&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;target_precision&quot;</span><span class="p">:</span> <span class="s2">&quot;Target precision: fp32, fp16, or int8&quot;</span>
                <span class="p">},</span>
                <span class="s2">&quot;returns&quot;</span><span class="p">:</span> <span class="s2">&quot;Dictionary with optimization results&quot;</span>
            <span class="p">}</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_tool_calling_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a prompt that enables tool calling&quot;&quot;&quot;</span>
        <span class="n">tools_description</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">available_tools</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a Jetson AI assistant with access to specialized tools. Analyze the user&#39;s request and determine if you need to call any tools to provide a complete answer.</span>

<span class="s2">Available Tools:</span>
<span class="si">{</span><span class="n">tools_description</span><span class="si">}</span>

<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">If you need to call tools, respond with a JSON object containing:</span>
<span class="se">{{</span>
<span class="s2">    &quot;needs_tools&quot;: true,</span>
<span class="s2">    &quot;tool_calls&quot;: [</span>
<span class="s2">        </span><span class="se">{{</span>
<span class="s2">            &quot;tool_name&quot;: &quot;tool_name&quot;,</span>
<span class="s2">            &quot;parameters&quot;: </span><span class="se">{{</span><span class="s2">&quot;param1&quot;: &quot;value1&quot;</span><span class="se">}}</span><span class="s2">,</span>
<span class="s2">            &quot;reasoning&quot;: &quot;Why this tool is needed&quot;</span>
<span class="s2">        </span><span class="se">}}</span>
<span class="s2">    ],</span>
<span class="s2">    &quot;response_plan&quot;: &quot;How you&#39;ll use the tool results to answer the user&quot;</span>
<span class="se">}}</span>

<span class="s2">If no tools are needed, respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;needs_tools&quot;: false,</span>
<span class="s2">    &quot;direct_response&quot;: &quot;Your direct answer to the user&quot;</span>
<span class="se">}}</span>

<span class="s2">Analyze the request and respond:</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">prompt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">execute_tool_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tool_call</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute a tool call and return the result&quot;&quot;&quot;</span>
        <span class="n">tool_name</span> <span class="o">=</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;tool_name&quot;</span><span class="p">]</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;parameters&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">tool_name</span> <span class="o">==</span> <span class="s2">&quot;get_system_info&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">toolkit</span><span class="o">.</span><span class="n">get_system_info</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">tool_name</span> <span class="o">==</span> <span class="s2">&quot;benchmark_model&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">toolkit</span><span class="o">.</span><span class="n">benchmark_model</span><span class="p">(</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">tool_name</span> <span class="o">==</span> <span class="s2">&quot;optimize_model&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">toolkit</span><span class="o">.</span><span class="n">optimize_model</span><span class="p">(</span><span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Unknown tool: </span><span class="si">{</span><span class="n">tool_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">process_request_with_tools</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process a user request, calling tools if needed&quot;&quot;&quot;</span>
        <span class="c1"># Step 1: Determine if tools are needed</span>
        <span class="n">tool_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_tool_calling_prompt</span><span class="p">(</span><span class="n">user_request</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">tool_prompt</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Parse the response</span>
            <span class="n">response_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;needs_tools&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;direct_response&quot;</span><span class="p">,</span> <span class="s2">&quot;No response provided&quot;</span><span class="p">)</span>

            <span class="c1"># Step 2: Execute tool calls</span>
            <span class="n">tool_results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tool_calls&quot;</span><span class="p">,</span> <span class="p">[]):</span>
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">execute_tool_call</span><span class="p">(</span><span class="n">tool_call</span><span class="p">)</span>
                <span class="n">tool_results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                    <span class="s2">&quot;tool_call&quot;</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">,</span>
                    <span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">result</span>
                <span class="p">})</span>

            <span class="c1"># Step 3: Generate final response with tool results</span>
            <span class="n">final_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">Tool Results:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_results</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">default</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Based on the tool results above, provide a comprehensive answer to the user&#39;s request. Include specific data from the tool results and practical recommendations.</span>

<span class="s2">Response:</span>
<span class="s2">&quot;&quot;&quot;</span>

            <span class="n">final_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">final_prompt</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">final_response</span>

        <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error parsing tool response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error processing request: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Example usage</span>
<span class="n">tool_prompter</span> <span class="o">=</span> <span class="n">ToolCallingPrompter</span><span class="p">(</span><span class="n">langchain_openai</span><span class="o">.</span><span class="n">llm</span><span class="p">)</span>

<span class="c1"># Test tool calling</span>
<span class="n">test_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;What&#39;s the current system status of my Jetson?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I want to benchmark my YOLOv8 model at /models/yolov8n.onnx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;How can I optimize my ResNet model for better performance?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Compare the performance of YOLOv8 vs MobileNet on Jetson&quot;</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üõ†Ô∏è Tool Calling Examples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">request</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_requests</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># Test first 2 requests</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. Request: </span><span class="si">{</span><span class="n">request</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">tool_prompter</span><span class="o">.</span><span class="n">process_request_with_tools</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This Tool Calling example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>tools
</code></pre></div></p>
</blockquote>
<hr />
<h2 id="function-calling-and-mcp-protocol">üîß Function Calling and MCP Protocol<a class="headerlink" href="#function-calling-and-mcp-protocol" title="Permanent link">&para;</a></h2>
<h3 id="function-calling-with-langchain">üéØ Function Calling with LangChain<a class="headerlink" href="#function-calling-with-langchain" title="Permanent link">&para;</a></h3>
<p>Function calling allows LLMs to execute specific functions based on natural language requests.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">inspect</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">wraps</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FunctionRegistry</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Registry for callable functions with automatic documentation&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">functions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">function_docs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Decorator to register functions&quot;&quot;&quot;</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="n">func_name</span> <span class="o">=</span> <span class="n">name</span> <span class="ow">or</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>

            <span class="c1"># Extract function signature and docstring</span>
            <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">description</span> <span class="ow">or</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span> <span class="ow">or</span> <span class="s2">&quot;No description available&quot;</span>

            <span class="c1"># Build parameter documentation</span>
            <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">params</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">annotation</span><span class="p">)</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span> <span class="k">else</span> <span class="s2">&quot;Any&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="n">param</span><span class="o">.</span><span class="n">default</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">default</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="n">param</span><span class="o">.</span><span class="n">default</span> <span class="o">==</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span>
                <span class="p">}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">functions</span><span class="p">[</span><span class="n">func_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">func</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">function_docs</span><span class="p">[</span><span class="n">func_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="n">doc</span><span class="p">,</span>
                <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span>
                <span class="s2">&quot;return_type&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span><span class="p">)</span> <span class="k">if</span> <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span> <span class="k">else</span> <span class="s2">&quot;Any&quot;</span>
            <span class="p">}</span>

            <span class="k">return</span> <span class="n">func</span>
        <span class="k">return</span> <span class="n">decorator</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_function_schema</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get JSON schema of all registered functions&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function_docs</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call a registered function with parameters&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">functions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Function &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; not found&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">functions</span><span class="p">[</span><span class="n">name</span><span class="p">](</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Function call failed: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

<span class="c1"># Create function registry</span>
<span class="n">jetson_functions</span> <span class="o">=</span> <span class="n">FunctionRegistry</span><span class="p">()</span>

<span class="c1"># Register Jetson-specific functions</span>
<span class="nd">@jetson_functions</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Monitor Jetson system resources and performance metrics&quot;</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">monitor_jetson_resources</span><span class="p">(</span><span class="n">duration_seconds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monitor Jetson system resources for specified duration&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

    <span class="c1"># Simulate monitoring (would use actual system calls)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;monitoring_duration&quot;</span><span class="p">:</span> <span class="n">duration_seconds</span><span class="p">,</span>
        <span class="s2">&quot;average_cpu_usage&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;peak_gpu_memory_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;average_temperature_c&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">35</span><span class="p">,</span> <span class="mi">65</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;power_consumption_w&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;thermal_throttling_events&quot;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">metrics</span>

<span class="nd">@jetson_functions</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Deploy and test an AI model on Jetson with performance analysis&quot;</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">deploy_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">test_images</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">precision</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fp16&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deploy model and run performance tests&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

    <span class="c1"># Simulate deployment process</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Simulate deployment time</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model_path&quot;</span><span class="p">:</span> <span class="n">model_path</span><span class="p">,</span>
        <span class="s2">&quot;deployment_status&quot;</span><span class="p">:</span> <span class="s2">&quot;success&quot;</span><span class="p">,</span>
        <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
        <span class="s2">&quot;test_images_processed&quot;</span><span class="p">:</span> <span class="n">test_images</span><span class="p">,</span>
        <span class="s2">&quot;average_fps&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;average_latency_ms&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;memory_usage_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;accuracy_score&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s2">&quot;deployment_time_seconds&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="nd">@jetson_functions</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Compare multiple AI models on Jetson platform&quot;</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_models</span><span class="p">(</span><span class="n">model_paths</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;coco_val&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare performance of multiple models&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

    <span class="n">comparisons</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">model_path</span> <span class="ow">in</span> <span class="n">model_paths</span><span class="p">:</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">comparison</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
            <span class="s2">&quot;fps&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
            <span class="s2">&quot;latency_ms&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
            <span class="s2">&quot;memory_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">1500</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
            <span class="s2">&quot;power_efficiency&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># FPS per Watt</span>
        <span class="p">}</span>
        <span class="n">comparisons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">comparison</span><span class="p">)</span>

    <span class="c1"># Find best model for each metric</span>
    <span class="n">best_fps</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">comparisons</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;fps&#39;</span><span class="p">])</span>
    <span class="n">best_accuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">comparisons</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">best_efficiency</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">comparisons</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;power_efficiency&#39;</span><span class="p">])</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;test_dataset&quot;</span><span class="p">:</span> <span class="n">test_dataset</span><span class="p">,</span>
        <span class="s2">&quot;models_compared&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_paths</span><span class="p">),</span>
        <span class="s2">&quot;detailed_results&quot;</span><span class="p">:</span> <span class="n">comparisons</span><span class="p">,</span>
        <span class="s2">&quot;recommendations&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;best_for_speed&quot;</span><span class="p">:</span> <span class="n">best_fps</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">],</span>
            <span class="s2">&quot;best_for_accuracy&quot;</span><span class="p">:</span> <span class="n">best_accuracy</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">],</span>
            <span class="s2">&quot;best_for_efficiency&quot;</span><span class="p">:</span> <span class="n">best_efficiency</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FunctionCallingPrompter</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM prompter with function calling capabilities&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">function_registry</span><span class="p">:</span> <span class="n">FunctionRegistry</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">registry</span> <span class="o">=</span> <span class="n">function_registry</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_function_calling_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create prompt for function calling&quot;&quot;&quot;</span>
        <span class="n">function_schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">get_function_schema</span><span class="p">()</span>

        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a Jetson AI assistant with access to specialized functions. Analyze the user&#39;s request and determine which functions to call.</span>

<span class="s2">Available Functions:</span>
<span class="si">{</span><span class="n">function_schema</span><span class="si">}</span>

<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">If you need to call functions, respond with a JSON object:</span>
<span class="se">{{</span>
<span class="s2">    &quot;needs_functions&quot;: true,</span>
<span class="s2">    &quot;function_calls&quot;: [</span>
<span class="s2">        </span><span class="se">{{</span>
<span class="s2">            &quot;function_name&quot;: &quot;function_name&quot;,</span>
<span class="s2">            &quot;parameters&quot;: </span><span class="se">{{</span><span class="s2">&quot;param1&quot;: &quot;value1&quot;</span><span class="se">}}</span><span class="s2">,</span>
<span class="s2">            &quot;reasoning&quot;: &quot;Why this function is needed&quot;</span>
<span class="s2">        </span><span class="se">}}</span>
<span class="s2">    ],</span>
<span class="s2">    &quot;execution_plan&quot;: &quot;How you&#39;ll use the function results&quot;</span>
<span class="se">}}</span>

<span class="s2">If no functions are needed, respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;needs_functions&quot;: false,</span>
<span class="s2">    &quot;direct_response&quot;: &quot;Your direct answer&quot;</span>
<span class="se">}}</span>

<span class="s2">Analyze and respond:</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">prompt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">execute_function_calls</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">function_calls</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute multiple function calls&quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">call</span> <span class="ow">in</span> <span class="n">function_calls</span><span class="p">:</span>
            <span class="n">func_name</span> <span class="o">=</span> <span class="n">call</span><span class="p">[</span><span class="s2">&quot;function_name&quot;</span><span class="p">]</span>
            <span class="n">parameters</span> <span class="o">=</span> <span class="n">call</span><span class="p">[</span><span class="s2">&quot;parameters&quot;</span><span class="p">]</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">call_function</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="o">**</span><span class="n">parameters</span><span class="p">)</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                    <span class="s2">&quot;function_call&quot;</span><span class="p">:</span> <span class="n">call</span><span class="p">,</span>
                    <span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
                    <span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;success&quot;</span>
                <span class="p">})</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                    <span class="s2">&quot;function_call&quot;</span><span class="p">:</span> <span class="n">call</span><span class="p">,</span>
                    <span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                    <span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;error&quot;</span>
                <span class="p">})</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">process_with_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process request with function calling&quot;&quot;&quot;</span>
        <span class="c1"># Step 1: Determine function calls</span>
        <span class="n">function_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_function_calling_prompt</span><span class="p">(</span><span class="n">user_request</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">function_prompt</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">response_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;needs_functions&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;direct_response&quot;</span><span class="p">,</span> <span class="s2">&quot;No response provided&quot;</span><span class="p">)</span>

            <span class="c1"># Step 2: Execute functions</span>
            <span class="n">function_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">execute_function_calls</span><span class="p">(</span>
                <span class="n">response_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;function_calls&quot;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="p">)</span>

            <span class="c1"># Step 3: Generate final response</span>
            <span class="n">final_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">Function Execution Results:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">function_results</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">default</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Based on the function results, provide a comprehensive answer to the user&#39;s request. Include specific data, insights, and actionable recommendations.</span>

<span class="s2">Response:</span>
<span class="s2">&quot;&quot;&quot;</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">final_prompt</span><span class="p">)</span>

        <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error parsing function response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error processing request: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Example usage</span>
<span class="n">function_prompter</span> <span class="o">=</span> <span class="n">FunctionCallingPrompter</span><span class="p">(</span><span class="n">langchain_openai</span><span class="o">.</span><span class="n">llm</span><span class="p">,</span> <span class="n">jetson_functions</span><span class="p">)</span>

<span class="c1"># Test function calling</span>
<span class="n">function_test_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Monitor my Jetson system for 30 seconds and tell me about performance&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Deploy my YOLOv8 model at /models/yolov8s.onnx and test it with 50 images&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Compare these models: [&#39;/models/yolo.onnx&#39;, &#39;/models/resnet.trt&#39;, &#39;/models/mobilenet.engine&#39;]&quot;</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üîß Function Calling Examples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">request</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">function_test_requests</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. Request: </span><span class="si">{</span><span class="n">request</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">function_prompter</span><span class="o">.</span><span class="n">process_with_functions</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This Function Calling example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>functions
</code></pre></div></p>
</blockquote>
<h3 id="model-context-protocol-mcp">üåê Model Context Protocol (MCP)<a class="headerlink" href="#model-context-protocol-mcp" title="Permanent link">&para;</a></h3>
<p>MCP is a new standard for connecting LLMs to external tools and data sources. Here's how it works from a prompt engineering perspective:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MCPServer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simplified MCP server implementation for Jetson tools&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resources</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_tool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">handler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register a tool with MCP server&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="n">description</span><span class="p">,</span>
            <span class="s2">&quot;handler&quot;</span><span class="p">:</span> <span class="n">handler</span><span class="p">,</span>
            <span class="s2">&quot;inputSchema&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_schema</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_resource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">content_provider</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register a resource (data source)&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resources</span><span class="p">[</span><span class="n">uri</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;uri&quot;</span><span class="p">:</span> <span class="n">uri</span><span class="p">,</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="n">description</span><span class="p">,</span>
            <span class="s2">&quot;provider&quot;</span><span class="p">:</span> <span class="n">content_provider</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">template</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register a prompt template&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="n">description</span><span class="p">,</span>
            <span class="s2">&quot;template&quot;</span><span class="p">:</span> <span class="n">template</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_schema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract JSON schema from function signature&quot;&quot;&quot;</span>
        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">handler</span><span class="p">)</span>
        <span class="n">properties</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">required</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
                <span class="n">param_type</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">annotation</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;class &#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="n">properties</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">param_type</span><span class="p">}</span>

                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">default</span> <span class="o">==</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
                    <span class="n">required</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
            <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="n">properties</span><span class="p">,</span>
            <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="n">required</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">list_tools</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List available tools&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;tools&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">values</span><span class="p">())}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call_tool</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">arguments</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call a tool with arguments&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Tool &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; not found&quot;</span><span class="p">}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="s2">&quot;handler&quot;</span><span class="p">](</span><span class="o">**</span><span class="n">arguments</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)}]}</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">arguments</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get a prompt template with arguments&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Prompt &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; not found&quot;</span><span class="p">}</span>

        <span class="n">template</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="p">[</span><span class="n">name</span><span class="p">][</span><span class="s2">&quot;template&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">arguments</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">formatted</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">arguments</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">formatted</span><span class="p">}}]}</span>
            <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Missing argument: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">template</span><span class="p">}}]}</span>

<span class="c1"># Create MCP server for Jetson</span>
<span class="n">jetson_mcp</span> <span class="o">=</span> <span class="n">MCPServer</span><span class="p">()</span>

<span class="c1"># Register tools</span>
<span class="k">def</span><span class="w"> </span><span class="nf">jetson_system_status</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get comprehensive Jetson system status&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;gpu_utilization&quot;</span><span class="p">:</span> <span class="mf">75.2</span><span class="p">,</span>
        <span class="s2">&quot;memory_usage_gb&quot;</span><span class="p">:</span> <span class="mf">6.4</span><span class="p">,</span>
        <span class="s2">&quot;temperature_c&quot;</span><span class="p">:</span> <span class="mf">52.1</span><span class="p">,</span>
        <span class="s2">&quot;power_draw_w&quot;</span><span class="p">:</span> <span class="mf">18.5</span><span class="p">,</span>
        <span class="s2">&quot;cpu_frequency_mhz&quot;</span><span class="p">:</span> <span class="mi">1900</span><span class="p">,</span>
        <span class="s2">&quot;gpu_frequency_mhz&quot;</span><span class="p">:</span> <span class="mi">1300</span>
    <span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">optimize_inference_pipeline</span><span class="p">(</span><span class="n">model_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_fps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimize inference pipeline for target performance&quot;&quot;&quot;</span>
    <span class="n">optimizations</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;yolo&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;TensorRT FP16&quot;</span><span class="p">,</span> <span class="s2">&quot;Dynamic batching&quot;</span><span class="p">,</span> <span class="s2">&quot;CUDA streams&quot;</span><span class="p">],</span>
        <span class="s2">&quot;resnet&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;TensorRT INT8&quot;</span><span class="p">,</span> <span class="s2">&quot;Layer fusion&quot;</span><span class="p">,</span> <span class="s2">&quot;Memory pooling&quot;</span><span class="p">],</span>
        <span class="s2">&quot;transformer&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Flash attention&quot;</span><span class="p">,</span> <span class="s2">&quot;KV cache optimization&quot;</span><span class="p">,</span> <span class="s2">&quot;Quantization&quot;</span><span class="p">]</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="n">model_type</span><span class="p">,</span>
        <span class="s2">&quot;target_fps&quot;</span><span class="p">:</span> <span class="n">target_fps</span><span class="p">,</span>
        <span class="s2">&quot;recommended_optimizations&quot;</span><span class="p">:</span> <span class="n">optimizations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;General TensorRT optimization&quot;</span><span class="p">]),</span>
        <span class="s2">&quot;estimated_speedup&quot;</span><span class="p">:</span> <span class="s2">&quot;2.5x&quot;</span><span class="p">,</span>
        <span class="s2">&quot;implementation_complexity&quot;</span><span class="p">:</span> <span class="s2">&quot;Medium&quot;</span>
    <span class="p">}</span>

<span class="n">jetson_mcp</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;jetson_status&quot;</span><span class="p">,</span> <span class="s2">&quot;Get current Jetson system status&quot;</span><span class="p">,</span> <span class="n">jetson_system_status</span><span class="p">)</span>
<span class="n">jetson_mcp</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;optimize_pipeline&quot;</span><span class="p">,</span> <span class="s2">&quot;Optimize inference pipeline&quot;</span><span class="p">,</span> <span class="n">optimize_inference_pipeline</span><span class="p">)</span>

<span class="c1"># Register prompt templates</span>
<span class="n">jetson_mcp</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
    <span class="s2">&quot;model_optimization&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Generate optimization plan for AI model&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">You are a Jetson optimization expert. Create a detailed optimization plan for:</span>

<span class="sd">Model: {model_name}</span>
<span class="sd">Current Performance: {current_fps} FPS</span>
<span class="sd">Target Performance: {target_fps} FPS</span>
<span class="sd">Platform: {jetson_model}</span>

<span class="sd">Provide:</span>
<span class="sd">1. Specific optimization techniques</span>
<span class="sd">2. Expected performance gains</span>
<span class="sd">3. Implementation steps</span>
<span class="sd">4. Potential trade-offs</span>

<span class="sd">Optimization Plan:</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="n">jetson_mcp</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
    <span class="s2">&quot;deployment_checklist&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Generate deployment checklist for Jetson AI application&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Create a comprehensive deployment checklist for:</span>

<span class="sd">Application: {app_name}</span>
<span class="sd">Model: {model_type}</span>
<span class="sd">Target Device: {device_type}</span>
<span class="sd">Performance Requirements: {requirements}</span>

<span class="sd">Include:</span>
<span class="sd">- Pre-deployment testing</span>
<span class="sd">- Performance validation</span>
<span class="sd">- Monitoring setup</span>
<span class="sd">- Troubleshooting steps</span>

<span class="sd">Deployment Checklist:</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MCPPrompter</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLM prompter with MCP integration&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">mcp_server</span><span class="p">:</span> <span class="n">MCPServer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mcp</span> <span class="o">=</span> <span class="n">mcp_server</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">process_with_mcp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process request using MCP tools and prompts&quot;&quot;&quot;</span>
        <span class="c1"># Get available tools</span>
        <span class="n">tools</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mcp</span><span class="o">.</span><span class="n">list_tools</span><span class="p">()</span>

        <span class="c1"># Create MCP-aware prompt</span>
        <span class="n">mcp_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You have access to MCP tools and prompts. Analyze the user request and determine the best approach.</span>

<span class="s2">Available Tools:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tools</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span>

<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">If you need to use MCP tools, respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;action&quot;: &quot;use_tools&quot;,</span>
<span class="s2">    &quot;tools_needed&quot;: [</span>
<span class="s2">        </span><span class="se">{{</span><span class="s2">&quot;tool_name&quot;: &quot;tool_name&quot;, &quot;arguments&quot;: </span><span class="se">{{</span><span class="s2">&quot;arg1&quot;: &quot;value1&quot;</span><span class="se">}}}}</span>
<span class="s2">    ]</span>
<span class="se">}}</span>

<span class="s2">If you need a specific prompt template, respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;action&quot;: &quot;use_prompt&quot;,</span>
<span class="s2">    &quot;prompt_name&quot;: &quot;prompt_name&quot;,</span>
<span class="s2">    &quot;arguments&quot;: </span><span class="se">{{</span><span class="s2">&quot;arg1&quot;: &quot;value1&quot;</span><span class="se">}}</span>
<span class="se">}}</span>

<span class="s2">If you can answer directly, respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;action&quot;: &quot;direct_response&quot;,</span>
<span class="s2">    &quot;response&quot;: &quot;Your answer&quot;</span>
<span class="se">}}</span>

<span class="s2">Analyze and respond:</span>
<span class="s2">&quot;&quot;&quot;</span>

        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">mcp_prompt</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">action_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">action_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;action&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="s2">&quot;use_tools&quot;</span><span class="p">:</span>
                <span class="c1"># Execute MCP tools</span>
                <span class="n">tool_results</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">action_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tools_needed&quot;</span><span class="p">,</span> <span class="p">[]):</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mcp</span><span class="o">.</span><span class="n">call_tool</span><span class="p">(</span>
                        <span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;tool_name&quot;</span><span class="p">],</span>
                        <span class="n">tool_call</span><span class="p">[</span><span class="s2">&quot;arguments&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">tool_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

                <span class="c1"># Generate response with tool results</span>
                <span class="n">final_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">MCP Tool Results:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_results</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Provide a comprehensive response based on the tool results:</span>
<span class="s2">&quot;&quot;&quot;</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">final_prompt</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s2">&quot;use_prompt&quot;</span><span class="p">:</span>
                <span class="c1"># Use MCP prompt template</span>
                <span class="n">prompt_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mcp</span><span class="o">.</span><span class="n">get_prompt</span><span class="p">(</span>
                    <span class="n">action_data</span><span class="p">[</span><span class="s2">&quot;prompt_name&quot;</span><span class="p">],</span>
                    <span class="n">action_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;arguments&quot;</span><span class="p">,</span> <span class="p">{})</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">in</span> <span class="n">prompt_result</span><span class="p">:</span>
                    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error using prompt: </span><span class="si">{</span><span class="n">prompt_result</span><span class="p">[</span><span class="s1">&#39;error&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>

                <span class="c1"># Execute the prompt</span>
                <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_result</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s2">&quot;direct_response&quot;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">action_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="s2">&quot;No response provided&quot;</span><span class="p">)</span>

        <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error parsing MCP response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error processing MCP request: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Example usage</span>
<span class="n">mcp_prompter</span> <span class="o">=</span> <span class="n">MCPPrompter</span><span class="p">(</span><span class="n">langchain_openai</span><span class="o">.</span><span class="n">llm</span><span class="p">,</span> <span class="n">jetson_mcp</span><span class="p">)</span>

<span class="c1"># Test MCP integration</span>
<span class="n">mcp_test_requests</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;What&#39;s my current Jetson system status?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Create an optimization plan for my YOLOv8 model running at 15 FPS, targeting 30 FPS on Jetson Orin Nano&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Generate a deployment checklist for my object detection application&quot;</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üåê MCP Integration Examples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">request</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mcp_test_requests</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. Request: </span><span class="si">{</span><span class="n">request</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">mcp_prompter</span><span class="o">.</span><span class="n">process_with_mcp</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> This MCP integration example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>mcp
</code></pre></div></p>
</blockquote>
<hr />
<h2 id="lab-advanced-prompt-engineering-on-jetson-with-langchain">üß™ Lab: Advanced Prompt Engineering on Jetson with LangChain<a class="headerlink" href="#lab-advanced-prompt-engineering-on-jetson-with-langchain" title="Permanent link">&para;</a></h2>
<h3 id="lab-objectives">üéØ Lab Objectives<a class="headerlink" href="#lab-objectives" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Master Core Techniques</strong>: Implement and compare different prompt engineering approaches</li>
<li><strong>Build Tool Integration</strong>: Create LLM systems that can call external tools and functions</li>
<li><strong>Develop MCP Applications</strong>: Build applications using the Model Context Protocol</li>
<li><strong>Optimize for Jetson</strong>: Apply prompt engineering specifically for edge AI scenarios</li>
</ol>
<h3 id="lab-setup">üõ†Ô∏è Lab Setup<a class="headerlink" href="#lab-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Install required packages</span>
pip<span class="w"> </span>install<span class="w"> </span>langchain<span class="w"> </span>langchain-openai<span class="w"> </span>langchain-community
pip<span class="w"> </span>install<span class="w"> </span>ollama<span class="w"> </span>llama-cpp-python
pip<span class="w"> </span>install<span class="w"> </span>pydantic<span class="w"> </span>psutil

<span class="c1"># Start Ollama (if using local models)</span>
ollama<span class="w"> </span>serve
ollama<span class="w"> </span>pull<span class="w"> </span>llama3.2:3b
</code></pre></div>
<h3 id="exercise-1-prompt-engineering-comparison">üìã Exercise 1: Prompt Engineering Comparison<a class="headerlink" href="#exercise-1-prompt-engineering-comparison" title="Permanent link">&para;</a></h3>
<p>Create a comprehensive comparison system for different prompting techniques:</p>
<p><div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PromptType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">BASIC</span> <span class="o">=</span> <span class="s2">&quot;basic&quot;</span>
    <span class="n">CHAIN_OF_THOUGHT</span> <span class="o">=</span> <span class="s2">&quot;chain_of_thought&quot;</span>
    <span class="n">FEW_SHOT</span> <span class="o">=</span> <span class="s2">&quot;few_shot&quot;</span>
    <span class="n">ROLE_BASED</span> <span class="o">=</span> <span class="s2">&quot;role_based&quot;</span>
    <span class="n">IN_CONTEXT_LEARNING</span> <span class="o">=</span> <span class="s2">&quot;in_context_learning&quot;</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PromptResult</span><span class="p">:</span>
    <span class="n">prompt_type</span><span class="p">:</span> <span class="n">PromptType</span>
    <span class="n">response</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">response_time</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">token_count</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">quality_score</span><span class="p">:</span> <span class="nb">float</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PromptEngineeringLab</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Comprehensive lab for testing prompt engineering techniques&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm_clients</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm_clients</span> <span class="o">=</span> <span class="n">llm_clients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">PromptType</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create different prompt variations for the same task&quot;&quot;&quot;</span>

        <span class="n">prompts</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">PromptType</span><span class="o">.</span><span class="n">BASIC</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">task</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>

            <span class="n">PromptType</span><span class="o">.</span><span class="n">CHAIN_OF_THOUGHT</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="si">{</span><span class="n">task</span><span class="si">}</span>

<span class="s2">Let&#39;s think step by step:</span>
<span class="s2">1. First, I need to understand the problem</span>
<span class="s2">2. Then, I&#39;ll analyze the requirements</span>
<span class="s2">3. Next, I&#39;ll consider the constraints</span>
<span class="s2">4. Finally, I&#39;ll provide a comprehensive solution</span>

<span class="s2">Step-by-step analysis:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>

            <span class="n">PromptType</span><span class="o">.</span><span class="n">FEW_SHOT</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Here are some examples of similar tasks:</span>

<span class="s2">Example 1:</span>
<span class="s2">Task: Optimize a CNN model for mobile deployment</span>
<span class="s2">Solution: Use quantization, pruning, and knowledge distillation</span>

<span class="s2">Example 2:</span>
<span class="s2">Task: Reduce inference latency for object detection</span>
<span class="s2">Solution: Use TensorRT, optimize batch size, and implement async processing</span>

<span class="s2">Now solve this task:</span>
<span class="si">{</span><span class="n">task</span><span class="si">}</span>

<span class="s2">Solution:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>

            <span class="n">PromptType</span><span class="o">.</span><span class="n">ROLE_BASED</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a senior NVIDIA Jetson optimization engineer with 10+ years of experience in edge AI deployment. You specialize in maximizing performance while minimizing power consumption.</span>

<span class="s2">Task: </span><span class="si">{</span><span class="n">task</span><span class="si">}</span>

<span class="s2">As an expert, provide your professional recommendation:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>

            <span class="n">PromptType</span><span class="o">.</span><span class="n">IN_CONTEXT_LEARNING</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">I&#39;ll teach you a new format for Jetson optimization reports:</span>

<span class="s2">Format:</span>
</code></pre></div>
OPTIMIZATION REPORT
==================
Model: [model_name]
Current Performance: [fps] FPS, [latency]ms latency
Target Performance: [target_fps] FPS</p>
<p>Optimization Strategy:
- Technique 1: [description] -&gt; Expected gain: [X]%
- Technique 2: [description] -&gt; Expected gain: [Y]%</p>
<p>Implementation Priority:
1. [High priority item]
2. [Medium priority item]
3. [Low priority item]</p>
<p>Risk Assessment: [Low/Medium/High]
Estimated Timeline: [X] days
<div class="highlight"><pre><span></span><code>Now use this format to solve:
{task}
&quot;&quot;&quot;
        }

        return prompts

    def evaluate_response(self, response: str, expected_keywords: List[str]) -&gt; float:
        &quot;&quot;&quot;Simple quality evaluation based on keyword presence&quot;&quot;&quot;
        score = 0.0
        response_lower = response.lower()

        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                score += 1.0

        return min(score / len(expected_keywords), 1.0) if expected_keywords else 0.5

    def run_prompt_comparison(self, task: str, expected_keywords: List[str]) -&gt; List[PromptResult]:
        &quot;&quot;&quot;Run the same task with different prompting techniques&quot;&quot;&quot;
        prompts = self.create_prompts(task)
        results = []

        for prompt_type, prompt_text in prompts.items():
            print(f&quot;\nTesting {prompt_type.value} prompting...&quot;)

            for llm_name, llm in self.llm_clients.items():
                start_time = time.time()

                try:
                    response = llm.invoke(prompt_text)
                    response_time = time.time() - start_time

                    # Estimate token count (rough approximation)
                    token_count = len(response.split()) * 1.3

                    # Evaluate quality
                    quality_score = self.evaluate_response(response, expected_keywords)

                    result = PromptResult(
                        prompt_type=prompt_type,
                        response=response,
                        response_time=response_time,
                        token_count=int(token_count),
                        quality_score=quality_score
                    )

                    results.append(result)
                    print(f&quot;  {llm_name}: {quality_score:.2f} quality, {response_time:.2f}s&quot;)

                except Exception as e:
                    print(f&quot;  {llm_name}: Error - {e}&quot;)

        return results

    def generate_comparison_report(self, results: List[PromptResult]) -&gt; str:
        &quot;&quot;&quot;Generate a comprehensive comparison report&quot;&quot;&quot;
        report = &quot;\n&quot; + &quot;=&quot; * 60
        report += &quot;\nPROMPT ENGINEERING COMPARISON REPORT\n&quot;
        report += &quot;=&quot; * 60 + &quot;\n&quot;

        # Group by prompt type
        by_type = {}
        for result in results:
            if result.prompt_type not in by_type:
                by_type[result.prompt_type] = []
            by_type[result.prompt_type].append(result)

        # Calculate averages
        for prompt_type, type_results in by_type.items():
            avg_quality = sum(r.quality_score for r in type_results) / len(type_results)
            avg_time = sum(r.response_time for r in type_results) / len(type_results)
            avg_tokens = sum(r.token_count for r in type_results) / len(type_results)

            report += f&quot;\n{prompt_type.value.upper()}:\n&quot;
            report += f&quot;  Average Quality: {avg_quality:.3f}\n&quot;
            report += f&quot;  Average Time: {avg_time:.2f}s\n&quot;
            report += f&quot;  Average Tokens: {avg_tokens:.0f}\n&quot;

        # Find best performing technique
        best_quality = max(results, key=lambda r: r.quality_score)
        fastest = min(results, key=lambda r: r.response_time)

        report += f&quot;\nBEST PERFORMERS:\n&quot;
        report += f&quot;  Highest Quality: {best_quality.prompt_type.value} ({best_quality.quality_score:.3f})\n&quot;
        report += f&quot;  Fastest Response: {fastest.prompt_type.value} ({fastest.response_time:.2f}s)\n&quot;

        return report

# Initialize lab
lab = PromptEngineeringLab({
    &quot;openai&quot;: langchain_openai.llm,
    &quot;ollama&quot;: langchain_ollama.llm,
    &quot;llamacpp&quot;: langchain_llamacpp.llm
})

# Test scenarios
test_scenarios = [
    {
        &quot;task&quot;: &quot;How can I optimize a YOLOv8 model running at 15 FPS to achieve 30 FPS on Jetson Orin Nano?&quot;,
        &quot;keywords&quot;: [&quot;tensorrt&quot;, &quot;quantization&quot;, &quot;batch&quot;, &quot;optimization&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;engine&quot;]
    },
    {
        &quot;task&quot;: &quot;What&#39;s the best approach to deploy multiple AI models simultaneously on Jetson while maintaining real-time performance?&quot;,
        &quot;keywords&quot;: [&quot;pipeline&quot;, &quot;scheduling&quot;, &quot;memory&quot;, &quot;concurrent&quot;, &quot;optimization&quot;, &quot;resource&quot;]
    }
]

print(&quot;üß™ Starting Prompt Engineering Lab...&quot;)
print(&quot;=&quot; * 50)

for i, scenario in enumerate(test_scenarios, 1):
    print(f&quot;\nüìã Scenario {i}: {scenario[&#39;task&#39;][:50]}...&quot;)
    results = lab.run_prompt_comparison(scenario[&quot;task&quot;], scenario[&quot;keywords&quot;])
    report = lab.generate_comparison_report(results)
    print(report)
</code></pre></div></p>
<h3 id="exercise-2-advanced-tool-integration">üìã Exercise 2: Advanced Tool Integration<a class="headerlink" href="#exercise-2-advanced-tool-integration" title="Permanent link">&para;</a></h3>
<p>Build a comprehensive Jetson AI assistant with multiple tool capabilities:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">JetsonAIAssistant</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Advanced AI assistant for Jetson development&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conversation_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tool_registry</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_setup_tools</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mcp_server</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_setup_mcp</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_tools</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup comprehensive tool registry&quot;&quot;&quot;</span>
        <span class="n">tools</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;system_monitor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_monitor_system</span><span class="p">,</span>
            <span class="s2">&quot;model_benchmark&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_benchmark_model</span><span class="p">,</span>
            <span class="s2">&quot;optimization_advisor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_optimization_advice</span><span class="p">,</span>
            <span class="s2">&quot;deployment_validator&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_deployment</span><span class="p">,</span>
            <span class="s2">&quot;performance_predictor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_performance</span><span class="p">,</span>
            <span class="s2">&quot;resource_planner&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_plan_resources</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">tools</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_mcp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MCPServer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup MCP server with Jetson-specific capabilities&quot;&quot;&quot;</span>
        <span class="n">mcp</span> <span class="o">=</span> <span class="n">MCPServer</span><span class="p">()</span>

        <span class="c1"># Register advanced prompt templates</span>
        <span class="n">mcp</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
            <span class="s2">&quot;performance_analysis&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Analyze model performance and suggest improvements&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Perform a comprehensive performance analysis for:</span>

<span class="sd">Model: {model_name}</span>
<span class="sd">Current Metrics:</span>
<span class="sd">- FPS: {current_fps}</span>
<span class="sd">- Latency: {current_latency}ms</span>
<span class="sd">- Memory Usage: {memory_usage}MB</span>
<span class="sd">- Power Draw: {power_draw}W</span>

<span class="sd">Target Requirements:</span>
<span class="sd">- Minimum FPS: {target_fps}</span>
<span class="sd">- Maximum Latency: {max_latency}ms</span>
<span class="sd">- Memory Budget: {memory_budget}MB</span>
<span class="sd">- Power Budget: {power_budget}W</span>

<span class="sd">Platform: {platform}</span>
<span class="sd">Use Case: {use_case}</span>

<span class="sd">Provide:</span>
<span class="sd">1. Performance gap analysis</span>
<span class="sd">2. Bottleneck identification</span>
<span class="sd">3. Optimization roadmap with priorities</span>
<span class="sd">4. Risk assessment for each optimization</span>
<span class="sd">5. Expected timeline and resource requirements</span>

<span class="sd">Analysis:</span>
<span class="sd">&quot;&quot;&quot;</span>
        <span class="p">)</span>

        <span class="n">mcp</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
            <span class="s2">&quot;deployment_strategy&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Create deployment strategy for production&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Create a production deployment strategy for:</span>

<span class="sd">Application: {app_name}</span>
<span class="sd">Models: {models}</span>
<span class="sd">Target Devices: {devices}</span>
<span class="sd">Scale: {scale}</span>
<span class="sd">SLA Requirements: {sla}</span>

<span class="sd">Consider:</span>
<span class="sd">- Model versioning and updates</span>
<span class="sd">- A/B testing capabilities</span>
<span class="sd">- Monitoring and alerting</span>
<span class="sd">- Rollback procedures</span>
<span class="sd">- Performance optimization</span>
<span class="sd">- Security considerations</span>

<span class="sd">Deployment Strategy:</span>
<span class="sd">&quot;&quot;&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">mcp</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_monitor_system</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">duration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">60</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Advanced system monitoring&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

        <span class="c1"># Simulate comprehensive monitoring</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;monitoring_duration&quot;</span><span class="p">:</span> <span class="n">duration</span><span class="p">,</span>
            <span class="s2">&quot;system_health&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;cpu_usage_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;cpu_usage_peak&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;gpu_utilization_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">90</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;memory_usage_gb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;temperature_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">70</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;temperature_peak&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">85</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;power_consumption_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;thermal_throttling_events&quot;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;performance_metrics&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;inference_fps&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;latency_p50&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;latency_p95&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;memory_efficiency&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s2">&quot;gpu_memory_usage_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4000</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;alerts&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&quot;High temperature detected at 14:32&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Memory usage approaching limit&quot;</span>
            <span class="p">]</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.7</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_benchmark_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">test_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Comprehensive model benchmarking&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Simulate detailed benchmarking</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model_info&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
                <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="n">model_path</span><span class="p">,</span>
                <span class="s2">&quot;size_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="n">test_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="s2">&quot;fp16&quot;</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;performance&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;fps_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;fps_min&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;fps_max&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;latency_avg&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;latency_p95&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;throughput_imgs_sec&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;resource_usage&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;gpu_memory_mb&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">3000</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;cpu_usage_percent&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;power_draw_w&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">22</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
                <span class="s2">&quot;temperature_c&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;accuracy_metrics&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;map_50&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s2">&quot;map_75&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s2">&quot;recall&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="s2">&quot;optimization_suggestions&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s2">&quot;Consider TensorRT FP16 optimization for 2x speedup&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Batch processing could improve throughput by 30%&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Dynamic shapes optimization available&quot;</span>
            <span class="p">]</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">process_complex_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_request</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process complex requests using multiple tools and MCP&quot;&quot;&quot;</span>
        <span class="c1"># Add to conversation history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conversation_history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_request</span><span class="p">})</span>

        <span class="c1"># Create comprehensive analysis prompt</span>
        <span class="n">analysis_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are an advanced Jetson AI assistant with access to comprehensive tools and MCP capabilities.</span>

<span class="s2">Conversation History:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conversation_history</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:],</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Current Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">Available Tools:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tool_registry</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Analyze the request and create an execution plan. Respond with:</span>
<span class="se">{{</span>
<span class="s2">    &quot;analysis&quot;: &quot;Your understanding of the request&quot;,</span>
<span class="s2">    &quot;execution_plan&quot;: [</span>
<span class="s2">        </span><span class="se">{{</span>
<span class="s2">            &quot;step&quot;: 1,</span>
<span class="s2">            &quot;action&quot;: &quot;tool_call|mcp_prompt|direct_response&quot;,</span>
<span class="s2">            &quot;details&quot;: &quot;Specific action details&quot;,</span>
<span class="s2">            &quot;reasoning&quot;: &quot;Why this step is needed&quot;</span>
<span class="s2">        </span><span class="se">}}</span>
<span class="s2">    ],</span>
<span class="s2">    &quot;expected_outcome&quot;: &quot;What the user should expect&quot;</span>
<span class="se">}}</span>

<span class="s2">Analyze and plan:</span>
<span class="s2">&quot;&quot;&quot;</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">plan_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">analysis_prompt</span><span class="p">)</span>
            <span class="n">plan</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">plan_response</span><span class="p">)</span>

            <span class="c1"># Execute the plan</span>
            <span class="n">execution_results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">plan</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;execution_plan&quot;</span><span class="p">,</span> <span class="p">[]):</span>
                <span class="k">if</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;tool_call&quot;</span><span class="p">:</span>
                    <span class="c1"># Execute tool call</span>
                    <span class="n">tool_name</span> <span class="o">=</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;details&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tool_name&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">tool_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tool_registry</span><span class="p">:</span>
                        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tool_registry</span><span class="p">[</span><span class="n">tool_name</span><span class="p">](</span><span class="o">**</span><span class="n">step</span><span class="p">[</span><span class="s2">&quot;details&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;parameters&quot;</span><span class="p">,</span> <span class="p">{}))</span>
                        <span class="n">execution_results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">],</span> <span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>

                <span class="k">elif</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;mcp_prompt&quot;</span><span class="p">:</span>
                    <span class="c1"># Use MCP prompt</span>
                    <span class="n">prompt_name</span> <span class="o">=</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;details&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;prompt_name&quot;</span><span class="p">)</span>
                    <span class="n">prompt_args</span> <span class="o">=</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;details&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;arguments&quot;</span><span class="p">,</span> <span class="p">{})</span>
                    <span class="n">prompt_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mcp_server</span><span class="o">.</span><span class="n">get_prompt</span><span class="p">(</span><span class="n">prompt_name</span><span class="p">,</span> <span class="n">prompt_args</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">prompt_result</span><span class="p">:</span>
                        <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_result</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
                        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>
                        <span class="n">execution_results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">],</span> <span class="s2">&quot;result&quot;</span><span class="p">:</span> <span class="n">result</span><span class="p">})</span>

            <span class="c1"># Generate final response</span>
            <span class="n">final_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">User Request: </span><span class="si">{</span><span class="n">user_request</span><span class="si">}</span>

<span class="s2">Execution Results:</span>
<span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">execution_results</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">default</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span><span class="si">}</span>

<span class="s2">Based on the execution results, provide a comprehensive, actionable response to the user. Include:</span>
<span class="s2">1. Direct answer to their question</span>
<span class="s2">2. Specific data and insights from the tools</span>
<span class="s2">3. Actionable recommendations</span>
<span class="s2">4. Next steps they should consider</span>

<span class="s2">Response:</span>
<span class="s2">&quot;&quot;&quot;</span>

            <span class="n">final_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">final_prompt</span><span class="p">)</span>

            <span class="c1"># Add to conversation history</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conversation_history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">final_response</span><span class="p">})</span>

            <span class="k">return</span> <span class="n">final_response</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error processing complex request: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Initialize advanced assistant</span>
<span class="n">assistant</span> <span class="o">=</span> <span class="n">JetsonAIAssistant</span><span class="p">(</span><span class="n">langchain_openai</span><span class="o">.</span><span class="n">llm</span><span class="p">)</span>

<span class="c1"># Test complex scenarios</span>
<span class="n">complex_scenarios</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;I need to deploy 3 different AI models on my Jetson Orin for a production surveillance system. The models are YOLOv8 for detection, a classification model for verification, and a tracking model. I need 30 FPS minimum with less than 20W power consumption. Can you help me create an optimization and deployment plan?&quot;</span><span class="p">,</span>

    <span class="s2">&quot;My current object detection pipeline is running at 18 FPS but I need 25 FPS. The model is using 3.2GB GPU memory and the system temperature reaches 78¬∞C under load. What&#39;s the best optimization strategy?&quot;</span><span class="p">,</span>

    <span class="s2">&quot;I want to compare the performance of YOLOv8, YOLOv10, and RT-DETR on Jetson Orin Nano for real-time person detection in retail environments. Can you help me set up benchmarks and provide recommendations?&quot;</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ü§ñ Advanced AI Assistant Testing:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">scenario</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">complex_scenarios</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üìã Complex Scenario </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Request: </span><span class="si">{</span><span class="n">scenario</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">assistant</span><span class="o">.</span><span class="n">process_complex_request</span><span class="p">(</span><span class="n">scenario</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Response: </span><span class="si">{</span><span class="n">response</span><span class="p">[:</span><span class="mi">400</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="exercise-3-production-ready-mcp-application">üìã Exercise 3: Production-Ready MCP Application<a class="headerlink" href="#exercise-3-production-ready-mcp-application" title="Permanent link">&para;</a></h3>
<p>Create a production-ready application using MCP for Jetson AI development:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ProductionMCPApp</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Production-ready MCP application for Jetson AI development&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mcp_server</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_mcp_server</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm_clients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_llm_clients</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session_manager</span> <span class="o">=</span> <span class="n">SessionManager</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">performance_tracker</span> <span class="o">=</span> <span class="n">PerformanceTracker</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_mcp_server</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MCPServer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize comprehensive MCP server&quot;&quot;&quot;</span>
        <span class="n">server</span> <span class="o">=</span> <span class="n">MCPServer</span><span class="p">()</span>

        <span class="c1"># Register production tools</span>
        <span class="n">server</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;jetson_diagnostics&quot;</span><span class="p">,</span> <span class="s2">&quot;Run comprehensive Jetson diagnostics&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_diagnostics</span><span class="p">)</span>
        <span class="n">server</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;model_optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;Optimize models for production deployment&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimize_model</span><span class="p">)</span>
        <span class="n">server</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;deployment_manager&quot;</span><span class="p">,</span> <span class="s2">&quot;Manage model deployments&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_manage_deployment</span><span class="p">)</span>
        <span class="n">server</span><span class="o">.</span><span class="n">register_tool</span><span class="p">(</span><span class="s2">&quot;performance_analyzer&quot;</span><span class="p">,</span> <span class="s2">&quot;Analyze system performance&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_analyze_performance</span><span class="p">)</span>

        <span class="c1"># Register production prompt templates</span>
        <span class="n">server</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
            <span class="s2">&quot;production_readiness_check&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Comprehensive production readiness assessment&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_production_readiness_template</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">server</span><span class="o">.</span><span class="n">register_prompt</span><span class="p">(</span>
            <span class="s2">&quot;incident_response&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Generate incident response plan&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_incident_response_template</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">server</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_production_readiness_template</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">CONDUCT PRODUCTION READINESS ASSESSMENT</span>
<span class="s2">=====================================</span>

<span class="s2">Application: </span><span class="si">{app_name}</span>
<span class="s2">Environment: </span><span class="si">{environment}</span>
<span class="s2">Models: </span><span class="si">{models}</span>
<span class="s2">Expected Load: </span><span class="si">{expected_load}</span>
<span class="s2">SLA Requirements: </span><span class="si">{sla_requirements}</span>

<span class="s2">Assessment Areas:</span>

<span class="s2">1. PERFORMANCE VALIDATION</span>
<span class="s2">   - Benchmark results under expected load</span>
<span class="s2">   - Latency and throughput analysis</span>
<span class="s2">   - Resource utilization assessment</span>
<span class="s2">   - Stress testing results</span>

<span class="s2">2. RELIABILITY &amp; STABILITY</span>
<span class="s2">   - Error handling mechanisms</span>
<span class="s2">   - Failover procedures</span>
<span class="s2">   - Recovery strategies</span>
<span class="s2">   - Memory leak detection</span>

<span class="s2">3. MONITORING &amp; OBSERVABILITY</span>
<span class="s2">   - Metrics collection setup</span>
<span class="s2">   - Alerting configuration</span>
<span class="s2">   - Logging implementation</span>
<span class="s2">   - Dashboard availability</span>

<span class="s2">4. SECURITY CONSIDERATIONS</span>
<span class="s2">   - Model security validation</span>
<span class="s2">   - Data privacy compliance</span>
<span class="s2">   - Access control implementation</span>
<span class="s2">   - Vulnerability assessment</span>

<span class="s2">5. OPERATIONAL READINESS</span>
<span class="s2">   - Deployment automation</span>
<span class="s2">   - Rollback procedures</span>
<span class="s2">   - Documentation completeness</span>
<span class="s2">   - Team training status</span>

<span class="s2">Provide a comprehensive assessment with:</span>
<span class="s2">- Go/No-Go recommendation</span>
<span class="s2">- Critical issues to address</span>
<span class="s2">- Risk mitigation strategies</span>
<span class="s2">- Timeline for production deployment</span>

<span class="s2">ASSESSMENT REPORT:</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_incident_response_template</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">INCIDENT RESPONSE PLAN</span>
<span class="s2">====================</span>

<span class="s2">Incident Type: </span><span class="si">{incident_type}</span>
<span class="s2">Severity: </span><span class="si">{severity}</span>
<span class="s2">Affected Systems: </span><span class="si">{affected_systems}</span>
<span class="s2">Impact: </span><span class="si">{impact}</span>

<span class="s2">IMMEDIATE ACTIONS (0-15 minutes):</span>
<span class="s2">1. Assess incident scope and impact</span>
<span class="s2">2. Implement immediate containment</span>
<span class="s2">3. Notify stakeholders</span>
<span class="s2">4. Begin incident logging</span>

<span class="s2">SHORT-TERM ACTIONS (15-60 minutes):</span>
<span class="s2">1. Detailed investigation</span>
<span class="s2">2. Implement workarounds</span>
<span class="s2">3. Escalate if necessary</span>
<span class="s2">4. Communicate status updates</span>

<span class="s2">LONG-TERM ACTIONS (1+ hours):</span>
<span class="s2">1. Root cause analysis</span>
<span class="s2">2. Permanent fix implementation</span>
<span class="s2">3. System validation</span>
<span class="s2">4. Post-incident review</span>

<span class="s2">SPECIFIC PROCEDURES:</span>
<span class="s2">- Performance degradation: </span><span class="si">{performance_procedures}</span>
<span class="s2">- Model accuracy issues: </span><span class="si">{accuracy_procedures}</span>
<span class="s2">- System failures: </span><span class="si">{failure_procedures}</span>
<span class="s2">- Security incidents: </span><span class="si">{security_procedures}</span>

<span class="s2">CONTACT INFORMATION:</span>
<span class="s2">- On-call engineer: </span><span class="si">{oncall_contact}</span>
<span class="s2">- Escalation manager: </span><span class="si">{escalation_contact}</span>
<span class="s2">- Vendor support: </span><span class="si">{vendor_contact}</span>

<span class="s2">Generate detailed incident response plan:</span>
<span class="s2">&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_production_workflow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">workflow_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run production workflows using MCP&quot;&quot;&quot;</span>

        <span class="n">workflow_results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;workflow_type&quot;</span><span class="p">:</span> <span class="n">workflow_type</span><span class="p">,</span>
            <span class="s2">&quot;start_time&quot;</span><span class="p">:</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">(),</span>
            <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">parameters</span><span class="p">,</span>
            <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;running&quot;</span>
        <span class="p">}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">workflow_type</span> <span class="o">==</span> <span class="s2">&quot;model_deployment&quot;</span><span class="p">:</span>
                <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_execute_deployment_workflow</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">workflow_type</span> <span class="o">==</span> <span class="s2">&quot;performance_optimization&quot;</span><span class="p">:</span>
                <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_execute_optimization_workflow</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">workflow_type</span> <span class="o">==</span> <span class="s2">&quot;production_validation&quot;</span><span class="p">:</span>
                <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_execute_validation_workflow</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;status&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;completed&quot;</span>
            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;start_time&quot;</span><span class="p">]</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;status&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;failed&quot;</span>
            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="n">workflow_results</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">workflow_results</span>

<span class="c1"># Initialize production app</span>
<span class="n">production_app</span> <span class="o">=</span> <span class="n">ProductionMCPApp</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">üè≠ Production MCP Application Ready&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available workflows: model_deployment, performance_optimization, production_validation&quot;</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p><strong>Note:</strong> These lab exercises are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>lab
</code></pre></div></p>
</blockquote>
<h3 id="lab-results-and-analysis">üìä Lab Results and Analysis<a class="headerlink" href="#lab-results-and-analysis" title="Permanent link">&para;</a></h3>
<p>After completing all exercises, analyze your results:</p>
<ol>
<li><strong>Prompt Engineering Effectiveness</strong>: Which techniques worked best for different types of tasks?</li>
<li><strong>Tool Integration Performance</strong>: How did tool calling improve response quality and accuracy?</li>
<li><strong>MCP Protocol Benefits</strong>: What advantages did MCP provide over direct tool calling?</li>
<li><strong>Jetson-Specific Optimizations</strong>: Which prompt engineering approaches were most effective for edge AI scenarios?</li>
</ol>
<h3 id="next-steps">üéØ Next Steps<a class="headerlink" href="#next-steps" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Experiment with Custom Models</strong>: Try the techniques with different local models</li>
<li><strong>Build Domain-Specific Tools</strong>: Create tools specific to your AI application domain</li>
<li><strong>Implement Production Monitoring</strong>: Add comprehensive logging and monitoring to your MCP applications</li>
<li><strong>Optimize for Edge Deployment</strong>: Focus on minimizing latency and resource usage for production edge AI systems</li>
</ol>
<hr />
<h2 id="conclusion">üéâ Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h2>
<p>This tutorial covered comprehensive prompt engineering techniques for Jetson AI development, from basic prompting strategies to advanced tool calling and MCP protocol implementation. You've learned how to:</p>
<ul>
<li><strong>Master Core Techniques</strong>: Chain-of-thought, few-shot learning, role-based prompting, and in-context learning</li>
<li><strong>Integrate LangChain</strong>: Leverage LangChain for model-agnostic prompt engineering and structured output</li>
<li><strong>Implement Tool Calling</strong>: Enable LLMs to interact with external tools and functions</li>
<li><strong>Use MCP Protocol</strong>: Build scalable applications using the Model Context Protocol</li>
<li><strong>Optimize for Jetson</strong>: Apply prompt engineering specifically for edge AI scenarios</li>
</ul>
<p>These techniques enable you to build sophisticated AI applications that can reason, plan, and execute complex tasks on Jetson platforms, making your edge AI systems more intelligent and capable.</p>
<h3 id="goal">üéØ Goal<a class="headerlink" href="#goal" title="Permanent link">&para;</a></h3>
<p>Test and compare prompt engineering on three backends:</p>
<ol>
<li>llama-cpp-python</li>
<li>Ollama</li>
<li>OpenAI API</li>
</ol>
<h3 id="prompt-types-to-try">üîÅ Prompt Types to Try<a class="headerlink" href="#prompt-types-to-try" title="Permanent link">&para;</a></h3>
<ul>
<li>Instructional prompt</li>
<li>Few-shot learning</li>
<li>Chain-of-thought reasoning</li>
<li>Rewriting and follow-ups</li>
</ul>
<hr />
<h2 id="deliverables">üìã Deliverables<a class="headerlink" href="#deliverables" title="Permanent link">&para;</a></h2>
<ul>
<li>Code + PromptTemplate examples</li>
<li>Comparison table of responses from three backends</li>
<li>Bonus: Add memory support for follow-up context</li>
</ul>
<hr />
<h2 id="summary">üß† Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<ul>
<li>Start with local inference and basic prompting</li>
<li>Move to API-based or structured LangChain interfaces</li>
<li>Use LangChain to modularize prompt types and switch LLM backends (OpenAI, Ollama, or llama-cpp)</li>
<li>Jetson Orin Nano supports local inference with quantized models using llama.cpp or Ollama</li>
</ul>
<hr />
<h2 id="unified-python-script">üöÄ Unified Python Script<a class="headerlink" href="#unified-python-script" title="Permanent link">&para;</a></h2>
<p>All the Python code examples from this tutorial have been consolidated into a single, unified script called <code>jetson_prompt_toolkit.py</code>. This script provides a command-line interface to experiment with different prompt engineering techniques, backends, and advanced features.</p>
<h3 id="installation">üì• Installation<a class="headerlink" href="#installation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Clone the repository if you haven&#39;t already</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/yourusername/edgeAI.git
<span class="nb">cd</span><span class="w"> </span>edgeAI

<span class="c1"># Install dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>openai<span class="w"> </span>langchain<span class="w"> </span>langchain-openai<span class="w"> </span>langchain-community<span class="w"> </span>pydantic

<span class="c1"># Optional: Install Ollama for local inference</span>
<span class="c1"># Follow instructions at https://ollama.ai/</span>

<span class="c1"># Optional: Install llama-cpp-python for local inference</span>
pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python
</code></pre></div>
<h3 id="usage-examples">üîß Usage Examples<a class="headerlink" href="#usage-examples" title="Permanent link">&para;</a></h3>
<h4 id="basic-prompt-engineering-techniques">Basic Prompt Engineering Techniques<a class="headerlink" href="#basic-prompt-engineering-techniques" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Test Chain-of-Thought reasoning with OpenAI</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>cot<span class="w"> </span>--backends<span class="w"> </span>openai

<span class="c1"># Compare all techniques across multiple backends</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>all<span class="w"> </span>--backends<span class="w"> </span>openai,ollama
</code></pre></div>
<h4 id="compare-different-backends">Compare Different Backends<a class="headerlink" href="#compare-different-backends" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Compare responses from different backends</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>compare<span class="w"> </span>--backends<span class="w"> </span>openai,ollama,llamacpp<span class="w"> </span>--llamacpp-model<span class="w"> </span>/path/to/model.gguf
</code></pre></div>
<h4 id="structured-output-generation">Structured Output Generation<a class="headerlink" href="#structured-output-generation" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Generate a YOLOv8 optimization plan</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>structured<span class="w"> </span>--output<span class="w"> </span>optimization_plan<span class="w"> </span>--backends<span class="w"> </span>openai

<span class="c1"># Compare Jetson devices</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>structured<span class="w"> </span>--output<span class="w"> </span>device_comparison<span class="w"> </span>--backends<span class="w"> </span>openai
</code></pre></div>
<h4 id="tool-calling-demonstrations">Tool Calling Demonstrations<a class="headerlink" href="#tool-calling-demonstrations" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Process a request using tool calling</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>tools<span class="w"> </span>--request<span class="w"> </span><span class="s2">&quot;What&#39;s the current system status of my Jetson device?&quot;</span>
</code></pre></div>
<h4 id="function-calling-demonstrations">Function Calling Demonstrations<a class="headerlink" href="#function-calling-demonstrations" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Process a request using function calling</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>functions<span class="w"> </span>--request<span class="w"> </span><span class="s2">&quot;I need to optimize my YOLOv8 model for Jetson Nano&quot;</span>
</code></pre></div>
<h4 id="mcp-protocol-demonstrations">MCP Protocol Demonstrations<a class="headerlink" href="#mcp-protocol-demonstrations" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Process a request using MCP</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>mcp<span class="w"> </span>--request<span class="w"> </span><span class="s2">&quot;Create a deployment checklist for my YOLOv8 model on Jetson Xavier NX&quot;</span>
</code></pre></div>
<h4 id="advanced-assistant-demonstrations">Advanced Assistant Demonstrations<a class="headerlink" href="#advanced-assistant-demonstrations" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Process a complex request with the Jetson AI Assistant</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>assistant<span class="w"> </span>--request<span class="w"> </span><span class="s2">&quot;I need to deploy multiple AI models on my Jetson AGX Orin for a smart retail application&quot;</span>
</code></pre></div>
<h4 id="production-mcp-application">Production MCP Application<a class="headerlink" href="#production-mcp-application" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Run a production workflow</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>production
</code></pre></div>
<h3 id="switching-models">üîÑ Switching Models<a class="headerlink" href="#switching-models" title="Permanent link">&para;</a></h3>
<p>You can specify which models to use with each backend:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Use a specific OpenAI model</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>cot<span class="w"> </span>--backends<span class="w"> </span>openai<span class="w"> </span>--openai-model<span class="w"> </span>gpt-4o

<span class="c1"># Use a specific Ollama model</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>cot<span class="w"> </span>--backends<span class="w"> </span>ollama<span class="w"> </span>--ollama-model<span class="w"> </span>llama3.1:8b

<span class="c1"># Use a specific llama-cpp-python model</span>
python<span class="w"> </span>jetson_prompt_toolkit.py<span class="w"> </span>--mode<span class="w"> </span>basic<span class="w"> </span>--technique<span class="w"> </span>cot<span class="w"> </span>--backends<span class="w"> </span>llamacpp<span class="w"> </span>--llamacpp-model<span class="w"> </span>/path/to/model.gguf
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>