
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>🚀 02: Accelerated Computing on Jetson (CUDA, Numba, NumPy, PyTorch) - Jetson Cyber & AI Summer Camp</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#02-accelerated-computing-on-jetson-cuda-numba-numpy-pytorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-header__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Jetson Cyber & AI Summer Camp
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              🚀 02: Accelerated Computing on Jetson (CUDA, Numba, NumPy, PyTorch)
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Jetson Cyber &amp; AI Summer Camp" class="md-nav__button md-logo" aria-label="Jetson Cyber & AI Summer Camp" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Jetson Cyber & AI Summer Camp
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00_sjsujetsontool_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sjsujetsontool Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01a_nvidia_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to NVIDIA Jetson
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01b_linux_networking_tools.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Linux and Networking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Core Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_programming_env_python_cpp_cuda.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Programming Environment (Python/C++/CUDA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03_accelerated_computing_python_cuda.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Accelerated Python + CUDA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04a_numpy_pytorch_intro.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Numpy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04b_pytorch_intro.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cyber Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Cyber Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01b_linux_networking_tools.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux Networking Tools
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01c_packet_sniffing_monitoring.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Packet Sniffing & Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01d_linux_cyber_defense_basics.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux Cyber Defense Tools
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01e_linux_cyber_attack_simulation.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simulated Attacks & Detection
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    AI & LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            AI & LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_cnn_image_processing_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CNNs + Image Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06_transformers_llms_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformers & LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07_nlp_applications_llm_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP + Inference Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08_prompt_engineering_langchain_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09_rag_app_langchain_jetson/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG Apps
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_local_ai_agents_jetson.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Local AI Agents
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Final Project
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Final Project
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_final_challenges_hackathon.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hackathon & Challenges
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Learning Objectives
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#theoretical-background" class="md-nav__link">
    <span class="md-ellipsis">
      📚 Theoretical Background
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📚 Theoretical Background">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-accelerated-computing" class="md-nav__link">
    <span class="md-ellipsis">
      What is Accelerated Computing?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu-vs-gpu-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CPU vs GPU Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu-architecture-deep-dive" class="md-nav__link">
    <span class="md-ellipsis">
      🏗️ GPU Architecture Deep Dive
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🏗️ GPU Architecture Deep Dive">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#streaming-multiprocessors-sms" class="md-nav__link">
    <span class="md-ellipsis">
      Streaming Multiprocessors (SMs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-hierarchy-and-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Hierarchy and Performance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-computing-models" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 Parallel Computing Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧮 Parallel Computing Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simd-vs-simt" class="md-nav__link">
    <span class="md-ellipsis">
      SIMD vs SIMT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amdahls-law-and-parallel-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      Amdahl's Law and Parallel Efficiency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-access-patterns-and-coalescing" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Access Patterns and Coalescing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jetson-hardware" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 Jetson Hardware:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      ⏱️ Memory Considerations:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#types-of-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 Types of Parallelism:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-programming-model" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA Programming Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA Programming Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#thread-hierarchy-and-execution-model" class="md-nav__link">
    <span class="md-ellipsis">
      Thread Hierarchy and Execution Model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cuda-c" class="md-nav__link">
    <span class="md-ellipsis">
      🔧 CUDA (C++)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔧 CUDA (C++)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-programming-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ CUDA Programming Fundamentals
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✅ CUDA Programming Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-hierarchy-and-management" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Hierarchy and Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compilation-and-execution" class="md-nav__link">
    <span class="md-ellipsis">
      Compilation and Execution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-cuda-features" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced CUDA Features
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#numba-python-cuda-jit" class="md-nav__link">
    <span class="md-ellipsis">
      🐍 Numba (Python CUDA JIT)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🐍 Numba (Python CUDA JIT)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-numba-cuda" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ Basic Numba CUDA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-numba-features" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ Advanced Numba Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="✅ Advanced Numba Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shared-memory-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Shared Memory Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-management-and-streams" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Management and Streams
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-profiling-with-numba" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Profiling with Numba
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#installation-and-setup" class="md-nav__link">
    <span class="md-ellipsis">
      🐳 Installation and Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      🚀 Advanced Optimization Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🚀 Advanced Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#performance-profiling-and-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      📊 Performance Profiling and Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📊 Performance Profiling and Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nvidia-nsight-systems-integration" class="md-nav__link">
    <span class="md-ellipsis">
      NVIDIA Nsight Systems Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-performance-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Performance Metrics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-optimization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      ⚡ Memory Optimization Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="⚡ Memory Optimization Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-pool-management" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Pool Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unified-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      Unified Memory Management
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#numpy-with-cupy-gpu-drop-in-replacement" class="md-nav__link">
    <span class="md-ellipsis">
      📊 NumPy with CuPy (GPU drop-in replacement)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📊 NumPy with CuPy (GPU drop-in replacement)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cupy-example" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ CuPy Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-container" class="md-nav__link">
    <span class="md-ellipsis">
      🐳 In Container
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch-on-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      🔥 PyTorch on GPU
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔥 PyTorch on GPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ PyTorch Matrix Multiplication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#simple-nn-training-with-loss" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ Simple NN Training (with loss)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-measurement-tools" class="md-nav__link">
    <span class="md-ellipsis">
      📈 Performance Measurement Tools
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📈 Performance Measurement Tools">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchcudaevent" class="md-nav__link">
    <span class="md-ellipsis">
      torch.cuda.Event
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tegrastats-outside-container" class="md-nav__link">
    <span class="md-ellipsis">
      tegrastats (outside container)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comprehensive-lab-accelerated-computing-mastery" class="md-nav__link">
    <span class="md-ellipsis">
      🧪 Comprehensive Lab: Accelerated Computing Mastery
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🧪 Comprehensive Lab: Accelerated Computing Mastery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Lab Objectives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Lab Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-1-gpu-architecture-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Exercise 1: GPU Architecture Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exercise 1: GPU Architecture Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-11-hardware-profiling" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1.1: Hardware Profiling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-12-memory-bandwidth-benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1.2: Memory Bandwidth Benchmark
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-13-occupancy-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1.3: Occupancy Analysis
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-2-parallel-algorithm-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Exercise 2: Parallel Algorithm Implementation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exercise 2: Parallel Algorithm Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-21-optimized-parallel-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      Task 2.1: Optimized Parallel Reduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-22-matrix-transpose-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Task 2.2: Matrix Transpose Optimization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exercise-3-framework-performance-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Exercise 3: Framework Performance Comparison
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exercise 3: Framework Performance Comparison">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-31-comprehensive-framework-benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      Task 3.1: Comprehensive Framework Benchmark
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-deliverables" class="md-nav__link">
    <span class="md-ellipsis">
      Lab Deliverables
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab Deliverables">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#required-submissions" class="md-nav__link">
    <span class="md-ellipsis">
      Required Submissions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bonus-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Bonus Challenges
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab-assessment-criteria" class="md-nav__link">
    <span class="md-ellipsis">
      Lab Assessment Criteria
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-and-next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      Summary and Next Steps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#framework-comparison-summary" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Framework Comparison Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="02-accelerated-computing-on-jetson-cuda-numba-numpy-pytorch">🚀 02: Accelerated Computing on Jetson (CUDA, Numba, NumPy, PyTorch)<a class="headerlink" href="#02-accelerated-computing-on-jetson-cuda-numba-numpy-pytorch" title="Permanent link">&para;</a></h1>
<p>Accelerated computing leverages GPUs and other hardware accelerators to significantly improve performance in data processing, AI, and scientific computation. NVIDIA Jetson devices are optimized for this via CUDA and Tensor Cores.</p>
<p>This module introduces key libraries and techniques to write accelerated code using:</p>
<ul>
<li><strong>CUDA (native + via libraries)</strong></li>
<li><strong>Numba</strong> (Python JIT for GPU)</li>
<li><strong>NumPy (with CuPy)</strong></li>
<li><strong>PyTorch with GPU acceleration</strong></li>
</ul>
<hr />
<h2 id="learning-objectives">🎯 Learning Objectives<a class="headerlink" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<p>By the end of this tutorial, you will:
- Understand the fundamentals of accelerated computing and parallel processing
- Master GPU architecture, memory hierarchy, and execution models
- Learn CUDA programming concepts and optimization techniques
- Implement GPU-accelerated applications using various frameworks
- Optimize code for Jetson devices with performance profiling
- Apply theoretical knowledge to real-world acceleration problems</p>
<hr />
<h2 id="theoretical-background">📚 Theoretical Background<a class="headerlink" href="#theoretical-background" title="Permanent link">&para;</a></h2>
<h3 id="what-is-accelerated-computing">What is Accelerated Computing?<a class="headerlink" href="#what-is-accelerated-computing" title="Permanent link">&para;</a></h3>
<p>Accelerated computing uses specialized hardware (like GPUs) to perform computations faster than traditional CPUs. This is particularly effective for:</p>
<ul>
<li><strong>Parallel workloads</strong>: Tasks that can be divided into many smaller, independent operations</li>
<li><strong>Mathematical computations</strong>: Linear algebra, signal processing, machine learning</li>
<li><strong>Data processing</strong>: Large datasets, image/video processing</li>
</ul>
<h3 id="cpu-vs-gpu-architecture">CPU vs GPU Architecture<a class="headerlink" href="#cpu-vs-gpu-architecture" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>CPU</th>
<th>GPU</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cores</strong></td>
<td>Few (4-16) powerful cores</td>
<td>Many (hundreds to thousands) simpler cores</td>
</tr>
<tr>
<td><strong>Design</strong></td>
<td>Optimized for sequential processing</td>
<td>Optimized for parallel processing</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Large cache, complex memory hierarchy</td>
<td>Smaller cache, high bandwidth memory</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Complex logic, branching</td>
<td>Simple operations on large datasets</td>
</tr>
</tbody>
</table>
<h3 id="gpu-architecture-deep-dive">🏗️ GPU Architecture Deep Dive<a class="headerlink" href="#gpu-architecture-deep-dive" title="Permanent link">&para;</a></h3>
<h4 id="streaming-multiprocessors-sms"><strong>Streaming Multiprocessors (SMs)</strong><a class="headerlink" href="#streaming-multiprocessors-sms" title="Permanent link">&para;</a></h4>
<p>Modern GPUs are organized into Streaming Multiprocessors, each containing:</p>
<ul>
<li><strong>CUDA Cores</strong>: Basic processing units (32-128 per SM)</li>
<li><strong>Shared Memory</strong>: Fast, low-latency memory shared among threads in a block</li>
<li><strong>Registers</strong>: Fastest memory, private to each thread</li>
<li><strong>Warp Schedulers</strong>: Manage groups of 32 threads (warps)</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Understanding GPU architecture programmatically</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pycuda.driver</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pycuda.autoinit</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pycuda.compiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">SourceModule</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_gpu_properties</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Query and display GPU architecture properties&quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">props</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">get_attributes</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== GPU Architecture Properties ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device Name: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">name</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compute Capability: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">compute_capability</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiprocessor Count: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MULTIPROCESSOR_COUNT</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Threads per Block: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_THREADS_PER_BLOCK</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Block Dimensions: (</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_X</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_Y</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_Z</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Grid Dimensions: (</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_X</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_Y</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_Z</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shared Memory per Block: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_SHARED_MEMORY_PER_BLOCK</span><span class="p">]</span><span class="si">}</span><span class="s2"> bytes&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Registers per Block: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_REGISTERS_PER_BLOCK</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warp Size: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">WARP_SIZE</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Global Memory: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">total_memory</span><span class="p">()</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory Clock Rate: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MEMORY_CLOCK_RATE</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1000</span><span class="si">}</span><span class="s2"> MHz&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory Bus Width: </span><span class="si">{</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">GLOBAL_MEMORY_BUS_WIDTH</span><span class="p">]</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>

    <span class="c1"># Calculate theoretical memory bandwidth</span>
    <span class="n">memory_bandwidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MEMORY_CLOCK_RATE</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> 
                       <span class="n">props</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">GLOBAL_MEMORY_BUS_WIDTH</span><span class="p">]</span> <span class="o">/</span> <span class="mi">8</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e6</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Memory Bandwidth: </span><span class="si">{</span><span class="n">memory_bandwidth</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> GB/s&quot;</span><span class="p">)</span>

<span class="n">get_gpu_properties</span><span class="p">()</span>
</code></pre></div>
<h4 id="memory-hierarchy-and-performance"><strong>Memory Hierarchy and Performance</strong><a class="headerlink" href="#memory-hierarchy-and-performance" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">memory_benchmark_kernel</span><span class="p">(</span><span class="n">global_mem</span><span class="p">,</span> <span class="n">shared_mem_size</span><span class="p">,</span> <span class="n">iterations</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark different memory types&quot;&quot;&quot;</span>
    <span class="c1"># Shared memory allocation</span>
    <span class="n">shared_mem</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shared_mem_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">bid</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bid</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">tid</span>

    <span class="c1"># Initialize shared memory</span>
    <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">shared_mem_size</span><span class="p">:</span>
        <span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">*</span> <span class="mf">1.0</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="c1"># Global memory access pattern</span>
    <span class="n">global_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">global_mem</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">global_sum</span> <span class="o">+=</span> <span class="n">global_mem</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="c1"># Shared memory access pattern</span>
    <span class="n">shared_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">shared_sum</span> <span class="o">+=</span> <span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span> <span class="o">%</span> <span class="n">shared_mem_size</span><span class="p">]</span>

    <span class="c1"># Write results back</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">global_mem</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">global_mem</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_sum</span> <span class="o">+</span> <span class="n">shared_sum</span>

<span class="k">def</span><span class="w"> </span><span class="nf">memory_performance_analysis</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analyze memory access patterns and performance&quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">d_data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>
    <span class="n">shared_mem_size</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">iterations</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Memory Performance Analysis ===&quot;</span><span class="p">)</span>

    <span class="c1"># Benchmark memory access</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">memory_benchmark_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span>
        <span class="n">d_data</span><span class="p">,</span> <span class="n">shared_mem_size</span><span class="p">,</span> <span class="n">iterations</span>
    <span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">gpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="c1"># Calculate memory throughput</span>
    <span class="n">bytes_accessed</span> <span class="o">=</span> <span class="n">N</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">iterations</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Read + Write, 4 bytes per float</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">bytes_accessed</span> <span class="o">/</span> <span class="n">gpu_time</span> <span class="o">/</span> <span class="mf">1e9</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU Time: </span><span class="si">{</span><span class="n">gpu_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory Throughput: </span><span class="si">{</span><span class="n">throughput</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB/s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Operations: </span><span class="si">{</span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">iterations</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">gpu_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> M ops/sec&quot;</span><span class="p">)</span>

<span class="n">memory_performance_analysis</span><span class="p">()</span>
</code></pre></div>
<h3 id="parallel-computing-models">🧮 Parallel Computing Models<a class="headerlink" href="#parallel-computing-models" title="Permanent link">&para;</a></h3>
<h4 id="simd-vs-simt"><strong>SIMD vs SIMT</strong><a class="headerlink" href="#simd-vs-simt" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>SIMD (Single Instruction, Multiple Data)</strong>: Traditional vector processing</li>
<li><strong>SIMT (Single Instruction, Multiple Thread)</strong>: GPU's execution model</li>
<li>Threads in a warp execute the same instruction</li>
<li>Allows for thread divergence (with performance cost)</li>
<li>More flexible than pure SIMD</li>
</ul>
<h4 id="amdahls-law-and-parallel-efficiency"><strong>Amdahl's Law and Parallel Efficiency</strong><a class="headerlink" href="#amdahls-law-and-parallel-efficiency" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">amdahls_law</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate speedup using Amdahl&#39;s Law</span>
<span class="sd">    p: fraction of program that can be parallelized (0-1)</span>
<span class="sd">    n: number of processors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_amdahls_law</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the impact of parallel fraction on speedup&quot;&quot;&quot;</span>
    <span class="n">processors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 1 to 1000 processors</span>
    <span class="n">parallel_fractions</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parallel_fractions</span><span class="p">:</span>
        <span class="n">speedup</span> <span class="o">=</span> <span class="p">[</span><span class="n">amdahls_law</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">processors</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">processors</span><span class="p">,</span> <span class="n">speedup</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Parallel fraction = </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Processors&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Speedup&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Amdahl&#39;s Law: Theoretical Speedup vs Number of Processors&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Calculate practical implications</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Amdahl&#39;s Law Analysis ===&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]:</span>
        <span class="n">max_speedup</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parallel fraction </span><span class="si">{</span><span class="n">p</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">%: Maximum possible speedup = </span><span class="si">{</span><span class="n">max_speedup</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

<span class="n">plot_amdahls_law</span><span class="p">()</span>
</code></pre></div>
<h4 id="memory-access-patterns-and-coalescing"><strong>Memory Access Patterns and Coalescing</strong><a class="headerlink" href="#memory-access-patterns-and-coalescing" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">coalesced_access</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate coalesced memory access&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="c1"># Coalesced access: consecutive threads access consecutive memory</span>
        <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">strided_access</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate strided memory access&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="c1"># Strided access: threads access memory with a stride</span>
        <span class="n">access_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="n">stride</span><span class="p">)</span> <span class="o">%</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span>
        <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">access_idx</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">random_access</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate random memory access&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="c1"># Random access: unpredictable memory access pattern</span>
        <span class="n">access_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">%</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span>
        <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">access_idx</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">memory_access_benchmark</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark different memory access patterns&quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">d_data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">d_result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">d_indices</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Memory Access Pattern Benchmark ===&quot;</span><span class="p">)</span>

    <span class="c1"># Coalesced access</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">coalesced_access</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_data</span><span class="p">,</span> <span class="n">d_result</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">coalesced_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coalesced access: </span><span class="si">{</span><span class="n">coalesced_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="c1"># Strided access</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">strided_access</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_data</span><span class="p">,</span> <span class="n">d_result</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">strided_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Strided access (stride=32): </span><span class="si">{</span><span class="n">strided_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="c1"># Random access</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">random_access</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_data</span><span class="p">,</span> <span class="n">d_result</span><span class="p">,</span> <span class="n">d_indices</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">random_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random access: </span><span class="si">{</span><span class="n">random_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Performance ratios:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Strided vs Coalesced: </span><span class="si">{</span><span class="n">strided_time</span><span class="o">/</span><span class="n">coalesced_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x slower&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random vs Coalesced: </span><span class="si">{</span><span class="n">random_time</span><span class="o">/</span><span class="n">coalesced_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x slower&quot;</span><span class="p">)</span>

<span class="n">memory_access_benchmark</span><span class="p">()</span>
</code></pre></div>
<h3 id="jetson-hardware">🔧 Jetson Hardware:<a class="headerlink" href="#jetson-hardware" title="Permanent link">&para;</a></h3>
<ul>
<li>CUDA cores: For general-purpose parallelism</li>
<li>Tensor cores: Accelerated AI math (INT8, FP16)</li>
<li>DLA (Deep Learning Accelerator): Fixed function for inference</li>
</ul>
<h3 id="memory-considerations">⏱️ Memory Considerations:<a class="headerlink" href="#memory-considerations" title="Permanent link">&para;</a></h3>
<ul>
<li>CUDA global memory: large but slower</li>
<li>Shared memory: faster but limited to thread blocks</li>
<li>Memory transfers between CPU (host) and GPU (device) are expensive and must be minimized</li>
</ul>
<h3 id="types-of-parallelism">🧮 Types of Parallelism:<a class="headerlink" href="#types-of-parallelism" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Data Parallelism</strong>: Same operation applied to many elements (e.g., matrix multiplication)</li>
<li><strong>Task Parallelism</strong>: Different operations executed in parallel (e.g., CNN layers, asynchronous operations)</li>
</ul>
<h3 id="cuda-programming-model">CUDA Programming Model<a class="headerlink" href="#cuda-programming-model" title="Permanent link">&para;</a></h3>
<p>CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform:</p>
<ul>
<li><strong>Host</strong>: CPU and its memory</li>
<li><strong>Device</strong>: GPU and its memory</li>
<li><strong>Kernel</strong>: Function that runs on GPU</li>
<li><strong>Thread</strong>: Basic unit of execution</li>
<li><strong>Block</strong>: Group of threads that can cooperate</li>
<li><strong>Grid</strong>: Collection of blocks</li>
</ul>
<h4 id="thread-hierarchy-and-execution-model"><strong>Thread Hierarchy and Execution Model</strong><a class="headerlink" href="#thread-hierarchy-and-execution-model" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Understanding CUDA thread hierarchy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">thread_info_kernel</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Kernel to understand thread hierarchy&quot;&quot;&quot;</span>
    <span class="c1"># Thread indices</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">tz</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">z</span>

    <span class="c1"># Block indices</span>
    <span class="n">bx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">bz</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">z</span>

    <span class="c1"># Block dimensions</span>
    <span class="n">bdx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="n">bdy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span>
    <span class="n">bdz</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">z</span>

    <span class="c1"># Grid dimensions</span>
    <span class="n">gdx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span>
    <span class="n">gdy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">y</span>
    <span class="n">gdz</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">z</span>

    <span class="c1"># Calculate global thread ID</span>
    <span class="n">global_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">bz</span> <span class="o">*</span> <span class="n">gdy</span> <span class="o">*</span> <span class="n">gdx</span> <span class="o">*</span> <span class="n">bdz</span> <span class="o">*</span> <span class="n">bdy</span> <span class="o">*</span> <span class="n">bdx</span> <span class="o">+</span>
                <span class="n">by</span> <span class="o">*</span> <span class="n">gdx</span> <span class="o">*</span> <span class="n">bdy</span> <span class="o">*</span> <span class="n">bdx</span> <span class="o">+</span>
                <span class="n">bx</span> <span class="o">*</span> <span class="n">bdy</span> <span class="o">*</span> <span class="n">bdx</span> <span class="o">+</span>
                <span class="n">tz</span> <span class="o">*</span> <span class="n">bdy</span> <span class="o">*</span> <span class="n">bdx</span> <span class="o">+</span>
                <span class="n">ty</span> <span class="o">*</span> <span class="n">bdx</span> <span class="o">+</span>
                <span class="n">tx</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">global_id</span> <span class="o">&lt;</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">output</span><span class="p">[</span><span class="n">global_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_id</span>

<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_thread_hierarchy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate CUDA thread hierarchy&quot;&quot;&quot;</span>
    <span class="c1"># Create a 3D grid and block configuration</span>
    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 32 threads per block</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># 4 blocks total</span>

    <span class="n">total_threads</span> <span class="o">=</span> <span class="p">(</span><span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span>
                    <span class="n">blocks_per_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">blocks_per_grid</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">blocks_per_grid</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">total_threads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== CUDA Thread Hierarchy Demo ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Threads per block: </span><span class="si">{</span><span class="n">threads_per_block</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Blocks per grid: </span><span class="si">{</span><span class="n">blocks_per_grid</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total threads: </span><span class="si">{</span><span class="n">total_threads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">thread_info_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_output</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">d_output</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Thread IDs: </span><span class="si">{</span><span class="n">result</span><span class="p">[:</span><span class="mi">16</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>  <span class="c1"># Show first 16 thread IDs</span>

<span class="n">demonstrate_thread_hierarchy</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="cuda-c">🔧 CUDA (C++)<a class="headerlink" href="#cuda-c" title="Permanent link">&para;</a></h2>
<p>The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data (SPMD) parallel jobs. Such jobs are self-contained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware.</p>
<p>Jetson supports native CUDA (C/C++) for fine-grained control. Ideal for performance-critical compute kernels.</p>
<p><strong>NVIDIA CUDA Compiler (NVCC)</strong>: <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/">NVCC</a> is a compiler driver provided by NVIDIA for compiling CUDA C/C++ programs. It's a toolchain that manages the compilation process, generating binary executables containing both host (CPU) code and device (GPU) code, including PTX and SASS.
    - It works by invoking other tools like a C++ compiler (e.g., g++) and the CUDA runtime library.
    - It's used to compile CUDA code, which is often found in source files with the .cu extension.
    - The output can be C code (for the host) or PTX code (for the device), and potentially directly compiled SASS code. </p>
<p>The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file. It is the purpose of nvcc, the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers. It accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. All non-CUDA compilation steps are forwarded to a C++ host compiler that is supported by nvcc, and nvcc translates its options to appropriate host compiler command line options.</p>
<blockquote>
<p>reference: <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#">Cuda C Programming Guide</a></p>
</blockquote>
<h3 id="cuda-programming-fundamentals">✅ CUDA Programming Fundamentals<a class="headerlink" href="#cuda-programming-fundamentals" title="Permanent link">&para;</a></h3>
<h4 id="memory-hierarchy-and-management"><strong>Memory Hierarchy and Management</strong><a class="headerlink" href="#memory-hierarchy-and-management" title="Permanent link">&para;</a></h4>
<p>CUDA memory hierarchy is crucial for performance optimization:</p>
<div class="highlight"><pre><span></span><code><span class="c1">// vector_add.cu - Basic CUDA kernel example</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda_runtime.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;chrono&gt;</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">vectorAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Matrix multiplication with shared memory optimization</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">matrixMulShared</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">As</span><span class="p">[</span><span class="mi">16</span><span class="p">][</span><span class="mi">16</span><span class="p">];</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">Bs</span><span class="p">[</span><span class="mi">16</span><span class="p">][</span><span class="mi">16</span><span class="p">];</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">bx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">ty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ty</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tx</span><span class="p">;</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">15</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Load data into shared memory</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span>
<span class="w">            </span><span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tx</span><span class="p">];</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ty</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span>
<span class="w">            </span><span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[(</span><span class="n">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ty</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">];</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">Bs</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Compute partial sum</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">As</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Bs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Host memory allocation</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">h_A</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="n">h_B</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="n">h_C</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Initialize data</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Device memory allocation</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">d_A</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_B</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_A</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_B</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_C</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy data to device</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span><span class="w"> </span><span class="n">h_A</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span><span class="w"> </span><span class="n">h_B</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Launch kernel</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">blocksPerGrid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="p">;</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
<span class="w">    </span><span class="n">vectorAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocksPerGrid</span><span class="p">,</span><span class="w"> </span><span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span><span class="w"> </span><span class="n">d_B</span><span class="p">,</span><span class="w"> </span><span class="n">d_C</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">duration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">microseconds</span><span class="o">&gt;</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Kernel execution time: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">duration</span><span class="p">.</span><span class="n">count</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; microseconds&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Copy result back</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">d_C</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Verify result</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">success</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">abs</span><span class="p">(</span><span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">1e-5</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">success</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">            </span><span class="k">break</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Test &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="p">(</span><span class="n">success</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="s">&quot;PASSED&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FAILED&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Cleanup</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<h4 id="compilation-and-execution"><strong>Compilation and Execution</strong><a class="headerlink" href="#compilation-and-execution" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Compile CUDA code</span>
nvcc<span class="w"> </span>-o<span class="w"> </span>vector_add<span class="w"> </span>vector_add.cu<span class="w"> </span>-arch<span class="o">=</span>sm_87<span class="w">  </span><span class="c1"># For Jetson Orin</span>
nvcc<span class="w"> </span>-o<span class="w"> </span>vector_add<span class="w"> </span>vector_add.cu<span class="w"> </span>-arch<span class="o">=</span>sm_72<span class="w">  </span><span class="c1"># For Jetson Xavier NX</span>

<span class="c1"># Run the program</span>
./vector_add
</code></pre></div>
<h4 id="advanced-cuda-features"><strong>Advanced CUDA Features</strong><a class="headerlink" href="#advanced-cuda-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1">// streams_example.cu - Asynchronous execution with CUDA streams</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuda_runtime.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">nStreams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">streamSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">nStreams</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">streamBytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">streamSize</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Allocate pinned host memory for faster transfers</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">h_data</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaMallocHost</span><span class="p">(</span><span class="o">&amp;</span><span class="n">h_data</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

<span class="w">    </span><span class="c1">// Initialize data</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">h_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Allocate device memory</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">d_data</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_data</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

<span class="w">    </span><span class="c1">// Create streams</span>
<span class="w">    </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">nStreams</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nStreams</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Launch kernels asynchronously</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nStreams</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">streamSize</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// Async memory copy H2D</span>
<span class="w">        </span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_data</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span><span class="w"> </span><span class="o">&amp;</span><span class="n">h_data</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span><span class="w"> </span><span class="n">streamBytes</span><span class="p">,</span>
<span class="w">                       </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>

<span class="w">        </span><span class="c1">// Launch kernel</span>
<span class="w">        </span><span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">streamSize</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_data</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span><span class="w"> </span><span class="n">streamSize</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Async memory copy D2H</span>
<span class="w">        </span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="o">&amp;</span><span class="n">h_data</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span><span class="w"> </span><span class="o">&amp;</span><span class="n">d_data</span><span class="p">[</span><span class="n">offset</span><span class="p">],</span><span class="w"> </span><span class="n">streamBytes</span><span class="p">,</span>
<span class="w">                       </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Wait for all streams to complete</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nStreams</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="n">cudaStreamDestroy</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Streams execution completed&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Cleanup</span>
<span class="w">    </span><span class="n">cudaFreeHost</span><span class="p">(</span><span class="n">h_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_data</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<hr />
<h2 id="numba-python-cuda-jit">🐍 Numba (Python CUDA JIT)<a class="headerlink" href="#numba-python-cuda-jit" title="Permanent link">&para;</a></h2>
<p>Numba allows writing Python functions that compile to GPU code via LLVM and CUDA backend, providing near-native performance with Python syntax.</p>
<h3 id="basic-numba-cuda">✅ Basic Numba CUDA<a class="headerlink" href="#basic-numba-cuda" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">OUT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">add_kernel</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">](</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">OUT</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU sum[0]:&quot;</span><span class="p">,</span> <span class="n">OUT</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<h3 id="advanced-numba-features">✅ Advanced Numba Features<a class="headerlink" href="#advanced-numba-features" title="Permanent link">&para;</a></h3>
<h4 id="shared-memory-optimization"><strong>Shared Memory Optimization</strong><a class="headerlink" href="#shared-memory-optimization" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span><span class="p">,</span> <span class="n">float32</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matrix_mul_shared</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Matrix multiplication using shared memory for better performance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Define shared memory arrays</span>
    <span class="n">sA</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">sB</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Thread and block indices</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">bx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span>

    <span class="c1"># Calculate global thread position</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">ty</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">tx</span>

    <span class="c1"># Initialize accumulator</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Loop over tiles</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)):</span>
        <span class="c1"># Load data into shared memory</span>
        <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">sA</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sA</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">if</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">sB</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ty</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sB</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Synchronize threads</span>
        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

        <span class="c1"># Compute partial dot product</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">sA</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sB</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span>

        <span class="c1"># Synchronize before loading next tile</span>
        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="c1"># Write result</span>
    <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>

<span class="c1"># Example usage</span>
<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_matrix_multiplication</span><span class="p">():</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># GPU computation</span>
    <span class="n">d_A</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">d_B</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">d_C</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

    <span class="c1"># Configure grid and block dimensions</span>
    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">blocks_per_grid_x</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">blocks_per_grid_y</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blocks_per_grid_x</span><span class="p">,</span> <span class="n">blocks_per_grid_y</span><span class="p">)</span>

    <span class="c1"># Warm up</span>
    <span class="n">matrix_mul_shared</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Benchmark</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">matrix_mul_shared</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">gpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="c1"># Copy result back</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">d_C</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>

    <span class="c1"># CPU comparison</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">cpu_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
    <span class="n">cpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU time: </span><span class="si">{</span><span class="n">gpu_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU time: </span><span class="si">{</span><span class="n">cpu_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup: </span><span class="si">{</span><span class="n">cpu_time</span><span class="o">/</span><span class="n">gpu_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results match: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_result</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">benchmark_matrix_multiplication</span><span class="p">()</span>
</code></pre></div>
<h4 id="memory-management-and-streams"><strong>Memory Management and Streams</strong><a class="headerlink" href="#memory-management-and-streams" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_kernel</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Complex computation kernel&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
        <span class="c1"># Simulate complex computation</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Intensive computation</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">temp</span> <span class="o">*</span> <span class="mf">1.01</span> <span class="o">+</span> <span class="mf">0.001</span>
        <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">async_processing_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate asynchronous processing with streams&quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="n">num_streams</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">num_streams</span>

    <span class="c1"># Allocate pinned memory for faster transfers</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">pinned_array</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">pinned_array</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Initialize data</span>
    <span class="n">data</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Create streams</span>
    <span class="n">streams</span> <span class="o">=</span> <span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_streams</span><span class="p">)]</span>

    <span class="c1"># Allocate device memory</span>
    <span class="n">d_data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">d_result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Process chunks asynchronously</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_streams</span><span class="p">):</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">current_chunk_size</span> <span class="o">=</span> <span class="n">end_idx</span> <span class="o">-</span> <span class="n">start_idx</span>

        <span class="k">with</span> <span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="c1"># Copy data to device</span>
            <span class="n">d_data</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

            <span class="c1"># Launch kernel</span>
            <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
            <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_chunk_size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>
            <span class="n">compute_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span>
                <span class="n">d_data</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span> 
                <span class="n">d_result</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span> 
                <span class="n">current_chunk_size</span>
            <span class="p">)</span>

            <span class="c1"># Copy result back</span>
            <span class="n">result</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_result</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

    <span class="c1"># Wait for all streams to complete</span>
    <span class="k">for</span> <span class="n">stream</span> <span class="ow">in</span> <span class="n">streams</span><span class="p">:</span>
        <span class="n">stream</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="n">total_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Async processing completed in </span><span class="si">{</span><span class="n">total_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Throughput: </span><span class="si">{</span><span class="n">N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> M elements/sec&quot;</span><span class="p">)</span>

<span class="n">async_processing_example</span><span class="p">()</span>
</code></pre></div>
<h4 id="performance-profiling-with-numba"><strong>Performance Profiling with Numba</strong><a class="headerlink" href="#performance-profiling-with-numba" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>

<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cuda_timer</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager for timing CUDA operations&quot;&quot;&quot;</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">event</span><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">event</span><span class="p">()</span>

    <span class="n">start</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
    <span class="k">yield</span>
    <span class="n">end</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
    <span class="n">end</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">event_elapsed_time</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA operation took: </span><span class="si">{</span><span class="n">elapsed_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

<span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">reduction_kernel</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">partial_sums</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parallel reduction using shared memory&quot;&quot;&quot;</span>
    <span class="c1"># Shared memory for this block</span>
    <span class="n">shared</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">bid</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bid</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">tid</span>

    <span class="c1"># Load data into shared memory</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">shared</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shared</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="c1"># Perform reduction in shared memory</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">while</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
            <span class="n">shared</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>
        <span class="n">stride</span> <span class="o">//=</span> <span class="mi">2</span>

    <span class="c1"># Write result for this block</span>
    <span class="k">if</span> <span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">partial_sums</span><span class="p">[</span><span class="n">bid</span><span class="p">]</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">optimized_reduction</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimized parallel reduction&quot;&quot;&quot;</span>
    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

    <span class="c1"># Allocate memory for partial sums</span>
    <span class="n">partial_sums</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">cuda_timer</span><span class="p">():</span>
        <span class="n">reduction_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="n">partial_sums</span><span class="p">)</span>

        <span class="c1"># Final reduction on CPU (small array)</span>
        <span class="n">host_partial</span> <span class="o">=</span> <span class="n">partial_sums</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
        <span class="n">final_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">host_partial</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">final_sum</span>

<span class="c1"># Performance comparison</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">7</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Performance Comparison:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

<span class="c1"># CPU reduction</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">cpu_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">cpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU sum: </span><span class="si">{</span><span class="n">cpu_sum</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (Time: </span><span class="si">{</span><span class="n">cpu_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s)&quot;</span><span class="p">)</span>

<span class="c1"># GPU reduction</span>
<span class="n">gpu_sum</span> <span class="o">=</span> <span class="n">optimized_reduction</span><span class="p">(</span><span class="n">d_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU sum: </span><span class="si">{</span><span class="n">gpu_sum</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup: </span><span class="si">{</span><span class="n">cpu_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">0.001</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>  <span class="c1"># Approximate GPU time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results match: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">cpu_sum</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">gpu_sum</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">1e-3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="installation-and-setup">🐳 Installation and Setup<a class="headerlink" href="#installation-and-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Install Numba with CUDA support</span>
pip<span class="w"> </span>install<span class="w"> </span>numba

<span class="c1"># Verify CUDA support</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from numba import cuda; print(&#39;CUDA available:&#39;, cuda.is_available())&quot;</span>
</code></pre></div>
<p>Ensure container is run with <code>--runtime=nvidia</code></p>
<hr />
<h2 id="advanced-optimization-techniques">🚀 Advanced Optimization Techniques<a class="headerlink" href="#advanced-optimization-techniques" title="Permanent link">&para;</a></h2>
<h3 id="performance-profiling-and-analysis">📊 Performance Profiling and Analysis<a class="headerlink" href="#performance-profiling-and-analysis" title="Permanent link">&para;</a></h3>
<h4 id="nvidia-nsight-systems-integration"><strong>NVIDIA Nsight Systems Integration</strong><a class="headerlink" href="#nvidia-nsight-systems-integration" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>

<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">nsight_profile</span><span class="p">(</span><span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;profile.nsys-rep&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Context manager for NVIDIA Nsight Systems profiling&quot;&quot;&quot;</span>
    <span class="c1"># Start profiling</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;nsys start --output=</span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cmd</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="c1"># Stop profiling</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;nsys&quot;</span><span class="p">,</span> <span class="s2">&quot;stop&quot;</span><span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Profile saved to: </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;View with: nsys-ui </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Example usage with profiling</span>
<span class="k">def</span><span class="w"> </span><span class="nf">profile_gpu_workload</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Example of profiling GPU workload&quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

    <span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">compute_intensive_kernel</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">iterations</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">temp</span> <span class="o">*</span> <span class="mf">1.1</span> <span class="o">+</span> <span class="mf">0.01</span>
            <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>

    <span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">d_data</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">d_result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

    <span class="k">with</span> <span class="n">nsight_profile</span><span class="p">(</span><span class="s2">&quot;gpu_workload.nsys-rep&quot;</span><span class="p">):</span>
        <span class="c1"># Multiple kernel launches for profiling</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">compute_intensive_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span>
                <span class="n">d_data</span><span class="p">,</span> <span class="n">d_result</span><span class="p">,</span> <span class="mi">100</span>
            <span class="p">)</span>
            <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

<span class="c1"># profile_gpu_workload()  # Uncomment to run profiling</span>
</code></pre></div>
<h4 id="custom-performance-metrics"><strong>Custom Performance Metrics</strong><a class="headerlink" href="#custom-performance-metrics" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">GPUtil</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PerformanceMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Container for performance metrics&quot;&quot;&quot;</span>
    <span class="n">execution_time</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">gpu_utilization</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">gpu_memory_used</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">gpu_memory_total</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">cpu_utilization</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">system_memory_used</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">power_consumption</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="k">class</span><span class="w"> </span><span class="nc">JetsonPerformanceProfiler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Comprehensive performance profiler for Jetson devices&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">PerformanceMetrics</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span> <span class="o">=</span> <span class="n">GPUtil</span><span class="o">.</span><span class="n">getGPUs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">GPUtil</span><span class="o">.</span><span class="n">getGPUs</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_current_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PerformanceMetrics</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Capture current system metrics&quot;&quot;&quot;</span>
        <span class="c1"># GPU metrics</span>
        <span class="n">gpu_util</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">load</span> <span class="o">*</span> <span class="mi">100</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="n">gpu_mem_used</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">memoryUsed</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="n">gpu_mem_total</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">memoryTotal</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu</span> <span class="k">else</span> <span class="mf">0.0</span>

        <span class="c1"># CPU and system metrics</span>
        <span class="n">cpu_util</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">cpu_percent</span><span class="p">(</span><span class="n">interval</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">virtual_memory</span><span class="p">()</span>

        <span class="c1"># Jetson-specific metrics (if available)</span>
        <span class="n">power</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_power_consumption</span><span class="p">()</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_temperature</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">PerformanceMetrics</span><span class="p">(</span>
            <span class="n">execution_time</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># Will be set by context manager</span>
            <span class="n">gpu_utilization</span><span class="o">=</span><span class="n">gpu_util</span><span class="p">,</span>
            <span class="n">gpu_memory_used</span><span class="o">=</span><span class="n">gpu_mem_used</span><span class="p">,</span>
            <span class="n">gpu_memory_total</span><span class="o">=</span><span class="n">gpu_mem_total</span><span class="p">,</span>
            <span class="n">cpu_utilization</span><span class="o">=</span><span class="n">cpu_util</span><span class="p">,</span>
            <span class="n">system_memory_used</span><span class="o">=</span><span class="n">memory</span><span class="o">.</span><span class="n">used</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span>  <span class="c1"># GB</span>
            <span class="n">power_consumption</span><span class="o">=</span><span class="n">power</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temp</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_power_consumption</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get power consumption (Jetson-specific)&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Try to read from Jetson power monitoring</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/sys/bus/i2c/drivers/ina3221x/1-0040/iio:device0/in_power0_input&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">/</span> <span class="mf">1000.0</span>  <span class="c1"># Convert to watts</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_temperature</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get GPU temperature&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/sys/class/thermal/thermal_zone1/temp&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">/</span> <span class="mf">1000.0</span>  <span class="c1"># Convert to Celsius</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">profile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Context manager for profiling code execution&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting profiling: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">start_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_current_metrics</span><span class="p">()</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span> <span class="bp">self</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">end_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_current_metrics</span><span class="p">()</span>

            <span class="c1"># Calculate execution time</span>
            <span class="n">execution_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
            <span class="n">end_metrics</span><span class="o">.</span><span class="n">execution_time</span> <span class="o">=</span> <span class="n">execution_time</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end_metrics</span><span class="p">)</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Profiling completed: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Execution time: </span><span class="si">{</span><span class="n">execution_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU utilization: </span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">gpu_utilization</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU memory: </span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">gpu_memory_used</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">gpu_memory_total</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU utilization: </span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">cpu_utilization</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">end_metrics</span><span class="o">.</span><span class="n">power_consumption</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Power consumption: </span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">power_consumption</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">W&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">end_metrics</span><span class="o">.</span><span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature: </span><span class="si">{</span><span class="n">end_metrics</span><span class="o">.</span><span class="n">temperature</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">°C&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate_report</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;performance_report.json&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate detailed performance report&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No metrics collected yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Calculate statistics</span>
        <span class="n">avg_exec_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">execution_time</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span>
        <span class="n">avg_gpu_util</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">gpu_utilization</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span>
        <span class="n">avg_cpu_util</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">cpu_utilization</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">)</span>

        <span class="n">report</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;total_runs&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">),</span>
                <span class="s2">&quot;average_execution_time&quot;</span><span class="p">:</span> <span class="n">avg_exec_time</span><span class="p">,</span>
                <span class="s2">&quot;average_gpu_utilization&quot;</span><span class="p">:</span> <span class="n">avg_gpu_util</span><span class="p">,</span>
                <span class="s2">&quot;average_cpu_utilization&quot;</span><span class="p">:</span> <span class="n">avg_cpu_util</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="s2">&quot;detailed_metrics&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;execution_time&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">execution_time</span><span class="p">,</span>
                    <span class="s2">&quot;gpu_utilization&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">gpu_utilization</span><span class="p">,</span>
                    <span class="s2">&quot;gpu_memory_used&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">gpu_memory_used</span><span class="p">,</span>
                    <span class="s2">&quot;cpu_utilization&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">cpu_utilization</span><span class="p">,</span>
                    <span class="s2">&quot;power_consumption&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">power_consumption</span><span class="p">,</span>
                    <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">temperature</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span>
            <span class="p">]</span>
        <span class="p">}</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">report</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Performance report saved to: </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">report</span>

<span class="c1"># Example usage</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_profiling</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate comprehensive profiling&quot;&quot;&quot;</span>
    <span class="n">profiler</span> <span class="o">=</span> <span class="n">JetsonPerformanceProfiler</span><span class="p">()</span>

    <span class="c1"># Profile different workloads</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

    <span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">matrix_multiply_kernel</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
        <span class="n">row</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>

        <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">temp</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
            <span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>

    <span class="c1"># Test different matrix sizes</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">d_A</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        <span class="n">d_B</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        <span class="n">d_C</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

        <span class="n">threads_per_block</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">blocks_per_grid_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">blocks_per_grid_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blocks_per_grid_x</span><span class="p">,</span> <span class="n">blocks_per_grid_y</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matrix multiplication </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
            <span class="n">matrix_multiply_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">)</span>
            <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Generate comprehensive report</span>
    <span class="n">profiler</span><span class="o">.</span><span class="n">generate_report</span><span class="p">(</span><span class="s2">&quot;matrix_multiply_performance.json&quot;</span><span class="p">)</span>

<span class="c1"># demonstrate_profiling()  # Uncomment to run</span>
</code></pre></div>
<h3 id="memory-optimization-strategies">⚡ Memory Optimization Strategies<a class="headerlink" href="#memory-optimization-strategies" title="Permanent link">&para;</a></h3>
<h4 id="memory-pool-management"><strong>Memory Pool Management</strong><a class="headerlink" href="#memory-pool-management" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GPUMemoryPool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;GPU memory pool for efficient memory management&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_size_mb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">initial_size_mb</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># Convert to bytes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allocated_blocks</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">free_blocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Pre-allocate memory pool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_pool</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_pool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the memory pool&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Allocate large contiguous block</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory_pool</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initialized GPU memory pool: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">)</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">cuda</span><span class="o">.</span><span class="n">cudadrv</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">CudaDriverError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to allocate memory pool: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory_pool</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_bytes</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Allocate memory from pool&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_pool</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Fallback to regular allocation</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">size_bytes</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">array</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">array</span>
            <span class="k">return</span>

        <span class="c1"># Use memory pool (simplified implementation)</span>
        <span class="k">if</span> <span class="n">size_bytes</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span><span class="p">:</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span> <span class="o">//</span> <span class="mi">4</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">size_bytes</span> <span class="o">//</span> <span class="mi">4</span>
            <span class="n">allocated_view</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_pool</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span> <span class="o">+=</span> <span class="n">size_bytes</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">allocated_view</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="c1"># In a real implementation, you&#39;d track and reuse freed blocks</span>
                <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">MemoryError</span><span class="p">(</span><span class="s2">&quot;Insufficient memory in pool&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_usage_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get memory pool usage statistics&quot;&quot;&quot;</span>
        <span class="n">used_mb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">total_mb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">usage_percent</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_allocated</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;used_mb&quot;</span><span class="p">:</span> <span class="n">used_mb</span><span class="p">,</span>
            <span class="s2">&quot;total_mb&quot;</span><span class="p">:</span> <span class="n">total_mb</span><span class="p">,</span>
            <span class="s2">&quot;usage_percent&quot;</span><span class="p">:</span> <span class="n">usage_percent</span>
        <span class="p">}</span>

<span class="c1"># Example usage</span>
<span class="k">def</span><span class="w"> </span><span class="nf">memory_pool_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate memory pool usage&quot;&quot;&quot;</span>
    <span class="n">pool</span> <span class="o">=</span> <span class="n">GPUMemoryPool</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>  <span class="c1"># 256 MB pool</span>

    <span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">process_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">1.0</span>

    <span class="c1"># Process multiple batches using memory pool</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 1M elements</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">pool</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">gpu_data</span><span class="p">:</span>  <span class="c1"># 4 bytes per float32</span>
            <span class="k">with</span> <span class="n">pool</span><span class="o">.</span><span class="n">allocate</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">gpu_result</span><span class="p">:</span>
                <span class="c1"># Copy data to GPU</span>
                <span class="n">gpu_data_view</span> <span class="o">=</span> <span class="n">gpu_data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="n">gpu_result_view</span> <span class="o">=</span> <span class="n">gpu_result</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

                <span class="n">gpu_data_view</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">data</span>

                <span class="c1"># Process data</span>
                <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
                <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>
                <span class="n">process_data</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">gpu_data_view</span><span class="p">,</span> <span class="n">gpu_result_view</span><span class="p">)</span>

                <span class="c1"># Copy result back</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">gpu_result_view</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>

                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">batch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> processed. Pool usage: </span><span class="si">{</span><span class="n">pool</span><span class="o">.</span><span class="n">get_usage_stats</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># memory_pool_example()  # Uncomment to run</span>
</code></pre></div>
<h4 id="unified-memory-management"><strong>Unified Memory Management</strong><a class="headerlink" href="#unified-memory-management" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">UnifiedMemoryManager</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manager for CUDA Unified Memory&quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">allocate_unified</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Allocate unified memory accessible from both CPU and GPU&quot;&quot;&quot;</span>
        <span class="c1"># Note: This is a conceptual example</span>
        <span class="c1"># Actual unified memory allocation depends on CUDA version and hardware</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Allocate pinned memory for better performance</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">pinned_array</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">array</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="c1"># Fallback to regular numpy array</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prefetch_to_gpu</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prefetch unified memory to GPU&quot;&quot;&quot;</span>
        <span class="c1"># This would use cudaMemPrefetchAsync in actual CUDA code</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="s1">&#39;copy_to_device&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prefetch_to_cpu</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prefetch unified memory to CPU&quot;&quot;&quot;</span>
        <span class="c1"># This would use cudaMemPrefetchAsync in actual CUDA code</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="s1">&#39;copy_to_host&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">array</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">unified_memory_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate unified memory usage patterns&quot;&quot;&quot;</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">UnifiedMemoryManager</span><span class="p">()</span>

    <span class="c1"># Allocate large dataset</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 10M elements</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">allocate_unified</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">data</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">compute_statistics</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mean_result</span><span class="p">,</span> <span class="n">std_result</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute mean and standard deviation&quot;&quot;&quot;</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Shared memory for reduction</span>
        <span class="n">shared_sum</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">shared_sq_sum</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>

        <span class="c1"># Initialize shared memory</span>
        <span class="n">shared_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">shared_sq_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Each thread processes multiple elements</span>
        <span class="n">elements_per_thread</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">elements_per_thread</span><span class="p">):</span>
            <span class="n">data_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
            <span class="k">if</span> <span class="n">data_idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span>
                <span class="n">shared_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">val</span>
                <span class="n">shared_sq_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">val</span>

        <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

        <span class="c1"># Reduction in shared memory</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">while</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
                <span class="n">shared_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared_sum</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
                <span class="n">shared_sq_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared_sq_sum</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
            <span class="n">cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>
            <span class="n">stride</span> <span class="o">//=</span> <span class="mi">2</span>

        <span class="c1"># Write block results</span>
        <span class="k">if</span> <span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cuda</span><span class="o">.</span><span class="n">atomic</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mean_result</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">shared_sum</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">cuda</span><span class="o">.</span><span class="n">atomic</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">std_result</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">shared_sq_sum</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Allocate result arrays</span>
    <span class="n">mean_result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">std_result</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Initialize results</span>
    <span class="n">mean_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">std_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Prefetch data to GPU</span>
    <span class="n">gpu_data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">prefetch_to_gpu</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Launch kernel</span>
    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">)</span>

    <span class="n">compute_statistics</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">gpu_data</span><span class="p">,</span> <span class="n">mean_result</span><span class="p">,</span> <span class="n">std_result</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Get results</span>
    <span class="n">mean_val</span> <span class="o">=</span> <span class="n">mean_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="p">(</span><span class="n">std_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">mean_val</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">std_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU computed statistics:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean: </span><span class="si">{</span><span class="n">mean_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std:  </span><span class="si">{</span><span class="n">std_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify with CPU</span>
    <span class="n">cpu_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">cpu_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">CPU verification:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean: </span><span class="si">{</span><span class="n">cpu_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std:  </span><span class="si">{</span><span class="n">cpu_std</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Difference:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean diff: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mean_val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cpu_mean</span><span class="p">)</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std diff:  </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">std_val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cpu_std</span><span class="p">)</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># unified_memory_example()  # Uncomment to run</span>
</code></pre></div>
<hr />
<h2 id="numpy-with-cupy-gpu-drop-in-replacement">📊 NumPy with CuPy (GPU drop-in replacement)<a class="headerlink" href="#numpy-with-cupy-gpu-drop-in-replacement" title="Permanent link">&para;</a></h2>
<p>CuPy mimics NumPy's API but uses CUDA arrays and streams.</p>
<h3 id="cupy-example">✅ CuPy Example<a class="headerlink" href="#cupy-example" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">cupy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cp</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">cp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum:&quot;</span><span class="p">,</span> <span class="n">cp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</code></pre></div>
<h3 id="in-container">🐳 In Container<a class="headerlink" href="#in-container" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>cupy-cuda11x<span class="w">  </span><span class="c1"># Replace with correct Jetson CUDA version</span>
</code></pre></div>
<hr />
<h2 id="pytorch-on-gpu">🔥 PyTorch on GPU<a class="headerlink" href="#pytorch-on-gpu" title="Permanent link">&para;</a></h2>
<p>Jetson comes with optimized PyTorch preinstalled in the NVIDIA container.</p>
<h3 id="pytorch-matrix-multiplication">✅ PyTorch Matrix Multiplication<a class="headerlink" href="#pytorch-matrix-multiplication" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum:&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</code></pre></div>
<h3 id="simple-nn-training-with-loss">✅ Simple NN Training (with loss)<a class="headerlink" href="#simple-nn-training-with-loss" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="performance-measurement-tools">📈 Performance Measurement Tools<a class="headerlink" href="#performance-measurement-tools" title="Permanent link">&para;</a></h2>
<h3 id="torchcudaevent"><code>torch.cuda.Event</code><a class="headerlink" href="#torchcudaevent" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">start</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
<span class="c1"># ... run model</span>
<span class="n">end</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Time:&quot;</span><span class="p">,</span> <span class="n">start</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end</span><span class="p">),</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="tegrastats-outside-container"><code>tegrastats</code> (outside container)<a class="headerlink" href="#tegrastats-outside-container" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>tegrastats
</code></pre></div>
<p>Use <code>tegrastats</code> in host shell to monitor GPU usage, power, memory.</p>
<hr />
<h2 id="comprehensive-lab-accelerated-computing-mastery">🧪 Comprehensive Lab: Accelerated Computing Mastery<a class="headerlink" href="#comprehensive-lab-accelerated-computing-mastery" title="Permanent link">&para;</a></h2>
<h3 id="lab-objectives">Lab Objectives<a class="headerlink" href="#lab-objectives" title="Permanent link">&para;</a></h3>
<p>By completing this lab, you will:</p>
<ol>
<li><strong>Analyze GPU Architecture</strong>: Understand hardware capabilities and limitations</li>
<li><strong>Implement Optimized Algorithms</strong>: Apply memory hierarchy and parallel computing principles</li>
<li><strong>Compare Acceleration Frameworks</strong>: Evaluate performance across different tools</li>
<li><strong>Profile and Optimize</strong>: Use advanced profiling techniques for performance tuning</li>
<li><strong>Deploy Real-world Solutions</strong>: Create production-ready accelerated applications</li>
</ol>
<h3 id="lab-setup">Lab Setup<a class="headerlink" href="#lab-setup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Start Jetson container with all tools</span>
docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/workspace<span class="w"> </span>-w<span class="w"> </span>/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--shm-size<span class="o">=</span>2g<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>nvcr.io/nvidia/pytorch:24.04-py3<span class="w"> </span>/bin/bash

<span class="c1"># Install additional dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>matplotlib<span class="w"> </span>seaborn<span class="w"> </span>pycuda<span class="w"> </span>cupy-cuda11x
</code></pre></div>
<h3 id="exercise-1-gpu-architecture-analysis">Exercise 1: GPU Architecture Analysis<a class="headerlink" href="#exercise-1-gpu-architecture-analysis" title="Permanent link">&para;</a></h3>
<h4 id="task-11-hardware-profiling">Task 1.1: Hardware Profiling<a class="headerlink" href="#task-11-hardware-profiling" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pycuda.driver</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numba</span><span class="w"> </span><span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">numba_cuda</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">analyze_gpu_architecture</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Comprehensive GPU architecture analysis&quot;&quot;&quot;</span>

    <span class="c1"># Initialize CUDA</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">make_context</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get device properties</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">get_attributes</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== GPU Architecture Analysis ===&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device Name: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">name</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compute Capability: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">compute_capability</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total Memory: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">total_memory</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiprocessors: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MULTIPROCESSOR_COUNT</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA Cores per MP: </span><span class="si">{</span><span class="n">_get_cores_per_mp</span><span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">compute_capability</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total CUDA Cores: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MULTIPROCESSOR_COUNT</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">_get_cores_per_mp</span><span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">compute_capability</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Threads per Block: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_THREADS_PER_BLOCK</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Block Dimensions: (</span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_X</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_Y</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_BLOCK_DIM_Z</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max Grid Dimensions: (</span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_X</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_Y</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_GRID_DIM_Z</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shared Memory per Block: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_SHARED_MEMORY_PER_BLOCK</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> KB&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Registers per Block: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MAX_REGISTERS_PER_BLOCK</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warp Size: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">WARP_SIZE</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory Clock Rate: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MEMORY_CLOCK_RATE</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1000</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> MHz&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory Bus Width: </span><span class="si">{</span><span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">GLOBAL_MEMORY_BUS_WIDTH</span><span class="p">]</span><span class="si">}</span><span class="s2"> bits&quot;</span><span class="p">)</span>

        <span class="c1"># Calculate theoretical bandwidth</span>
        <span class="n">memory_clock_hz</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MEMORY_CLOCK_RATE</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">bus_width_bytes</span> <span class="o">=</span> <span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">GLOBAL_MEMORY_BUS_WIDTH</span><span class="p">]</span> <span class="o">//</span> <span class="mi">8</span>
        <span class="n">theoretical_bandwidth</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">memory_clock_hz</span> <span class="o">*</span> <span class="n">bus_width_bytes</span> <span class="o">/</span> <span class="mf">1e9</span>  <span class="c1"># GB/s</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Memory Bandwidth: </span><span class="si">{</span><span class="n">theoretical_bandwidth</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> GB/s&quot;</span><span class="p">)</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">context</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">device</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span>
        <span class="s1">&#39;compute_capability&#39;</span><span class="p">:</span> <span class="n">device</span><span class="o">.</span><span class="n">compute_capability</span><span class="p">(),</span>
        <span class="s1">&#39;multiprocessors&#39;</span><span class="p">:</span> <span class="n">attrs</span><span class="p">[</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_attribute</span><span class="o">.</span><span class="n">MULTIPROCESSOR_COUNT</span><span class="p">],</span>
        <span class="s1">&#39;theoretical_bandwidth&#39;</span><span class="p">:</span> <span class="n">theoretical_bandwidth</span>
    <span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_get_cores_per_mp</span><span class="p">(</span><span class="n">compute_capability</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get CUDA cores per multiprocessor based on compute capability&quot;&quot;&quot;</span>
    <span class="n">major</span><span class="p">,</span> <span class="n">minor</span> <span class="o">=</span> <span class="n">compute_capability</span>
    <span class="k">if</span> <span class="n">major</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>  <span class="c1"># Ampere (Orin)</span>
        <span class="k">return</span> <span class="mi">128</span>
    <span class="k">elif</span> <span class="n">major</span> <span class="o">==</span> <span class="mi">7</span><span class="p">:</span>  <span class="c1"># Turing/Volta</span>
        <span class="k">return</span> <span class="mi">64</span>
    <span class="k">elif</span> <span class="n">major</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>  <span class="c1"># Pascal</span>
        <span class="k">return</span> <span class="mi">64</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">32</span>  <span class="c1"># Default</span>

<span class="c1"># Run architecture analysis</span>
<span class="n">gpu_info</span> <span class="o">=</span> <span class="n">analyze_gpu_architecture</span><span class="p">()</span>
</code></pre></div>
<h4 id="task-12-memory-bandwidth-benchmark">Task 1.2: Memory Bandwidth Benchmark<a class="headerlink" href="#task-12-memory-bandwidth-benchmark" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">memory_bandwidth_kernel</span><span class="p">(</span><span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple memory copy kernel for bandwidth testing&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">data_out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_memory_bandwidth</span><span class="p">(</span><span class="n">sizes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark memory bandwidth for different data sizes&quot;&quot;&quot;</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
        <span class="c1"># Allocate data</span>
        <span class="n">data_in</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">data_out</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Configure kernel launch</span>
        <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

        <span class="c1"># Warm up</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
            <span class="n">memory_bandwidth_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="c1"># Benchmark</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">memory_bandwidth_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">data_in</span><span class="p">,</span> <span class="n">data_out</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
            <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

        <span class="n">avg_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
        <span class="n">data_size_gb</span> <span class="o">=</span> <span class="n">size</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="mf">1e9</span>  <span class="c1"># Read + Write, 4 bytes per float32</span>
        <span class="n">bandwidth</span> <span class="o">=</span> <span class="n">data_size_gb</span> <span class="o">/</span> <span class="n">avg_time</span>

        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="n">size</span><span class="p">,</span>
            <span class="s1">&#39;time_ms&#39;</span><span class="p">:</span> <span class="n">avg_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span>
            <span class="s1">&#39;bandwidth_gb_s&#39;</span><span class="p">:</span> <span class="n">bandwidth</span>
        <span class="p">})</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Size: </span><span class="si">{</span><span class="n">size</span><span class="si">:</span><span class="s2">&gt;10,</span><span class="si">}</span><span class="s2"> elements, Time: </span><span class="si">{</span><span class="n">avg_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2"> ms, Bandwidth: </span><span class="si">{</span><span class="n">bandwidth</span><span class="si">:</span><span class="s2">6.1f</span><span class="si">}</span><span class="s2"> GB/s&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Run bandwidth benchmark</span>
<span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">16384</span><span class="p">,</span> <span class="mi">65536</span><span class="p">,</span> <span class="mi">262144</span><span class="p">,</span> <span class="mi">1048576</span><span class="p">,</span> <span class="mi">4194304</span><span class="p">,</span> <span class="mi">16777216</span><span class="p">]</span>
<span class="n">bandwidth_results</span> <span class="o">=</span> <span class="n">benchmark_memory_bandwidth</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>

<span class="c1"># Plot results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sizes_mb</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;size&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mf">1e6</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">bandwidth_results</span><span class="p">]</span>
<span class="n">bandwidths</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;bandwidth_gb_s&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">bandwidth_results</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes_mb</span><span class="p">,</span> <span class="n">bandwidths</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Data Size (MB)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Bandwidth (GB/s)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Memory Bandwidth vs Data Size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;time_ms&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">bandwidth_results</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes_mb</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Data Size (MB)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Execution Time vs Data Size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;memory_bandwidth_analysis.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h4 id="task-13-occupancy-analysis">Task 1.3: Occupancy Analysis<a class="headerlink" href="#task-13-occupancy-analysis" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">analyze_occupancy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analyze kernel occupancy for different configurations&quot;&quot;&quot;</span>

    <span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">test_kernel</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">shared_size</span><span class="p">):</span>
        <span class="c1"># Allocate shared memory</span>
        <span class="n">shared_mem</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shared_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numba_cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tid</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>

        <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="c1"># Use shared memory</span>
            <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">shared_size</span><span class="p">:</span>
                <span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

            <span class="n">numba_cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

            <span class="c1"># Simple computation</span>
            <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">shared_size</span><span class="p">:</span>
                <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

    <span class="c1"># Test different configurations</span>
    <span class="n">data_size</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">data_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">configurations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>   <span class="c1"># threads_per_block, shared_memory_size</span>
        <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Occupancy Analysis ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Threads/Block&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Shared Mem&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Blocks/Grid&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Time (ms)&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Occupancy&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">threads_per_block</span><span class="p">,</span> <span class="n">shared_mem_size</span> <span class="ow">in</span> <span class="n">configurations</span><span class="p">:</span>
        <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

        <span class="c1"># Warm up</span>
        <span class="n">test_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="n">shared_mem_size</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="c1"># Benchmark</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">test_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="n">shared_mem_size</span><span class="p">)</span>
            <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

        <span class="n">avg_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>

        <span class="c1"># Estimate occupancy (simplified)</span>
        <span class="n">max_threads_per_sm</span> <span class="o">=</span> <span class="mi">2048</span>  <span class="c1"># Typical for modern GPUs</span>
        <span class="n">estimated_occupancy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">threads_per_block</span> <span class="o">*</span> <span class="n">blocks_per_grid</span> <span class="o">/</span> <span class="n">max_threads_per_sm</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">threads_per_block</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">shared_mem_size</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">blocks_per_grid</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">avg_time</span><span class="si">:</span><span class="s2">&lt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">estimated_occupancy</span><span class="si">:</span><span class="s2">&lt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">analyze_occupancy</span><span class="p">()</span>
</code></pre></div>
<h3 id="exercise-2-parallel-algorithm-implementation">Exercise 2: Parallel Algorithm Implementation<a class="headerlink" href="#exercise-2-parallel-algorithm-implementation" title="Permanent link">&para;</a></h3>
<h4 id="task-21-optimized-parallel-reduction">Task 2.1: Optimized Parallel Reduction<a class="headerlink" href="#task-21-optimized-parallel-reduction" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">parallel_reduction_optimized</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimized parallel reduction with multiple optimizations&quot;&quot;&quot;</span>

    <span class="c1"># Shared memory for this block</span>
    <span class="n">shared_data</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numba_cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">tid</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">bid</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="n">grid_size</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span>

    <span class="c1"># Global thread ID</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bid</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span> <span class="n">tid</span>

    <span class="c1"># Initialize shared memory</span>
    <span class="n">shared_data</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Grid-stride loop for coalesced memory access</span>
    <span class="k">while</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">shared_data</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="n">grid_size</span> <span class="o">*</span> <span class="n">block_size</span>

    <span class="n">numba_cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="c1"># Parallel reduction in shared memory</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">block_size</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">while</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
            <span class="n">shared_data</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared_data</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>
        <span class="n">stride</span> <span class="o">//=</span> <span class="mi">2</span>

    <span class="c1"># Write block result</span>
    <span class="k">if</span> <span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">atomic</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">shared_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_reduction_methods</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare different reduction implementations&quot;&quot;&quot;</span>

    <span class="c1"># Generate test data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">host_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">device_data</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">host_data</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Reduction Benchmark (Size: </span><span class="si">{</span><span class="n">size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">) ===&quot;</span><span class="p">)</span>

    <span class="c1"># CPU reference</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">cpu_result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">host_data</span><span class="p">)</span>
    <span class="n">cpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU NumPy:           </span><span class="si">{</span><span class="n">cpu_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms, Result: </span><span class="si">{</span><span class="n">cpu_result</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># GPU optimized reduction</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">)</span>

    <span class="c1"># Warm up</span>
    <span class="n">parallel_reduction_optimized</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">device_data</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
    <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Benchmark</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">parallel_reduction_optimized</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">device_data</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">gpu_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
    <span class="n">gpu_result</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">speedup</span> <span class="o">=</span> <span class="n">cpu_time</span> <span class="o">/</span> <span class="n">gpu_time</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU Optimized:       </span><span class="si">{</span><span class="n">gpu_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms, Result: </span><span class="si">{</span><span class="n">gpu_result</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2">, Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">6.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">cpu_result</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">gpu_result</span><span class="p">)</span><span class="si">:</span><span class="s2">12.8f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;cpu_time&#39;</span><span class="p">:</span> <span class="n">cpu_time</span><span class="p">,</span>
        <span class="s1">&#39;gpu_time&#39;</span><span class="p">:</span> <span class="n">gpu_time</span><span class="p">,</span>
        <span class="s1">&#39;speedup&#39;</span><span class="p">:</span> <span class="n">speedup</span><span class="p">,</span>
        <span class="s1">&#39;error&#39;</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">cpu_result</span> <span class="o">-</span> <span class="n">gpu_result</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1"># Test different sizes</span>
<span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10240</span><span class="p">,</span> <span class="mi">102400</span><span class="p">,</span> <span class="mi">1024000</span><span class="p">,</span> <span class="mi">10240000</span><span class="p">]:</span>
    <span class="n">benchmark_reduction_methods</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
</code></pre></div>
<h4 id="task-22-matrix-transpose-optimization">Task 2.2: Matrix Transpose Optimization<a class="headerlink" href="#task-22-matrix-transpose-optimization" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matrix_transpose_naive</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Naive matrix transpose implementation&quot;&quot;&quot;</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>

    <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">rows</span> <span class="ow">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">cols</span><span class="p">:</span>
        <span class="n">output_matrix</span><span class="p">[</span><span class="n">col</span><span class="p">,</span> <span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_matrix</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>

<span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">matrix_transpose_optimized</span><span class="p">(</span><span class="n">input_matrix</span><span class="p">,</span> <span class="n">output_matrix</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimized matrix transpose with shared memory tiling&quot;&quot;&quot;</span>

    <span class="n">TILE_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="c1"># Shared memory tile</span>
    <span class="n">tile</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">shared</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="n">TILE_SIZE</span><span class="p">,</span> <span class="n">TILE_SIZE</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numba_cuda</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Block and thread indices</span>
    <span class="n">bx</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>

    <span class="c1"># Global coordinates for input</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">ty</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tx</span>

    <span class="c1"># Load tile into shared memory</span>
    <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">rows</span> <span class="ow">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">cols</span><span class="p">:</span>
        <span class="n">tile</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_matrix</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tile</span><span class="p">[</span><span class="n">ty</span><span class="p">,</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="n">numba_cuda</span><span class="o">.</span><span class="n">syncthreads</span><span class="p">()</span>

    <span class="c1"># Global coordinates for output (transposed)</span>
    <span class="n">row_out</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">ty</span>
    <span class="n">col_out</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE_SIZE</span> <span class="o">+</span> <span class="n">tx</span>

    <span class="c1"># Write transposed tile to output</span>
    <span class="k">if</span> <span class="n">row_out</span> <span class="o">&lt;</span> <span class="n">cols</span> <span class="ow">and</span> <span class="n">col_out</span> <span class="o">&lt;</span> <span class="n">rows</span><span class="p">:</span>
        <span class="n">output_matrix</span><span class="p">[</span><span class="n">row_out</span><span class="p">,</span> <span class="n">col_out</span><span class="p">]</span> <span class="o">=</span> <span class="n">tile</span><span class="p">[</span><span class="n">tx</span><span class="p">,</span> <span class="n">ty</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_transpose_methods</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare naive vs optimized matrix transpose&quot;&quot;&quot;</span>

    <span class="c1"># Generate test data</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">host_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">device_input</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">host_input</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Matrix Transpose Benchmark (</span><span class="si">{</span><span class="n">rows</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">cols</span><span class="si">}</span><span class="s2">) ===&quot;</span><span class="p">)</span>

    <span class="c1"># CPU reference</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">cpu_result</span> <span class="o">=</span> <span class="n">host_input</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">cpu_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CPU NumPy:           </span><span class="si">{</span><span class="n">cpu_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

    <span class="c1"># GPU Naive implementation</span>
    <span class="n">device_output_naive</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">((</span><span class="n">cols</span><span class="p">,</span> <span class="n">rows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">threads_per_block</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">blocks_per_grid_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">cols</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">blocks_per_grid_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blocks_per_grid_x</span><span class="p">,</span> <span class="n">blocks_per_grid_y</span><span class="p">)</span>

    <span class="c1"># Warm up</span>
    <span class="n">matrix_transpose_naive</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">device_input</span><span class="p">,</span> <span class="n">device_output_naive</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
    <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Benchmark naive</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">matrix_transpose_naive</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">device_input</span><span class="p">,</span> <span class="n">device_output_naive</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">naive_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
    <span class="n">naive_result</span> <span class="o">=</span> <span class="n">device_output_naive</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>

    <span class="c1"># GPU Optimized implementation</span>
    <span class="n">device_output_opt</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">((</span><span class="n">cols</span><span class="p">,</span> <span class="n">rows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">TILE_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">threads_per_block_opt</span> <span class="o">=</span> <span class="p">(</span><span class="n">TILE_SIZE</span><span class="p">,</span> <span class="n">TILE_SIZE</span><span class="p">)</span>
    <span class="n">blocks_per_grid_x_opt</span> <span class="o">=</span> <span class="p">(</span><span class="n">cols</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">TILE_SIZE</span>
    <span class="n">blocks_per_grid_y_opt</span> <span class="o">=</span> <span class="p">(</span><span class="n">rows</span> <span class="o">+</span> <span class="n">TILE_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">TILE_SIZE</span>
    <span class="n">blocks_per_grid_opt</span> <span class="o">=</span> <span class="p">(</span><span class="n">blocks_per_grid_x_opt</span><span class="p">,</span> <span class="n">blocks_per_grid_y_opt</span><span class="p">)</span>

    <span class="c1"># Warm up</span>
    <span class="n">matrix_transpose_optimized</span><span class="p">[</span><span class="n">blocks_per_grid_opt</span><span class="p">,</span> <span class="n">threads_per_block_opt</span><span class="p">](</span><span class="n">device_input</span><span class="p">,</span> <span class="n">device_output_opt</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
    <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># Benchmark optimized</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">matrix_transpose_optimized</span><span class="p">[</span><span class="n">blocks_per_grid_opt</span><span class="p">,</span> <span class="n">threads_per_block_opt</span><span class="p">](</span><span class="n">device_input</span><span class="p">,</span> <span class="n">device_output_opt</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">opt_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
    <span class="n">opt_result</span> <span class="o">=</span> <span class="n">device_output_opt</span><span class="o">.</span><span class="n">copy_to_host</span><span class="p">()</span>

    <span class="c1"># Calculate speedups and errors</span>
    <span class="n">cpu_speedup</span> <span class="o">=</span> <span class="n">cpu_time</span> <span class="o">/</span> <span class="n">naive_time</span>
    <span class="n">opt_speedup</span> <span class="o">=</span> <span class="n">naive_time</span> <span class="o">/</span> <span class="n">opt_time</span>
    <span class="n">naive_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">cpu_result</span> <span class="o">-</span> <span class="n">naive_result</span><span class="p">))</span>
    <span class="n">opt_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">cpu_result</span> <span class="o">-</span> <span class="n">opt_result</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU Naive:           </span><span class="si">{</span><span class="n">naive_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms, Speedup vs CPU: </span><span class="si">{</span><span class="n">cpu_speedup</span><span class="si">:</span><span class="s2">6.1f</span><span class="si">}</span><span class="s2">x, Error: </span><span class="si">{</span><span class="n">naive_error</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU Optimized:       </span><span class="si">{</span><span class="n">opt_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms, Speedup vs Naive: </span><span class="si">{</span><span class="n">opt_speedup</span><span class="si">:</span><span class="s2">6.1f</span><span class="si">}</span><span class="s2">x, Error: </span><span class="si">{</span><span class="n">opt_error</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;cpu_time&#39;</span><span class="p">:</span> <span class="n">cpu_time</span><span class="p">,</span>
        <span class="s1">&#39;naive_time&#39;</span><span class="p">:</span> <span class="n">naive_time</span><span class="p">,</span>
        <span class="s1">&#39;opt_time&#39;</span><span class="p">:</span> <span class="n">opt_time</span><span class="p">,</span>
        <span class="s1">&#39;naive_speedup&#39;</span><span class="p">:</span> <span class="n">cpu_speedup</span><span class="p">,</span>
        <span class="s1">&#39;opt_speedup&#39;</span><span class="p">:</span> <span class="n">opt_speedup</span>
    <span class="p">}</span>

<span class="c1"># Test different matrix sizes</span>
<span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)]:</span>
    <span class="n">benchmark_transpose_methods</span><span class="p">(</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<h3 id="exercise-3-framework-performance-comparison">Exercise 3: Framework Performance Comparison<a class="headerlink" href="#exercise-3-framework-performance-comparison" title="Permanent link">&para;</a></h3>
<h4 id="task-31-comprehensive-framework-benchmark">Task 3.1: Comprehensive Framework Benchmark<a class="headerlink" href="#task-31-comprehensive-framework-benchmark" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">cupy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_frameworks</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Comprehensive benchmark across all frameworks&quot;&quot;&quot;</span>

    <span class="c1"># Test configurations</span>
    <span class="n">matrix_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>
    <span class="n">vector_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10240</span><span class="p">,</span> <span class="mi">102400</span><span class="p">,</span> <span class="mi">1024000</span><span class="p">,</span> <span class="mi">10240000</span><span class="p">]</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Framework Performance Comparison ===&quot;</span><span class="p">)</span>

    <span class="c1"># Matrix Multiplication Benchmark</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Matrix Multiplication ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">matrix_sizes</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Matrix Size: </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># NumPy (CPU)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">a_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">b_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">c_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_np</span><span class="p">,</span> <span class="n">b_np</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">numpy_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># CuPy (GPU)</span>
        <span class="n">a_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a_np</span><span class="p">)</span>
        <span class="n">b_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">b_np</span><span class="p">)</span>

        <span class="c1"># Warm up</span>
        <span class="n">c_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_cp</span><span class="p">,</span> <span class="n">b_cp</span><span class="p">)</span>
        <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="o">.</span><span class="n">null</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">c_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_cp</span><span class="p">,</span> <span class="n">b_cp</span><span class="p">)</span>
            <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="o">.</span><span class="n">null</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">cupy_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># PyTorch (GPU)</span>
        <span class="n">a_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">b_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># Warm up</span>
        <span class="n">c_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_torch</span><span class="p">,</span> <span class="n">b_torch</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">c_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_torch</span><span class="p">,</span> <span class="n">b_torch</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">pytorch_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># Numba CUDA</span>
        <span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">matmul_kernel</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
            <span class="n">row</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">y</span>
            <span class="n">col</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>

            <span class="k">if</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="ow">and</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
                    <span class="n">tmp</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
                <span class="n">C</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>

        <span class="n">a_numba</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">a_np</span><span class="p">)</span>
        <span class="n">b_numba</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">b_np</span><span class="p">)</span>
        <span class="n">c_numba</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">threads_per_block</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">blocks_per_grid_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">blocks_per_grid_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blocks_per_grid_x</span><span class="p">,</span> <span class="n">blocks_per_grid_y</span><span class="p">)</span>

        <span class="c1"># Warm up</span>
        <span class="n">matmul_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">a_numba</span><span class="p">,</span> <span class="n">b_numba</span><span class="p">,</span> <span class="n">c_numba</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">matmul_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">a_numba</span><span class="p">,</span> <span class="n">b_numba</span><span class="p">,</span> <span class="n">c_numba</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
            <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">numba_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># Store results</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;matmul_size&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;numpy_matmul&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numpy_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;cupy_matmul&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cupy_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;pytorch_matmul&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pytorch_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;numba_matmul&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numba_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  NumPy:   </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CuPy:    </span><span class="si">{</span><span class="n">cupy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">cupy_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  PyTorch: </span><span class="si">{</span><span class="n">pytorch_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">pytorch_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Numba:   </span><span class="si">{</span><span class="n">numba_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">numba_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>

    <span class="c1"># Element-wise Operations Benchmark</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Element-wise Operations (sin(x) + cos(x)) ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">vector_sizes</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Vector Size: </span><span class="si">{</span><span class="n">size</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># NumPy (CPU)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">x_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">y_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">numpy_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># CuPy (GPU)</span>
        <span class="n">x_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span>

        <span class="c1"># Warm up</span>
        <span class="n">y_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_cp</span><span class="p">)</span> <span class="o">+</span> <span class="n">cp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_cp</span><span class="p">)</span>
        <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="o">.</span><span class="n">null</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">y_cp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_cp</span><span class="p">)</span> <span class="o">+</span> <span class="n">cp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_cp</span><span class="p">)</span>
            <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="o">.</span><span class="n">null</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">cupy_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># PyTorch (GPU)</span>
        <span class="n">x_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># Warm up</span>
        <span class="n">y_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_torch</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_torch</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">y_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_torch</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x_torch</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">pytorch_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># Numba CUDA</span>
        <span class="nd">@numba_cuda</span><span class="o">.</span><span class="n">jit</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">elementwise_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
                <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">libdevice</span><span class="o">.</span><span class="n">sinf</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">+</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">libdevice</span><span class="o">.</span><span class="n">cosf</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

        <span class="n">x_numba</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">x_np</span><span class="p">)</span>
        <span class="n">y_numba</span> <span class="o">=</span> <span class="n">numba_cuda</span><span class="o">.</span><span class="n">device_array</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="n">blocks_per_grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">threads_per_block</span>

        <span class="c1"># Warm up</span>
        <span class="n">elementwise_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">x_numba</span><span class="p">,</span> <span class="n">y_numba</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
        <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">elementwise_kernel</span><span class="p">[</span><span class="n">blocks_per_grid</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="p">](</span><span class="n">x_numba</span><span class="p">,</span> <span class="n">y_numba</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
            <span class="n">numba_cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
        <span class="n">numba_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>

        <span class="c1"># Store results</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;elementwise_size&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;numpy_elementwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numpy_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;cupy_elementwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cupy_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;pytorch_elementwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pytorch_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;numba_elementwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numba_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  NumPy:   </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CuPy:    </span><span class="si">{</span><span class="n">cupy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">cupy_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  PyTorch: </span><span class="si">{</span><span class="n">pytorch_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">pytorch_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Numba:   </span><span class="si">{</span><span class="n">numba_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> ms (Speedup: </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">/</span><span class="n">numba_time</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">x)&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># Run comprehensive benchmark</span>
<span class="n">benchmark_results</span> <span class="o">=</span> <span class="n">benchmark_frameworks</span><span class="p">()</span>

<span class="c1"># Plot results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="c1"># Matrix multiplication performance</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">sizes</span> <span class="o">=</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;matmul_size&#39;</span><span class="p">]</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numpy_matmul&#39;</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;NumPy (CPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;cupy_matmul&#39;</span><span class="p">],</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CuPy (GPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;pytorch_matmul&#39;</span><span class="p">],</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PyTorch (GPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numba_matmul&#39;</span><span class="p">],</span> <span class="s1">&#39;d-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Numba CUDA&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Matrix Size&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Matrix Multiplication Performance&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Matrix multiplication speedup</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">numpy_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numpy_matmul&#39;</span><span class="p">])</span>
<span class="n">cupy_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;cupy_matmul&#39;</span><span class="p">])</span>
<span class="n">pytorch_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;pytorch_matmul&#39;</span><span class="p">])</span>
<span class="n">numba_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numba_matmul&#39;</span><span class="p">])</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">cupy_speedup</span><span class="p">,</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CuPy vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">pytorch_speedup</span><span class="p">,</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PyTorch vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">numba_speedup</span><span class="p">,</span> <span class="s1">&#39;d-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Numba vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Matrix Size&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Speedup Factor&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Matrix Multiplication Speedup&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Element-wise performance</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">sizes</span> <span class="o">=</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;elementwise_size&#39;</span><span class="p">]</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numpy_elementwise&#39;</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;NumPy (CPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;cupy_elementwise&#39;</span><span class="p">],</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CuPy (GPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;pytorch_elementwise&#39;</span><span class="p">],</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PyTorch (GPU)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numba_elementwise&#39;</span><span class="p">],</span> <span class="s1">&#39;d-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Numba CUDA&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Vector Size&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (ms)&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Element-wise Operations Performance&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Element-wise speedup</span>
<span class="n">ax4</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">numpy_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numpy_elementwise&#39;</span><span class="p">])</span>
<span class="n">cupy_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;cupy_elementwise&#39;</span><span class="p">])</span>
<span class="n">pytorch_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;pytorch_elementwise&#39;</span><span class="p">])</span>
<span class="n">numba_speedup</span> <span class="o">=</span> <span class="n">numpy_times</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;numba_elementwise&#39;</span><span class="p">])</span>

<span class="n">ax4</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">cupy_speedup</span><span class="p">,</span> <span class="s1">&#39;s-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CuPy vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">pytorch_speedup</span><span class="p">,</span> <span class="s1">&#39;^-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PyTorch vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">numba_speedup</span><span class="p">,</span> <span class="s1">&#39;d-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Numba vs NumPy&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Vector Size&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Speedup Factor&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Element-wise Operations Speedup&#39;</span><span class="p">)</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;framework_performance_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h3 id="lab-deliverables">Lab Deliverables<a class="headerlink" href="#lab-deliverables" title="Permanent link">&para;</a></h3>
<h4 id="required-submissions">Required Submissions<a class="headerlink" href="#required-submissions" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Performance Analysis Report</strong> (PDF, 5-8 pages)</li>
<li>GPU architecture analysis with hardware specifications</li>
<li>Memory bandwidth benchmarks and analysis</li>
<li>Occupancy analysis for different kernel configurations</li>
<li>Framework performance comparison with detailed charts</li>
<li>
<p>Optimization recommendations based on findings</p>
</li>
<li>
<p><strong>Optimized Algorithm Implementations</strong> (Python files)</p>
</li>
<li><code>gpu_architecture_analysis.py</code>: Complete hardware profiling code</li>
<li><code>optimized_algorithms.py</code>: Parallel reduction and matrix transpose implementations</li>
<li><code>framework_benchmark.py</code>: Comprehensive framework comparison</li>
<li>
<p>All code must be well-documented with performance comments</p>
</li>
<li>
<p><strong>Profiling Results</strong> (Data files and visualizations)</p>
</li>
<li><code>memory_bandwidth_analysis.png</code>: Bandwidth vs data size plots</li>
<li><code>framework_performance_comparison.png</code>: Performance comparison charts</li>
<li><code>profiling_data.json</code>: Raw benchmark data in JSON format</li>
<li>
<p><code>optimization_analysis.txt</code>: Detailed analysis of optimization techniques</p>
</li>
<li>
<p><strong>Framework Comparison Study</strong> (Technical report)</p>
</li>
<li>Quantitative comparison of NumPy, CuPy, PyTorch, and Numba</li>
<li>Use case recommendations for each framework</li>
<li>Performance scaling analysis</li>
<li>Memory usage and efficiency analysis</li>
</ol>
<h4 id="bonus-challenges">Bonus Challenges<a class="headerlink" href="#bonus-challenges" title="Permanent link">&para;</a></h4>
<p>🏆 <strong>CUDA Master</strong>: Implement a custom CUDA C++ kernel and compare with Numba
🏆 <strong>Memory Wizard</strong>: Implement and benchmark unified memory vs explicit memory management
🏆 <strong>Profiling Expert</strong>: Use NVIDIA Nsight Systems to create detailed profiling reports
🏆 <strong>Optimization Guru</strong>: Achieve &gt;90% of theoretical memory bandwidth in custom kernels
🏆 <strong>Framework Innovator</strong>: Create a hybrid solution combining multiple frameworks</p>
<h3 id="lab-assessment-criteria">Lab Assessment Criteria<a class="headerlink" href="#lab-assessment-criteria" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Criterion</th>
<th>Weight</th>
<th>Excellent (90-100%)</th>
<th>Good (80-89%)</th>
<th>Satisfactory (70-79%)</th>
<th>Needs Improvement (&lt;70%)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Technical Implementation</strong></td>
<td>40%</td>
<td>All algorithms implemented correctly with advanced optimizations</td>
<td>Most algorithms correct with good optimizations</td>
<td>Basic implementations working</td>
<td>Incomplete or incorrect implementations</td>
</tr>
<tr>
<td><strong>Performance Analysis</strong></td>
<td>30%</td>
<td>Comprehensive analysis with deep insights and recommendations</td>
<td>Good analysis with clear findings</td>
<td>Basic analysis with some insights</td>
<td>Superficial or incomplete analysis</td>
</tr>
<tr>
<td><strong>Code Quality</strong></td>
<td>20%</td>
<td>Excellent documentation, clean code, proper error handling</td>
<td>Good documentation and code structure</td>
<td>Adequate documentation</td>
<td>Poor documentation or code quality</td>
</tr>
<tr>
<td><strong>Innovation &amp; Optimization</strong></td>
<td>10%</td>
<td>Creative optimizations and novel approaches</td>
<td>Some optimization attempts</td>
<td>Basic optimizations</td>
<td>No optimization efforts</td>
</tr>
</tbody>
</table>
<h3 id="summary-and-next-steps">Summary and Next Steps<a class="headerlink" href="#summary-and-next-steps" title="Permanent link">&para;</a></h3>
<p>This comprehensive lab has provided you with:</p>
<p>✅ <strong>Deep Understanding</strong>: GPU architecture, memory hierarchy, and parallel computing principles
✅ <strong>Practical Skills</strong>: Implementation of optimized CUDA kernels and algorithm optimization
✅ <strong>Framework Expertise</strong>: Comparative analysis of major GPU computing frameworks
✅ <strong>Profiling Mastery</strong>: Advanced performance analysis and optimization techniques
✅ <strong>Real-world Application</strong>: Production-ready accelerated computing solutions</p>
<p><strong>Key Takeaways:</strong>
- GPU architecture directly impacts algorithm design and performance
- Memory access patterns are crucial for achieving high performance
- Different frameworks excel in different use cases
- Profiling and measurement are essential for optimization
- Theoretical knowledge must be combined with practical implementation</p>
<p><strong>Future Exploration:</strong>
- Advanced CUDA features (cooperative groups, tensor cores)
- Multi-GPU programming and scaling
- Integration with deep learning frameworks
- Real-time systems and low-latency computing
- Domain-specific optimizations (computer vision, scientific computing)</p>
<hr />
<h2 id="framework-comparison-summary">📌 Framework Comparison Summary<a class="headerlink" href="#framework-comparison-summary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Framework</th>
<th>Language</th>
<th>Learning Curve</th>
<th>Performance</th>
<th>Jetson Support</th>
<th>Best Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CUDA C++</strong></td>
<td>C++</td>
<td>❌ Steep</td>
<td>✅ Maximum</td>
<td>✅ Native</td>
<td>Custom kernels, maximum performance</td>
</tr>
<tr>
<td><strong>Numba CUDA</strong></td>
<td>Python</td>
<td>✅ Moderate</td>
<td>✅ Excellent</td>
<td>✅ Container</td>
<td>Mathematical algorithms, prototyping</td>
</tr>
<tr>
<td><strong>CuPy</strong></td>
<td>Python</td>
<td>✅ Easy</td>
<td>✅ Very Good</td>
<td>✅ Container</td>
<td>NumPy replacement, scientific computing</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>Python</td>
<td>✅ Easy</td>
<td>✅ Optimized</td>
<td>✅ Container</td>
<td>Machine learning, tensor operations</td>
</tr>
</tbody>
</table>
<p><strong>Jetson Orin Nano</strong> provides a complete ecosystem for accelerated computing, from low-level CUDA programming to high-level framework integration, making it ideal for edge AI development and deployment.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>