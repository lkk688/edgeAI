# 🏁 Final Project: Hackathon & Challenges

## 🎓 Purpose

The final stage of this Jetson course is a hands-on Hackathon project. Students will work in teams to design, build, and demonstrate a real-world AI+Cyber application on Jetson Orin Nano.

---

## 💡 Project Themes and Tutorials

### 1. 🤖 AI Agent

Create a local LLM-powered assistant using llama.cpp or Ollama with basic tool usage.

#### 🧰 Starting Code

```python
from langchain.tools import PythonREPLTool
from langchain.agents import initialize_agent, AgentType
from langchain.llms import LlamaCpp

llm = LlamaCpp(model_path="/models/mistral.gguf", n_gpu_layers=60)
tools = [PythonREPLTool()]

# TODO: Add your own custom tool, e.g., file reader or command runner

agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
response = agent.run("What is 15 squared?")
print(response)
```

#### 🛠️ Student Task

* [ ] Add memory to retain multi-turn conversation
* [ ] Add 1–2 custom tools (e.g., File system or network scanner)
* [ ] Refine the prompt formatting and output parsing

#### 🧪 Evaluation Benchmark

| Metric           | Points | Competition Benchmark                 |
| ---------------- | ------ | ------------------------------------- |
| Tool integration | 4      | Longest tool chain executed correctly |
| Use of memory    | 3      | Number of coherent multi-turn queries |
| Custom prompt    | 3      | Best response formatting and clarity  |

---

### 2. 🧠 RAG Application

Build a local document-based question answering tool using LangChain.

#### 🧰 Starting Code

```python
from langchain.document_loaders import TextLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import LlamaCpp

loader = TextLoader("data.txt")
docs = loader.load()

# TODO: Add a document splitter and support for multiple file types

embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(docs, embedding, persist_directory="ragdb")
retriever = vectorstore.as_retriever()
llm = LlamaCpp(model_path="/models/mistral.gguf")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

query = "What is Jetson used for?"
print(qa.run(query))
```

#### 🛠️ Student Task

* [ ] Chunk large documents into sections
* [ ] Add PDF, Markdown, or log file loaders
* [ ] Enhance retrieval ranking

#### 🧪 Evaluation Benchmark

| Metric             | Points | Competition Benchmark                       |
| ------------------ | ------ | ------------------------------------------- |
| Retriever accuracy | 4      | % of correct answers over 5 known queries   |
| Prompt clarity     | 3      | # of correct follow-ups generated by prompt |
| Model efficiency   | 3      | Avg token/s for answering RAG queries       |

---

### 3. 🎥 Vision + LLM Hybrid

Use a CNN model (e.g., YOLO) to generate object detection data, then describe it with an LLM.

#### 🧰 Starting Code

```python
import torch
from transformers import pipeline
from PIL import Image

labels = ["cat", "laptop"]  # TODO: Replace with real detection output
caption = ", ".join(labels)

pipe = pipeline("text-generation", model="sshleifer/tiny-gpt2")
prompt = f"Image contains: {caption}. Describe the scene."
response = pipe(prompt, max_length=50)
print(response[0]['generated_text'])
```

#### 🛠️ Student Task

* [ ] Integrate live YOLO detection
* [ ] Replace tiny-gpt2 with local LLM
* [ ] Optimize runtime and output parsing

#### 🧪 Evaluation Benchmark

| Metric            | Points | Competition Benchmark                     |
| ----------------- | ------ | ----------------------------------------- |
| Working detection | 4      | # of unique object classes detected       |
| LLM explanation   | 3      | Scene description detail score (1–5)      |
| Real-time speed   | 3      | FPS running detection + response pipeline |

---

### 4. 🔐 Cybersecurity with AI

Monitor Linux system and use an LLM to interpret logs and detect issues.

#### 🧰 Starting Code

```python
import os
from langchain.llms import LlamaCpp
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.embeddings import SentenceTransformerEmbeddings

os.system("ps aux > logs/ps_snapshot.txt")
loader = TextLoader("logs/ps_snapshot.txt")
docs = loader.load()

# TODO: Add log sources like auth.log, netstat, custom scanner

embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(docs, embedding)
llm = LlamaCpp(model_path="/models/mistral.gguf")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())
print(qa.run("Are there suspicious processes running?"))
```

#### 🛠️ Student Task

* [ ] Add log variety and real-time updates
* [ ] Present findings with a dashboard or alerts
* [ ] Use multiple prompts to explain risk levels

#### 🧪 Evaluation Benchmark

| Metric                  | Points | Competition Benchmark                         |
| ----------------------- | ------ | --------------------------------------------- |
| Log coverage            | 4      | # of log types and anomaly cases detected     |
| LLM explanation quality | 3      | Top 3 log explanations judged for clarity     |
| Monitoring loop/script  | 3      | # of alerts issued over 5 minutes of test run |

---

## ✅ General Requirements

* Project must run on Jetson (no cloud dependency)
* At least one local LLM must be used (Ollama or llama-cpp)
* Prompt design must be part of the logic
* Modular and documented code preferred

---

## 📦 Submission Checklist

* [ ] Working code repo (GitHub or .zip)
* [ ] README with instructions
* [ ] 2-minute demo video (screen or camera)
* [ ] Team names and GitHub handles

---

## 🧠 Takeaway

This is your chance to apply:

* Jetson-accelerated AI
* Prompt engineering
* Local inference
* Cyber/LLM integration

> Build something useful. Make it private. Make it fast. Make it edge-native on Jetson!
